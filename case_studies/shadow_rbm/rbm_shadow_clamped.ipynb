{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-14T21:33:09.486685Z",
     "start_time": "2025-06-14T21:33:08.613542Z"
    }
   },
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib.data_loading import load_measurements, MixedDataLoader\n",
    "\n",
    "####\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, Any, Sequence, Callable\n",
    "\n",
    "import jax\n",
    "import jax.lax\n",
    "from jax.random import PRNGKey\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import linen as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_dir = \"data\"\n",
    "model_dir = Path(\"./models\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data resides in        : {data_dir}\")\n",
    "print(f\"Model will be saved to : {model_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data resides in        : data\n",
      "Model will be saved to : models\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:33:11.145681Z",
     "start_time": "2025-06-14T21:33:11.119046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClampedRBM(nn.Module):\n",
    "    num_visible: int\n",
    "    num_hidden: int\n",
    "    k: int = 1\n",
    "    T: float = 1.0\n",
    "\n",
    "    def setup(self):\n",
    "        self.W = self.param(\"W\", nn.initializers.normal(0.01), (self.num_visible, self.num_hidden))\n",
    "        self.b = self.param(\"b\", nn.initializers.zeros, (self.num_visible,))\n",
    "        self.c = self.param(\"c\", nn.initializers.zeros, (self.num_hidden,))\n",
    "\n",
    "    def _free_energy(self, v: jnp.ndarray) -> jnp.ndarray:\n",
    "        return -(v @ self.b) - jnp.sum(jax.nn.softplus(v @ self.W + self.c), -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(state, W, b, c, T, clamp_mask, clamp_vals):\n",
    "        v, key = state\n",
    "        key, h_key, v_key = jax.random.split(key, 3)\n",
    "\n",
    "        # for simplicity reasons we sample the full visible vector and overwrite the clamped units\n",
    "        h = jax.random.bernoulli(h_key, jax.nn.sigmoid((v @ W + c) / T)).astype(jnp.float32)\n",
    "        v = jax.random.bernoulli(v_key, jax.nn.sigmoid((h @ W.T + b) / T)).astype(jnp.float32)\n",
    "\n",
    "        v = jnp.where(clamp_mask, clamp_vals, v)\n",
    "        return v, key\n",
    "\n",
    "    def __call__(self, data: jnp.ndarray, aux_vars: Dict[str, Any]) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n",
    "        key = aux_vars[\"key\"]\n",
    "        clamp_mask = aux_vars[\"clamp_mask\"]\n",
    "\n",
    "        batch_size = data.shape[0]\n",
    "        v_data = data.reshape(batch_size, -1).astype(jnp.float32)\n",
    "        clamp_mask = jnp.broadcast_to(clamp_mask, v_data.shape)\n",
    "        clamp_vals = jnp.where(clamp_mask, v_data, 0.)\n",
    "\n",
    "        key, init_key = jax.random.split(key)\n",
    "        gibbs_chain = jax.random.bernoulli(init_key, p=0.5, shape=v_data.shape).astype(jnp.float32)\n",
    "        gibbs_chain = jnp.where(clamp_mask, clamp_vals, gibbs_chain)\n",
    "\n",
    "        step_fn = lambda i, s: self._gibbs_step(s, self.W, self.b, self.c, self.T, clamp_mask, clamp_vals)\n",
    "        vk, key = jax.lax.fori_loop(0, self.k, step_fn, (gibbs_chain, key))\n",
    "        vk = jax.lax.stop_gradient(vk)\n",
    "\n",
    "        fe_data  = jnp.mean(self._free_energy(v_data))\n",
    "        fe_model = jnp.mean(self._free_energy(vk))\n",
    "\n",
    "        l2_regularization = jnp.sum(self.W**2) + jnp.sum(self.b**2) + jnp.sum(self.c**2)\n",
    "        loss  = fe_data - fe_model + aux_vars[\"l2_strength\"] * l2_regularization\n",
    "\n",
    "        aux_vars = { \"key\": key, \"free_energy_data\": fe_data, \"free_energy_model\": fe_model }\n",
    "\n",
    "        return loss, aux_vars\n",
    "\n",
    "    @nn.nowrap\n",
    "    def generate(self,\n",
    "                 clamp_vals: jnp.ndarray,  # (B, V) flattened or (B, …) to be flattened\n",
    "                 clamp_mask: jnp.ndarray,  # (1|B, V) bool\n",
    "                 T_schedule: jnp.ndarray,  # (L,)\n",
    "                 key: PRNGKey) -> jnp.ndarray:\n",
    "\n",
    "        clamp_vals = clamp_vals.reshape(clamp_vals.shape[0], -1).astype(jnp.float32)\n",
    "        clamp_mask = jnp.broadcast_to(clamp_mask.astype(bool), clamp_vals.shape)\n",
    "\n",
    "        key, init_key = jax.random.split(key)\n",
    "        v = jax.random.bernoulli(init_key, 0.5, shape=clamp_vals.shape).astype(jnp.float32)\n",
    "        v = jnp.where(clamp_mask, clamp_vals, v)\n",
    "\n",
    "        def step(i, s):\n",
    "            return self._gibbs_step(s, self.W, self.b, self.c,\n",
    "                                    T_schedule[i], clamp_mask, clamp_vals)\n",
    "\n",
    "        v_final, _ = jax.lax.fori_loop(0, len(T_schedule), step, (v, key))\n",
    "        return v_final  # caller can reshape back if desired"
   ],
   "id": "3da1a7e002764eb2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:33:14.378286Z",
     "start_time": "2025-06-14T21:33:14.359328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "        state: TrainState,\n",
    "        batch: jnp.ndarray,\n",
    "        aux_vars: Dict[str, Any]) -> Tuple[TrainState, jnp.ndarray, Dict[str, Any]]:\n",
    "\n",
    "    loss_fn = lambda params: state.apply_fn({\"params\": params}, batch, aux_vars)\n",
    "\n",
    "    (loss, aux_vars), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    return state, loss, aux_vars\n",
    "\n",
    "\n",
    "def train(\n",
    "        state: TrainState,\n",
    "        loader: MixedDataLoader,\n",
    "        num_epochs: int,\n",
    "        key: PRNGKey,\n",
    "        l2_strength: float,\n",
    "        lr_schedule_fn: Callable[[int], float]) -> Tuple[TrainState, Dict[int, float]]:\n",
    "\n",
    "    metrics: Dict[int, Any] = {}\n",
    "    clamp_mask = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tot_loss, batches = 0.0, 0\n",
    "\n",
    "        for batch in loader:\n",
    "            if clamp_mask is None:\n",
    "                batch_size, num_qubits, _ = batch.shape\n",
    "                clamp_mask = jnp.ones((batch_size, num_qubits, 3), dtype=bool)\n",
    "                clamp_mask = clamp_mask.at[:, :, 0].set(False)\n",
    "\n",
    "            key, subkey = jax.random.split(key)\n",
    "            aux_vars = {\n",
    "                \"key\": subkey,\n",
    "                \"clamp_mask\": clamp_mask,\n",
    "                \"l2_strength\": l2_strength,\n",
    "            }\n",
    "\n",
    "            state, loss, aux_out = train_step(state, batch, aux_vars)\n",
    "            key = aux_out[\"key\"]\n",
    "            free_E_model = aux_out[\"free_energy_model\"]\n",
    "            free_E_data = aux_out[\"free_energy_data\"]\n",
    "            model_samples = aux_out[\"model_samples\"]\n",
    "\n",
    "            tot_loss += float(loss)\n",
    "            batches  += 1\n",
    "\n",
    "        avg_loss = tot_loss / batches\n",
    "        lr       = lr_schedule_fn(state.step)\n",
    "\n",
    "        metrics[epoch] = dict(\n",
    "            loss=avg_loss,\n",
    "            free_energy_model=free_E_model,\n",
    "            free_energy_data=free_E_data,\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} │ \"\n",
    "              f\"Loss: {avg_loss:+.4f} │ \"\n",
    "              f\"Free En. Model: {free_E_model:.4f} │ \"\n",
    "              f\"Free En. Data: {free_E_data:.4f} │ \"\n",
    "              f\"Learning Rate: {lr:.5f}\")\n",
    "\n",
    "    return state, metrics"
   ],
   "id": "ee6baaf157159ffd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# next steps\n",
    "#   - train only on Z basis\n",
    "#   - finetune on mixed basis"
   ],
   "id": "4b01d89f3cf3a564"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
