{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import jax\n",
    "import jax.lax\n",
    "from jax.random import PRNGKey\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import linen as nn\n",
    "\n",
    "from typing import Optional, Tuple, Dict, Any, Sequence\n",
    "\n",
    "\n",
    "data_dir = \"./data\"\n",
    "print(f\"Data resides in        : {data_dir}\")"
   ],
   "id": "91a9becda4bd3db9"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RBM(nn.Module):\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "    k: int = 1\n",
    "    n_chains: int = 1000\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, data_batch: jnp.ndarray, aux_vars: Optional[Dict[str, Any]] = None) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n",
    "        W = self.param(\"W\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        b = self.param(\"b\", nn.initializers.zeros,        (self.n_visible,))\n",
    "        c = self.param(\"c\", nn.initializers.zeros,        (self.n_hidden,))\n",
    "        key = aux_vars[\"key\"]\n",
    "\n",
    "        key, bern_key = jax.random.split(key, 2)\n",
    "        v_chain_batch = jax.random.bernoulli(bern_key, p=0.5, shape=(self.n_chains, self.n_visible)).astype(jnp.float32)\n",
    "        model_batch, key = self._gibbs_sample(W, b, c, v_chain_batch, key, k=self.k)\n",
    "        model_batch = jax.lax.stop_gradient(model_batch)\n",
    "\n",
    "        free_energy_data = self._free_energy(W, b, c, data_batch)\n",
    "        free_energy_model = self._free_energy(W, b, c, model_batch)\n",
    "\n",
    "        loss = jnp.mean(free_energy_data) - jnp.mean(free_energy_model)\n",
    "        aux_vars[\"key\"] = key\n",
    "\n",
    "        return loss, aux_vars\n",
    "\n",
    "    @staticmethod\n",
    "    def _free_energy(W, b, c, v_batch):\n",
    "        visible_term = jnp.dot(v_batch, b)\n",
    "        hidden_term  = jnp.sum(jax.nn.softplus(v_batch @ W + c), axis=-1)\n",
    "        free_energy = -visible_term - hidden_term\n",
    "        return free_energy\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(i, state, W, b, c, T=1.0):\n",
    "        v_batch, key = state\n",
    "        key, h_key, v_key = jax.random.split(key, 3)\n",
    "\n",
    "        h_logits = (v_batch @ W + c) / T\n",
    "        h_probs = jax.nn.sigmoid(h_logits)\n",
    "        h = jax.random.bernoulli(h_key, h_probs).astype(jnp.float32)\n",
    "\n",
    "        v_logits = (h @ W.T + b) / T\n",
    "        v_probs = jax.nn.sigmoid(v_logits)\n",
    "        v = jax.random.bernoulli(v_key, v_probs).astype(jnp.float32)\n",
    "\n",
    "        return v, key\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_sample(W, b, c, v_batch, rng, k=1, T=1.0):\n",
    "        body_fun = lambda i, state: RBM._gibbs_step(i, state, W, b, c, T)\n",
    "        v_final, key = jax.lax.fori_loop(0, k, body_fun, (v_batch, rng))\n",
    "        return v_final, key\n",
    "\n",
    "    @staticmethod\n",
    "    def _annealing_step(i, state, W, b, c, T_schedule):\n",
    "        v, rng = state\n",
    "        T = T_schedule[i]\n",
    "        v_next, rng_next = RBM._gibbs_sample(W, b, c, v, rng, k=1, T=T)\n",
    "        return v_next, rng_next\n",
    "\n",
    "    @nn.nowrap\n",
    "    def generate(self, n_samples: int, T_schedule: jnp.ndarray, rng: PRNGKey) -> jnp.ndarray:\n",
    "        W = self.variables[\"params\"][\"W\"]\n",
    "        b = self.variables[\"params\"][\"b\"]\n",
    "        c = self.variables[\"params\"][\"c\"]\n",
    "\n",
    "        rng, init_key = jax.random.split(rng)\n",
    "        v = jax.random.bernoulli(init_key, p=0.5, shape=(n_samples, self.n_visible)).astype(jnp.float32)\n",
    "        state = (v, rng)\n",
    "\n",
    "        body_fun = lambda i, s: RBM._annealing_step(i, s, W, b, c, T_schedule)\n",
    "        v_final, _ = jax.lax.fori_loop(0, len(T_schedule), body_fun, state)\n",
    "        return v_final"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@jax.jit\n",
    "def train_step_amp(\n",
    "        state: TrainState,\n",
    "        batch_dict: Dict[str, jnp.ndarray],\n",
    "        aux_vars: Dict[str, Any]) -> Tuple[TrainState, jnp.ndarray, Dict[str, Any]]:\n",
    "\n",
    "    if len(batch_dict) != 1:\n",
    "        raise ValueError(\"Batch dictionary must contain exactly one entry.\")\n",
    "\n",
    "    (key, batch), = batch_dict.items()\n",
    "    if set(key) != {'Z'}:\n",
    "        raise ValueError(f\"Batch key must consist only of 'Z', got: {key}\")\n",
    "\n",
    "    loss_fn = lambda params: state.apply_fn({'params': params}, batch, aux_vars)\n",
    "    value_and_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "    (loss, aux_vars), grads = value_and_grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss, aux_vars\n",
    "\n",
    "\n",
    "def train_amp_rbm(\n",
    "        state: TrainState,\n",
    "        loader: MultiBasisDataLoader,\n",
    "        num_epochs: int,\n",
    "        rng: PRNGKey) -> Tuple[TrainState, Dict[int, float], PRNGKey]:\n",
    "\n",
    "    metrics = {}\n",
    "    aux_vars = {\"key\": rng}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tot_loss = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for data_batch in loader:\n",
    "            state, loss, aux_vars = train_step_amp(state, data_batch, aux_vars)\n",
    "            tot_loss += loss\n",
    "            batches += 1\n",
    "\n",
    "        metrics[epoch] = {\"loss_amp\": float(tot_loss / batches)}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} â”‚ Loss: {metrics[epoch]['loss_amp']:.4f}\")\n",
    "\n",
    "    return state, metrics, aux_vars[\"key\"]"
   ],
   "id": "e47124d18776f951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---- hyperparameters ----\n",
    "batch_size    = 128\n",
    "visible_units = 10\n",
    "hidden_units  = 30\n",
    "k_steps       = 10\n",
    "lr            = 1e-2\n",
    "num_epochs    = 50\n",
    "chains        = 1000\n",
    "\n",
    "random_seed = PRNGKey(42)\n",
    "rng, init_key = jax.random.split(random_seed)\n",
    "\n",
    "# model initialization\n",
    "model_amp = RBM(n_visible=visible_units, n_hidden=hidden_units, k=k_steps, n_chains=chains)\n",
    "dummy_batch = jnp.zeros((batch_size, visible_units), dtype=jnp.float32)\n",
    "variables_amp = model_amp.init(init_key, dummy_batch, {\"key\": rng})\n",
    "\n",
    "optimizer_amp = optax.adam(learning_rate=lr)\n",
    "state_amp = TrainState.create(apply_fn=model_amp.apply, params=variables_amp[\"params\"], tx=optimizer_amp)"
   ],
   "id": "b116e8d8fcb06030"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
