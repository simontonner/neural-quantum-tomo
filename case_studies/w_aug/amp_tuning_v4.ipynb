{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:37:01.067386Z",
     "start_time": "2025-06-01T12:37:01.062500Z"
    }
   },
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.lax\n",
    "from jax.random import PRNGKey\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import linen as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional, Tuple, Dict, Any, Sequence\n",
    "\n",
    "\n",
    "data_dir = \"./data\"\n",
    "print(f\"Data resides in        : {data_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data resides in        : ./data\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:37:02.193884Z",
     "start_time": "2025-06-01T12:37:02.184750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiBasisDataLoader:\n",
    "    def __init__(self, data_dict: dict[str, jnp.ndarray],\n",
    "                 batch_size: int = 128,\n",
    "                 shuffle: bool = True,\n",
    "                 drop_last: bool = False,\n",
    "                 seed: int = 0):\n",
    "        lengths = [len(v) for v in data_dict.values()]\n",
    "        if len(set(lengths)) != 1:\n",
    "            raise ValueError(f\"All arrays must have the same length, got: {lengths}\")\n",
    "\n",
    "        self.data = data_dict\n",
    "        self.n = lengths[0]\n",
    "        self.bs = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.idx_slices = [\n",
    "            (i, i + batch_size)\n",
    "            for i in range(0, self.n, batch_size)\n",
    "            if not drop_last or i + batch_size <= self.n\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.order = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(self.order)\n",
    "        self.slice_idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.slice_idx >= len(self.idx_slices):\n",
    "            raise StopIteration\n",
    "        s, e = self.idx_slices[self.slice_idx]\n",
    "        self.slice_idx += 1\n",
    "        return {k: v[self.order[s:e]] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "def load_measurements(folder: str, file_pattern: str = \"w_*.txt\") -> dict[str, jnp.ndarray]:\n",
    "    out: dict[str, jnp.ndarray] = {}\n",
    "\n",
    "    for fp in Path(folder).glob(file_pattern):\n",
    "        basis = fp.stem.split(\"_\")[2]\n",
    "\n",
    "        bitstrings = []\n",
    "        with fp.open() as f:\n",
    "            for line in f:\n",
    "                bitstring = np.fromiter((c.islower() for c in line.strip()), dtype=np.float32)\n",
    "                bitstrings.append(bitstring)\n",
    "\n",
    "        arr = jnp.asarray(np.stack(bitstrings))\n",
    "        if basis in out:\n",
    "            out[basis] = jnp.concatenate([out[basis], arr], axis=0)\n",
    "        else:\n",
    "            out[basis] = arr\n",
    "\n",
    "    return out"
   ],
   "id": "78de40dc6fd42395",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:37:03.489330Z",
     "start_time": "2025-06-01T12:37:03.264787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dict = load_measurements(\"data/\", \"w_*.txt\")\n",
    "\n",
    "keys_amp = [k for k in data_dict if re.fullmatch(r\"^Z+$\", k)]\n",
    "keys_pha = [k for k in data_dict if re.fullmatch(r\"^(?!Z+$).*\", k)]\n",
    "dict_amp = {k: data_dict[k] for k in keys_amp}\n",
    "dict_pha = {k: data_dict[k] for k in keys_pha}\n",
    "\n",
    "loader_amp = MultiBasisDataLoader(dict_amp, batch_size=128)\n",
    "loader_pha = MultiBasisDataLoader(dict_pha, batch_size=128)"
   ],
   "id": "d6edc336135bc877",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:37:04.659889Z",
     "start_time": "2025-06-01T12:37:04.650338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax, jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from flax import linen as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class PcdRBM(nn.Module):\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "    k: int = 1\n",
    "    T: float = 1.0\n",
    "    n_chains: int = 128\n",
    "\n",
    "    def setup(self):\n",
    "        self.W = self.param(\"W\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        self.b = self.param(\"b\", nn.initializers.zeros, (self.n_visible,))\n",
    "        self.c = self.param(\"c\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "        # initialise only once; RNG needed only here\n",
    "        def init_chain():\n",
    "            k = self.make_rng(\"pcd_init\")\n",
    "            return jax.random.bernoulli(\n",
    "                k, p=0.5,\n",
    "                shape=(self.n_chains, self.n_visible)\n",
    "            ).astype(jnp.float32)\n",
    "\n",
    "        self.v_chain = self.variable(\"pcd_state\", \"v_chain\", init_chain)\n",
    "\n",
    "    def __call__(self, data_batch: jnp.ndarray, key: PRNGKey) -> Tuple[jnp.ndarray, PRNGKey]:\n",
    "        gibbs_step_fn = lambda i, s: self._gibbs_step(s, self.W, self.b, self.c, self.T)\n",
    "        model_batch, key = jax.lax.fori_loop(0, self.k, gibbs_step_fn, (self.v_chain.value, key))\n",
    "        self.v_chain.value = jax.lax.stop_gradient(model_batch) # cut off gradients after Gibbs sampling\n",
    "\n",
    "        loss = jnp.mean(self._free_energy(data_batch)) - jnp.mean(self._free_energy(self.v_chain.value))\n",
    "        return loss, key\n",
    "\n",
    "    def _free_energy(self, v):\n",
    "        return -(v @ self.b) - jnp.sum(jax.nn.softplus(v @ self.W + self.c), -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(state, W, b, c, T):\n",
    "        v, key = state\n",
    "        key, h_key, v_key = jax.random.split(key, 3)\n",
    "        h = jax.random.bernoulli(h_key, jax.nn.sigmoid((v @ W + c)/T)).astype(jnp.float32)\n",
    "        v = jax.random.bernoulli(v_key, jax.nn.sigmoid((h @ W.T + b)/T)).astype(jnp.float32)\n",
    "        return v, key"
   ],
   "id": "3da1a7e002764eb2",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:47:29.044336Z",
     "start_time": "2025-06-01T12:47:29.038223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "from typing import Any\n",
    "\n",
    "@struct.dataclass\n",
    "class PcdTrainState(train_state.TrainState):\n",
    "    pcd_state: FrozenDict[str, Any] = struct.field(pytree_node=True,\n",
    "                                                   default_factory=lambda: FrozenDict())\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step_amp(\n",
    "        state: PcdTrainState,\n",
    "        batch_dict: Dict[str, jnp.ndarray],\n",
    "        key: PRNGKey\n",
    ") -> Tuple[PcdTrainState, jnp.ndarray, PRNGKey]:\n",
    "\n",
    "    if len(batch_dict) != 1:\n",
    "        raise ValueError(\"Batch dictionary must contain exactly one entry.\")\n",
    "\n",
    "    (name, batch), = batch_dict.items()\n",
    "    if set(name) != {'Z'}:\n",
    "        raise ValueError(f\"Batch key must consist only of 'Z', got: {name}\")\n",
    "\n",
    "    # Correct loss_fn â€” returns ((loss, key), mutable)\n",
    "    loss_fn = lambda params: state.apply_fn(\n",
    "        {\"params\": params, \"pcd_state\": state.pcd_state},\n",
    "        batch,\n",
    "        key,\n",
    "        mutable=[\"pcd_state\"]\n",
    "    )\n",
    "\n",
    "    ((loss, key), mutable), grads = jax.value_and_grad(\n",
    "        lambda p: loss_fn(p)[0],  # grads w.r.t. scalar loss only\n",
    "        has_aux=True\n",
    "    )(state.params)\n",
    "\n",
    "    # ðŸ”¥ This is now a traced FrozenDict â€” safe to index!\n",
    "    state = state.apply_gradients(grads=grads).replace(\n",
    "        pcd_state=mutable[\"pcd_state\"]\n",
    "    )\n",
    "    return state, loss, key\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_amp_rbm(\n",
    "        state: TrainState,\n",
    "        loader: MultiBasisDataLoader,\n",
    "        num_epochs: int,\n",
    "        key: PRNGKey) -> Tuple[TrainState, Dict[int, float]]:\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tot_loss = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for batch_dict in loader:\n",
    "            state, loss, key = train_step_amp(state, batch_dict, key)\n",
    "            tot_loss += loss\n",
    "            batches += 1\n",
    "\n",
    "        metrics[epoch] = {\"loss_amp\": float(tot_loss / batches)}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} â”‚ Loss: {metrics[epoch]['loss_amp']:.4f}\")\n",
    "\n",
    "    return state, metrics"
   ],
   "id": "ee6baaf157159ffd",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:47:30.172829Z",
     "start_time": "2025-06-01T12:47:30.027994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---- hyperparameters ----\n",
    "batch_size    = 128\n",
    "visible_units = 10\n",
    "hidden_units  = visible_units\n",
    "k_steps       = 5\n",
    "lr            = 1e-2\n",
    "num_epochs    = 50\n",
    "chains        = batch_size\n",
    "\n",
    "key_seed = PRNGKey(42)\n",
    "key, key_params, key_chains, key_dummy = jax.random.split(key_seed, 4)\n",
    "\n",
    "model_amp = PcdRBM(visible_units, hidden_units, k=k_steps, n_chains=chains)\n",
    "batch_dummy = jnp.zeros((batch_size, visible_units), dtype=jnp.float32)\n",
    "chain_dummy = jnp.zeros((chains, visible_units), dtype=jnp.float32)\n",
    "variables_amp = model_amp.init({\"params\": key_params, \"pcd_init\": key_chains}, batch_dummy, key_dummy)\n",
    "\n",
    "optimizer_amp = optax.adam(learning_rate=lr)\n",
    "state_amp = PcdTrainState.create(\n",
    "    apply_fn=model_amp.apply,\n",
    "    params=variables_amp[\"params\"],\n",
    "    tx=optimizer_amp,\n",
    "    pcd_state=variables_amp[\"pcd_state\"])"
   ],
   "id": "f0cc88aaeee8d911",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:47:30.971681Z",
     "start_time": "2025-06-01T12:47:30.909221Z"
    }
   },
   "cell_type": "code",
   "source": "train_amp_rbm(state_amp, loader_amp, num_epochs, key)",
   "id": "af3d5612c325fb2e",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_amp_rbm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[48], line 62\u001B[0m, in \u001B[0;36mtrain_amp_rbm\u001B[0;34m(state, loader, num_epochs, key)\u001B[0m\n\u001B[1;32m     59\u001B[0m batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_dict \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m---> 62\u001B[0m     state, loss, key \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step_amp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m     tot_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m     64\u001B[0m     batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "    \u001B[0;31m[... skipping hidden 11 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[48], line 34\u001B[0m, in \u001B[0;36mtrain_step_amp\u001B[0;34m(state, batch_dict, key)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Correct loss_fn â€” returns ((loss, key), mutable)\u001B[39;00m\n\u001B[1;32m     27\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m params: state\u001B[38;5;241m.\u001B[39mapply_fn(\n\u001B[1;32m     28\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m: params, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpcd_state\u001B[39m\u001B[38;5;124m\"\u001B[39m: state\u001B[38;5;241m.\u001B[39mpcd_state},\n\u001B[1;32m     29\u001B[0m     batch,\n\u001B[1;32m     30\u001B[0m     key,\n\u001B[1;32m     31\u001B[0m     mutable\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpcd_state\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     32\u001B[0m )\n\u001B[0;32m---> 34\u001B[0m ((loss, key), mutable), grads \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mvalue_and_grad(\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m p: loss_fn(p)[\u001B[38;5;241m0\u001B[39m],  \u001B[38;5;66;03m# grads w.r.t. scalar loss only\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     has_aux\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     37\u001B[0m )(state\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# ðŸ”¥ This is now a traced FrozenDict â€” safe to index!\u001B[39;00m\n\u001B[1;32m     40\u001B[0m state \u001B[38;5;241m=\u001B[39m state\u001B[38;5;241m.\u001B[39mapply_gradients(grads\u001B[38;5;241m=\u001B[39mgrads)\u001B[38;5;241m.\u001B[39mreplace(\n\u001B[1;32m     41\u001B[0m     pcd_state\u001B[38;5;241m=\u001B[39mmutable[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpcd_state\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     42\u001B[0m )\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/dlnn2/lib/python3.9/site-packages/jax/_src/lax/lax.py:1592\u001B[0m, in \u001B[0;36m_iter\u001B[0;34m(tracer)\u001B[0m\n\u001B[1;32m   1590\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_iter\u001B[39m(tracer):\n\u001B[1;32m   1591\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m tracer\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1592\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration over a 0-d array\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# same as numpy error\u001B[39;00m\n\u001B[1;32m   1593\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1594\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(tracer\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[0;31mTypeError\u001B[0m: iteration over a 0-d array"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
