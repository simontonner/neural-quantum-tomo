{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T10:46:16.768849Z",
     "start_time": "2025-06-03T10:46:16.762565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, Any, Sequence\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.lax\n",
    "from jax.random import PRNGKey\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.training import checkpoints\n",
    "from flax import linen as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from flax.training import checkpoints\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress specific Orbax sharding warning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=(\n",
    "        \"Couldn't find sharding info under RestoreArgs.*\"\n",
    "    ),\n",
    "    category=UserWarning,\n",
    "    module=\"orbax.checkpoint.type_handlers\"\n",
    ")\n",
    "\n",
    "data_dir = \"./data\"\n",
    "model_dir = \"./models\"\n",
    "model_prefix = \"rbm_amp_202506031229_0\"\n",
    "\n",
    "print(f\"Data resides in                         : {data_dir}\")\n",
    "print(f\"Amplitude RBM checkpoint to be loaded   : {model_dir}/{model_prefix}\")"
   ],
   "id": "cac0523cf96155b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data resides in                         : ./data\n",
      "Amplitude RBM checkpoint to be loaded   : ./models/rbm_amp_202506031229_0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T10:46:17.696600Z",
     "start_time": "2025-06-03T10:46:17.687599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiBasisDataLoader:\n",
    "    def __init__(self, data_dict: dict[str, jnp.ndarray],\n",
    "                 batch_size: int = 128,\n",
    "                 shuffle: bool = True,\n",
    "                 drop_last: bool = False,\n",
    "                 seed: int = 0):\n",
    "        lengths = [len(v) for v in data_dict.values()]\n",
    "        if len(set(lengths)) != 1:\n",
    "            raise ValueError(f\"All arrays must have the same length, got: {lengths}\")\n",
    "\n",
    "        self.data = data_dict\n",
    "        self.n = lengths[0]\n",
    "        self.bs = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.idx_slices = [\n",
    "            (i, i + batch_size)\n",
    "            for i in range(0, self.n, batch_size)\n",
    "            if not drop_last or i + batch_size <= self.n\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.order = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(self.order)\n",
    "        self.slice_idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.slice_idx >= len(self.idx_slices):\n",
    "            raise StopIteration\n",
    "        s, e = self.idx_slices[self.slice_idx]\n",
    "        self.slice_idx += 1\n",
    "        return {k: v[self.order[s:e]] for k, v in self.data.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_slices)\n",
    "\n",
    "\n",
    "def load_measurements(folder: str, file_pattern: str = \"w_*.txt\") -> dict[str, jnp.ndarray]:\n",
    "    out: dict[str, jnp.ndarray] = {}\n",
    "\n",
    "    for fp in Path(folder).glob(file_pattern):\n",
    "        basis = fp.stem.split(\"_\")[2]\n",
    "\n",
    "        bitstrings = []\n",
    "        with fp.open() as f:\n",
    "            for line in f:\n",
    "                bitstring = np.fromiter((c.islower() for c in line.strip()), dtype=np.float32)\n",
    "                bitstrings.append(bitstring)\n",
    "\n",
    "        arr = jnp.asarray(np.stack(bitstrings))\n",
    "        if basis in out:\n",
    "            out[basis] = jnp.concatenate([out[basis], arr], axis=0)\n",
    "        else:\n",
    "            out[basis] = arr\n",
    "\n",
    "    return out"
   ],
   "id": "162d3a19b52761f6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T10:46:18.903471Z",
     "start_time": "2025-06-03T10:46:18.686078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dict = load_measurements(\"data/\", \"w_*.txt\")\n",
    "keys_pha = [k for k in data_dict if re.fullmatch(r\"^(?!Z+$).*\", k)]\n",
    "dict_pha = {k: data_dict[k] for k in keys_pha}"
   ],
   "id": "4810713cb617047",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T10:46:19.787561Z",
     "start_time": "2025-06-03T10:46:19.776550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PairPhaseRBM(nn.Module):\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "\n",
    "    def setup(self):\n",
    "        zeros = lambda shape: jnp.zeros(shape, dtype=jnp.float32)\n",
    "\n",
    "        # Amplitude RBM (frozen parameters)\n",
    "        self.W_amp = self.variable('amp_state', 'W_amp', zeros, (self.n_visible, self.n_hidden))\n",
    "        self.b_amp = self.variable('amp_state', 'b_amp', zeros, (self.n_visible,))\n",
    "        self.c_amp = self.variable('amp_state', 'c_amp', zeros, (self.n_hidden,))\n",
    "\n",
    "        # Phase RBM (trainable parameters)\n",
    "        self.W_pha = self.param('W_pha', nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        self.b_pha = self.param('b_pha', nn.initializers.zeros, (self.n_visible,))\n",
    "        self.c_pha = self.param('c_pha', nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "        # Rotation matrices\n",
    "        sqrt2 = jnp.sqrt(2.0)\n",
    "        self.rotators = {\n",
    "            'X': jnp.array([[1, 1], [1, -1]], dtype=jnp.complex64) / sqrt2,\n",
    "            'Y': jnp.array([[1, -1j], [1, 1j]], dtype=jnp.complex64) / sqrt2,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _free_energy(W, b, c, v):\n",
    "        visible_term = jnp.dot(v, b)\n",
    "        hidden_term = jnp.sum(jax.nn.softplus(jnp.dot(v, W) + c), axis=-1)\n",
    "        return -visible_term - hidden_term\n",
    "\n",
    "    @staticmethod\n",
    "    def _free_energy_grad(W, b, c, v):\n",
    "        pre = jnp.dot(v, W) + c\n",
    "        sig = jax.nn.sigmoid(pre)\n",
    "        dW = jnp.einsum('bi,bj->bij', v, sig)\n",
    "        return jnp.concatenate([dW.reshape(v.shape[0], -1), v, sig], axis=1)\n",
    "\n",
    "    def rotated_log_psi_and_grad(self, sigma_b, basis):\n",
    "        W_amp, b_amp, c_amp = self.W_amp.value, self.b_amp.value, self.c_amp.value\n",
    "        W_pha, b_pha, c_pha = self.W_pha, self.b_pha, self.c_pha\n",
    "\n",
    "        B, n = sigma_b.shape\n",
    "        non_z = [i for i, p in enumerate(basis) if p != 'Z']\n",
    "        if len(non_z) != 2:\n",
    "            raise ValueError(\"Basis must have exactly two non-Z qubits.\")\n",
    "        j, k = non_z\n",
    "\n",
    "        Rj, Rk = self.rotators[basis[j]], self.rotators[basis[k]]\n",
    "        U = jnp.kron(Rj, Rk)\n",
    "\n",
    "        combos = jnp.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=sigma_b.dtype)\n",
    "        tiled = jnp.tile(sigma_b[:, None, :], (1, 4, 1))\n",
    "        modified = tiled.at[:, :, [j, k]].set(combos[None, :, :])\n",
    "        flat = modified.reshape(B * 4, n)\n",
    "\n",
    "        F_amp = self._free_energy(W_amp, b_amp, c_amp, flat)\n",
    "        F_pha = self._free_energy(W_pha, b_pha, c_pha, flat)\n",
    "\n",
    "        log_mag = (-0.5 * F_amp).reshape(B, 4)\n",
    "        angle = (-0.5 * F_pha).reshape(B, 4)\n",
    "        M = jnp.max(log_mag, axis=1, keepdims=True)\n",
    "        scaled = jnp.exp(log_mag - M + 1j * angle)\n",
    "\n",
    "        idx = (sigma_b[:, j].astype(int) << 1) | sigma_b[:, k].astype(int)\n",
    "        Uc = U[:, idx].T\n",
    "        psi_rot = jnp.sum(Uc * scaled, axis=1)\n",
    "\n",
    "        grad_F_amp = self._free_energy_grad(W_amp, b_amp, c_amp, flat).reshape(B, 4, -1)\n",
    "        grad_F_pha = self._free_energy_grad(W_pha, b_pha, c_pha, flat).reshape(B, 4, -1)\n",
    "\n",
    "        grad_logpsi = -0.5 * grad_F_amp + -0.5j * grad_F_pha\n",
    "        psi_weighted_grad = jnp.einsum(\"bij,bi->bj\", grad_logpsi, Uc * scaled)\n",
    "        psi_ratio = psi_weighted_grad / (psi_rot[:, None] + 1e-12)\n",
    "\n",
    "        split = grad_F_amp.shape[-1]\n",
    "        grad_lambda = psi_ratio[:, :split].real\n",
    "        grad_mu = -psi_ratio[:, split:].imag\n",
    "\n",
    "        return grad_lambda, grad_mu, jnp.log(jnp.abs(psi_rot) + 1e-12), M.squeeze()\n",
    "\n",
    "    def __call__(self, data_batch_dict):\n",
    "        total_loss = 0.\n",
    "        for basis, sigma_b in data_batch_dict.items():\n",
    "            _, _, log_amp, M = self.rotated_log_psi_and_grad(sigma_b, basis)\n",
    "            total_loss += -2. * jnp.mean(log_amp + M)\n",
    "        return total_loss"
   ],
   "id": "ed3723ee401327ec",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T10:46:20.901353Z",
     "start_time": "2025-06-03T10:46:20.889993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def train_step_pha(state: TrainState, amp_vars: Dict[str, jnp.ndarray], batch_dict: Dict[str, jnp.ndarray]) -> Tuple[TrainState, jnp.ndarray]:\n",
    "    model_var_dict = { 'params': state.params, 'amp_state': amp_vars }\n",
    "    def loss_fn(params):\n",
    "        model_var_dict['params'] = params\n",
    "        return state.apply_fn(model_var_dict, batch_dict)\n",
    "\n",
    "    grads = {}\n",
    "    loss = 0.0\n",
    "    for basis, sigma_b in batch_dict.items():\n",
    "        grad_lambda, grad_mu, log_amp, M = state.apply_fn(model_var_dict, method=PairPhaseRBM.rotated_log_psi_and_grad, sigma_b=sigma_b, basis=basis)\n",
    "        loss += -2.0 * jnp.mean(log_amp + M)\n",
    "\n",
    "        flat_mu = jnp.mean(grad_mu, axis=0)\n",
    "\n",
    "        W_shape = state.params['W_pha'].shape\n",
    "        b_shape = state.params['b_pha'].shape\n",
    "        c_shape = state.params['c_pha'].shape\n",
    "\n",
    "        import numpy as np\n",
    "        nW = np.prod(W_shape)\n",
    "        nb = np.prod(b_shape)\n",
    "        nc = np.prod(c_shape)\n",
    "\n",
    "        W_mu = flat_mu[:nW].reshape(W_shape)\n",
    "        b_mu = flat_mu[nW:nW+nb].reshape(b_shape)\n",
    "        c_mu = flat_mu[nW+nb:].reshape(c_shape)\n",
    "\n",
    "        grads_basis = {\n",
    "            'W_pha': W_mu,\n",
    "            'b_pha': b_mu,\n",
    "            'c_pha': c_mu\n",
    "        }\n",
    "\n",
    "        grads = jax.tree_util.tree_map(lambda g1, g2: g1 + g2, grads, grads_basis) if grads else grads_basis\n",
    "\n",
    "    grads = jax.tree_util.tree_map(lambda g: g / len(batch_dict), grads)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\n",
    "\n",
    "def train_phase_rbm(state_pha: TrainState, amp_vars: Dict[str, jnp.ndarray], loader: MultiBasisDataLoader, num_epochs: int) -> Tuple[TrainState, Dict[int, float]]:\n",
    "    metrics = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        tot_loss = 0.0\n",
    "        batches = 0\n",
    "        for batch_dict in loader:\n",
    "            state_pha, loss = train_step_pha(state_pha, amp_vars, batch_dict)\n",
    "            tot_loss += loss\n",
    "            batches += 1\n",
    "        metrics[epoch] = {\"loss_pha\": float(tot_loss / batches)}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} â”‚ Loss: {metrics[epoch]['loss_pha']:.4f}\")\n",
    "    return state_pha, metrics"
   ],
   "id": "b8f78a4784851517",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T10:46:22.387894Z",
     "start_time": "2025-06-03T10:46:22.369760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params_amp = checkpoints.restore_checkpoint(\n",
    "    ckpt_dir=str(Path(model_dir).resolve()),\n",
    "    target=None,\n",
    "    prefix=model_prefix\n",
    ")\n",
    "\n",
    "W_amp = params_amp[\"W\"]\n",
    "b_amp = params_amp[\"b\"]\n",
    "c_amp = params_amp[\"c\"]\n",
    "\n",
    "amp_vars = {\n",
    "    \"W_amp\": W_amp,\n",
    "    \"b_amp\": b_amp,\n",
    "    \"c_amp\": c_amp,\n",
    "}"
   ],
   "id": "ff52dfd0a3cbe4dc",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T10:47:01.377629Z",
     "start_time": "2025-06-03T10:46:59.746837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size    = 6400\n",
    "lr            = 1e-3\n",
    "num_epochs    = 50 # will be increased but currently no conclusive downwards trend\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "loader_pha = MultiBasisDataLoader(dict_pha, batch_size=128)\n",
    "\n",
    "\n",
    "model_pha = PairPhaseRBM(n_visible=visible_units, n_hidden=hidden_units)\n",
    "dummy_dict = next(iter(loader_pha))\n",
    "variables_pha  = model_pha.init(rng, dummy_dict)\n",
    "\n",
    "#tx = optax.chain(\n",
    "#    natgrad_diag(damping=1e-3),\n",
    "#    optax.scale(-lr),  # standard gradient descent on natural step\n",
    "#)\n",
    "\n",
    "#state_pha = TrainState.create(\n",
    "#    apply_fn=model_pha.apply,\n",
    "#    params=variables_pha['params'],\n",
    "#    tx=tx,\n",
    "#)\n",
    "optimizer_pha = optax.adam(learning_rate=lr)\n",
    "state_pha = TrainState.create(apply_fn=model_pha.apply, params=variables_pha['params'], tx=optimizer_pha)\n"
   ],
   "id": "94f6f9f7a32d4348",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (10,) and (16,).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m model_pha \u001B[38;5;241m=\u001B[39m PairPhaseRBM(n_visible\u001B[38;5;241m=\u001B[39mvisible_units, n_hidden\u001B[38;5;241m=\u001B[39mhidden_units)\n\u001B[1;32m     11\u001B[0m dummy_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(loader_pha))\n\u001B[0;32m---> 12\u001B[0m variables_pha  \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_pha\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrng\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdummy_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#tx = optax.chain(\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m#    natgrad_diag(damping=1e-3),\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#    optax.scale(-lr),  # standard gradient descent on natural step\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m#    tx=tx,\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#)\u001B[39;00m\n\u001B[1;32m     24\u001B[0m optimizer_pha \u001B[38;5;241m=\u001B[39m optax\u001B[38;5;241m.\u001B[39madam(learning_rate\u001B[38;5;241m=\u001B[39mlr)\n",
      "    \u001B[0;31m[... skipping hidden 9 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[28], line 84\u001B[0m, in \u001B[0;36mPairPhaseRBM.__call__\u001B[0;34m(self, data_batch_dict)\u001B[0m\n\u001B[1;32m     82\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.\u001B[39m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m basis, sigma_b \u001B[38;5;129;01min\u001B[39;00m data_batch_dict\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m---> 84\u001B[0m     _, _, log_amp, M \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrotated_log_psi_and_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43msigma_b\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbasis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2.\u001B[39m \u001B[38;5;241m*\u001B[39m jnp\u001B[38;5;241m.\u001B[39mmean(log_amp \u001B[38;5;241m+\u001B[39m M)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_loss\n",
      "    \u001B[0;31m[... skipping hidden 2 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[28], line 56\u001B[0m, in \u001B[0;36mPairPhaseRBM.rotated_log_psi_and_grad\u001B[0;34m(self, sigma_b, basis)\u001B[0m\n\u001B[1;32m     53\u001B[0m modified \u001B[38;5;241m=\u001B[39m tiled\u001B[38;5;241m.\u001B[39mat[:, :, [j, k]]\u001B[38;5;241m.\u001B[39mset(combos[\u001B[38;5;28;01mNone\u001B[39;00m, :, :])\n\u001B[1;32m     54\u001B[0m flat \u001B[38;5;241m=\u001B[39m modified\u001B[38;5;241m.\u001B[39mreshape(B \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m4\u001B[39m, n)\n\u001B[0;32m---> 56\u001B[0m F_amp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_free_energy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mW_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m F_pha \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_free_energy(W_pha, b_pha, c_pha, flat)\n\u001B[1;32m     59\u001B[0m log_mag \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m F_amp)\u001B[38;5;241m.\u001B[39mreshape(B, \u001B[38;5;241m4\u001B[39m)\n",
      "Cell \u001B[0;32mIn[28], line 27\u001B[0m, in \u001B[0;36mPairPhaseRBM._free_energy\u001B[0;34m(W, b, c, v)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_free_energy\u001B[39m(W, b, c, v):\n\u001B[0;32m---> 27\u001B[0m     visible_term \u001B[38;5;241m=\u001B[39m \u001B[43mjnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     hidden_term \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39msum(jax\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39msoftplus(jnp\u001B[38;5;241m.\u001B[39mdot(v, W) \u001B[38;5;241m+\u001B[39m c), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39mvisible_term \u001B[38;5;241m-\u001B[39m hidden_term\n",
      "    \u001B[0;31m[... skipping hidden 11 frame]\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/dlnn2/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4573\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(a, b, precision, preferred_element_type)\u001B[0m\n\u001B[1;32m   4571\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4572\u001B[0m     contract_dims \u001B[38;5;241m=\u001B[39m ((a_ndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m,), (b_ndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m,))\n\u001B[0;32m-> 4573\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mlax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot_general\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdimension_numbers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcontract_dims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_dims\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4574\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mprecision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprecision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreferred_element_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreferred_element_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m lax_internal\u001B[38;5;241m.\u001B[39m_convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001B[0;31m[... skipping hidden 7 frame]\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/dlnn2/lib/python3.9/site-packages/jax/_src/lax/lax.py:2705\u001B[0m, in \u001B[0;36m_dot_general_shape_rule\u001B[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001B[0m\n\u001B[1;32m   2702\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m core\u001B[38;5;241m.\u001B[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001B[1;32m   2703\u001B[0m   msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdot_general requires contracting dimensions to have the same \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2704\u001B[0m          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2705\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001B[1;32m   2707\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _dot_general_shape_computation(lhs\u001B[38;5;241m.\u001B[39mshape, rhs\u001B[38;5;241m.\u001B[39mshape, dimension_numbers)\n",
      "\u001B[0;31mTypeError\u001B[0m: dot_general requires contracting dimensions to have the same shape, got (10,) and (16,)."
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
