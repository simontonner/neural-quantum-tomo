{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:16:54.254147Z",
     "start_time": "2025-06-01T13:16:54.249902Z"
    }
   },
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.lax\n",
    "from jax.random import PRNGKey\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import linen as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional, Tuple, Dict, Any, Sequence\n",
    "\n",
    "\n",
    "data_dir = \"./data\"\n",
    "print(f\"Data resides in        : {data_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data resides in        : ./data\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:16:55.089764Z",
     "start_time": "2025-06-01T13:16:55.080032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiBasisDataLoader:\n",
    "    def __init__(self, data_dict: dict[str, jnp.ndarray],\n",
    "                 batch_size: int = 128,\n",
    "                 shuffle: bool = True,\n",
    "                 drop_last: bool = False,\n",
    "                 seed: int = 0):\n",
    "        lengths = [len(v) for v in data_dict.values()]\n",
    "        if len(set(lengths)) != 1:\n",
    "            raise ValueError(f\"All arrays must have the same length, got: {lengths}\")\n",
    "\n",
    "        self.data = data_dict\n",
    "        self.n = lengths[0]\n",
    "        self.bs = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.idx_slices = [\n",
    "            (i, i + batch_size)\n",
    "            for i in range(0, self.n, batch_size)\n",
    "            if not drop_last or i + batch_size <= self.n\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.order = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(self.order)\n",
    "        self.slice_idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.slice_idx >= len(self.idx_slices):\n",
    "            raise StopIteration\n",
    "        s, e = self.idx_slices[self.slice_idx]\n",
    "        self.slice_idx += 1\n",
    "        return {k: v[self.order[s:e]] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "def load_measurements(folder: str, file_pattern: str = \"w_*.txt\") -> dict[str, jnp.ndarray]:\n",
    "    out: dict[str, jnp.ndarray] = {}\n",
    "\n",
    "    for fp in Path(folder).glob(file_pattern):\n",
    "        basis = fp.stem.split(\"_\")[2]\n",
    "\n",
    "        bitstrings = []\n",
    "        with fp.open() as f:\n",
    "            for line in f:\n",
    "                bitstring = np.fromiter((c.islower() for c in line.strip()), dtype=np.float32)\n",
    "                bitstrings.append(bitstring)\n",
    "\n",
    "        arr = jnp.asarray(np.stack(bitstrings))\n",
    "        if basis in out:\n",
    "            out[basis] = jnp.concatenate([out[basis], arr], axis=0)\n",
    "        else:\n",
    "            out[basis] = arr\n",
    "\n",
    "    return out"
   ],
   "id": "78de40dc6fd42395",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:16:56.205885Z",
     "start_time": "2025-06-01T13:16:55.992927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dict = load_measurements(\"data/\", \"w_*.txt\")\n",
    "\n",
    "keys_amp = [k for k in data_dict if re.fullmatch(r\"^Z+$\", k)]\n",
    "keys_pha = [k for k in data_dict if re.fullmatch(r\"^(?!Z+$).*\", k)]\n",
    "dict_amp = {k: data_dict[k] for k in keys_amp}\n",
    "dict_pha = {k: data_dict[k] for k in keys_pha}\n",
    "\n",
    "loader_amp = MultiBasisDataLoader(dict_amp, batch_size=128)\n",
    "loader_pha = MultiBasisDataLoader(dict_pha, batch_size=128)"
   ],
   "id": "d6edc336135bc877",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:28:30.256089Z",
     "start_time": "2025-06-01T13:28:30.245140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax, jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from flax import linen as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "    k: int = 1\n",
    "    T: float = 1.0\n",
    "\n",
    "    def setup(self):\n",
    "        self.W = self.param(\"W\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        self.b = self.param(\"b\", nn.initializers.zeros, (self.n_visible,))\n",
    "        self.c = self.param(\"c\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "    def __call__(self, data: jnp.ndarray, aux_vars: Dict[str, Any]) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n",
    "        gibbs_step_fn = lambda i, s: self._gibbs_step(s, self.W, self.b, self.c, self.T)\n",
    "        gibbs_chain, key = jax.lax.fori_loop(0, self.k, gibbs_step_fn, (aux_vars[\"gibbs_chain\"], aux_vars[\"key\"]))\n",
    "        gibbs_chain = jax.lax.stop_gradient(gibbs_chain) # cut off gradients after Gibbs sampling\n",
    "\n",
    "        loss = jnp.mean(self._free_energy(data)) - jnp.mean(self._free_energy(gibbs_chain))\n",
    "        return loss, {\"gibbs_chain\": gibbs_chain, \"key\": key}\n",
    "\n",
    "    def _free_energy(self, v):\n",
    "        return -(v @ self.b) - jnp.sum(jax.nn.softplus(v @ self.W + self.c), -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(state, W, b, c, T):\n",
    "        v, key = state\n",
    "        key, h_key, v_key = jax.random.split(key, 3)\n",
    "        h = jax.random.bernoulli(h_key, jax.nn.sigmoid((v @ W + c)/T)).astype(jnp.float32)\n",
    "        v = jax.random.bernoulli(v_key, jax.nn.sigmoid((h @ W.T + b)/T)).astype(jnp.float32)\n",
    "        return v, key"
   ],
   "id": "3da1a7e002764eb2",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:28:31.804543Z",
     "start_time": "2025-06-01T13:28:31.795877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flax import struct\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.core import FrozenDict\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "@jax.jit\n",
    "def train_step_amp(\n",
    "        state: TrainState,\n",
    "        batch_dict: Dict[str, jnp.ndarray],\n",
    "        gibbs_chain: jnp.ndarray,\n",
    "        key: PRNGKey) -> Tuple[TrainState, jnp.ndarray, jnp.ndarray, PRNGKey]:\n",
    "\n",
    "    if len(batch_dict) != 1:\n",
    "        raise ValueError(\"Batch dictionary must contain exactly one entry.\")\n",
    "\n",
    "    (basis_key, batch), = batch_dict.items()\n",
    "    if set(basis_key) != {'Z'}:\n",
    "        raise ValueError(f\"Batch key must consist only of 'Z', got: {basis_key}\")\n",
    "\n",
    "    aux_vars = {\"gibbs_chain\": gibbs_chain, \"key\": key}\n",
    "    loss_fn = lambda params: state.apply_fn({'params': params}, batch, aux_vars)\n",
    "    value_and_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "    (loss, aux_vars), grads = value_and_grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, aux_vars[\"gibbs_chain\"], aux_vars[\"key\"]\n",
    "\n",
    "def train_amp_rbm(\n",
    "        state: TrainState,\n",
    "        loader: MultiBasisDataLoader,\n",
    "        gibbs_chain: jnp.ndarray,\n",
    "        num_epochs: int,\n",
    "        key: PRNGKey) -> Tuple[TrainState, Dict[int, float]]:\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tot_loss = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for batch_dict in loader:\n",
    "            state, loss, gibbs_chain, key = train_step_amp(state, batch_dict, gibbs_chain, key)\n",
    "            tot_loss += loss\n",
    "            batches += 1\n",
    "\n",
    "        metrics[epoch] = {\"loss_amp\": float(tot_loss / batches)}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} │ Loss: {metrics[epoch]['loss_amp']:.4f}\")\n",
    "\n",
    "    return state, metrics"
   ],
   "id": "ee6baaf157159ffd",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:28:32.720305Z",
     "start_time": "2025-06-01T13:28:32.537122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---- hyperparameters ----\n",
    "batch_size    = 128\n",
    "visible_units = 10\n",
    "hidden_units  = visible_units * 3\n",
    "k_steps       = 5\n",
    "lr            = 1e-2\n",
    "num_epochs    = 100\n",
    "chains        = batch_size\n",
    "\n",
    "key_seed = PRNGKey(42)\n",
    "key, key_params, key_chains, key_dummy = jax.random.split(key_seed, 4)\n",
    "\n",
    "model_amp = RBM(visible_units, hidden_units, k=k_steps)\n",
    "batch_dummy = jnp.zeros((batch_size, visible_units), dtype=jnp.float32)\n",
    "aux_vars_dummy = {\"gibbs_chain\": jnp.zeros((batch_size, visible_units), dtype=jnp.float32), \"key\": key_dummy}\n",
    "variables_amp = model_amp.init({\"params\": key_params}, batch_dummy, aux_vars_dummy)\n",
    "\n",
    "optimizer_amp = optax.adam(learning_rate=lr)\n",
    "state_amp = TrainState.create(apply_fn=model_amp.apply, params=variables_amp[\"params\"], tx=optimizer_amp)\n",
    "gibbs_chain = jax.random.bernoulli(key_chains, p=0.5, shape=(chains, visible_units)).astype(jnp.float32)"
   ],
   "id": "f0cc88aaeee8d911",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:29:01.443862Z",
     "start_time": "2025-06-01T13:28:56.989698Z"
    }
   },
   "cell_type": "code",
   "source": "%time state_amp, metrics_amp = train_amp_rbm(state_amp, loader_amp, gibbs_chain, num_epochs, key)",
   "id": "af3d5612c325fb2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 │ Loss: 0.0467\n",
      "Epoch 2/100 │ Loss: 0.3607\n",
      "Epoch 3/100 │ Loss: 0.1674\n",
      "Epoch 4/100 │ Loss: 0.0638\n",
      "Epoch 5/100 │ Loss: 0.2074\n",
      "Epoch 6/100 │ Loss: 0.0881\n",
      "Epoch 7/100 │ Loss: 0.0958\n",
      "Epoch 8/100 │ Loss: 0.2018\n",
      "Epoch 9/100 │ Loss: 0.1452\n",
      "Epoch 10/100 │ Loss: 0.0755\n",
      "Epoch 11/100 │ Loss: 0.2002\n",
      "Epoch 12/100 │ Loss: 0.0791\n",
      "Epoch 13/100 │ Loss: 0.1727\n",
      "Epoch 14/100 │ Loss: 0.0488\n",
      "Epoch 15/100 │ Loss: 0.0535\n",
      "Epoch 16/100 │ Loss: 0.1995\n",
      "Epoch 17/100 │ Loss: 0.1300\n",
      "Epoch 18/100 │ Loss: 0.2105\n",
      "Epoch 19/100 │ Loss: 0.1713\n",
      "Epoch 20/100 │ Loss: -0.0011\n",
      "Epoch 21/100 │ Loss: 0.1648\n",
      "Epoch 22/100 │ Loss: 0.0858\n",
      "Epoch 23/100 │ Loss: 0.1657\n",
      "Epoch 24/100 │ Loss: 0.1111\n",
      "Epoch 25/100 │ Loss: 0.1437\n",
      "Epoch 26/100 │ Loss: 0.1500\n",
      "Epoch 27/100 │ Loss: 0.2081\n",
      "Epoch 28/100 │ Loss: 0.0744\n",
      "Epoch 29/100 │ Loss: 0.1683\n",
      "Epoch 30/100 │ Loss: 0.1901\n",
      "Epoch 31/100 │ Loss: 0.0507\n",
      "Epoch 32/100 │ Loss: 0.1598\n",
      "Epoch 33/100 │ Loss: 0.1303\n",
      "Epoch 34/100 │ Loss: 0.1198\n",
      "Epoch 35/100 │ Loss: 0.3186\n",
      "Epoch 36/100 │ Loss: 0.1426\n",
      "Epoch 37/100 │ Loss: 0.1295\n",
      "Epoch 38/100 │ Loss: 0.2240\n",
      "Epoch 39/100 │ Loss: 0.1460\n",
      "Epoch 40/100 │ Loss: 0.0690\n",
      "Epoch 41/100 │ Loss: 0.2297\n",
      "Epoch 42/100 │ Loss: 0.0445\n",
      "Epoch 43/100 │ Loss: 0.1600\n",
      "Epoch 44/100 │ Loss: 0.1685\n",
      "Epoch 45/100 │ Loss: 0.1824\n",
      "Epoch 46/100 │ Loss: 0.1845\n",
      "Epoch 47/100 │ Loss: 0.0631\n",
      "Epoch 48/100 │ Loss: 0.1052\n",
      "Epoch 49/100 │ Loss: -0.0256\n",
      "Epoch 50/100 │ Loss: 0.1828\n",
      "Epoch 51/100 │ Loss: 0.0837\n",
      "Epoch 52/100 │ Loss: 0.1915\n",
      "Epoch 53/100 │ Loss: 0.0492\n",
      "Epoch 54/100 │ Loss: 0.1637\n",
      "Epoch 55/100 │ Loss: 0.1475\n",
      "Epoch 56/100 │ Loss: 0.2031\n",
      "Epoch 57/100 │ Loss: 0.2042\n",
      "Epoch 58/100 │ Loss: 0.1757\n",
      "Epoch 59/100 │ Loss: 0.1274\n",
      "Epoch 60/100 │ Loss: 0.1417\n",
      "Epoch 61/100 │ Loss: 0.1797\n",
      "Epoch 62/100 │ Loss: 0.2114\n",
      "Epoch 63/100 │ Loss: 0.1458\n",
      "Epoch 64/100 │ Loss: 0.1334\n",
      "Epoch 65/100 │ Loss: 0.1950\n",
      "Epoch 66/100 │ Loss: 0.0906\n",
      "Epoch 67/100 │ Loss: 0.1496\n",
      "Epoch 68/100 │ Loss: 0.0555\n",
      "Epoch 69/100 │ Loss: 0.1251\n",
      "Epoch 70/100 │ Loss: 0.0563\n",
      "Epoch 71/100 │ Loss: 0.3540\n",
      "Epoch 72/100 │ Loss: 0.2078\n",
      "Epoch 73/100 │ Loss: 0.2834\n",
      "Epoch 74/100 │ Loss: 0.0974\n",
      "Epoch 75/100 │ Loss: 0.1848\n",
      "Epoch 76/100 │ Loss: 0.1631\n",
      "Epoch 77/100 │ Loss: 0.1172\n",
      "Epoch 78/100 │ Loss: 0.0649\n",
      "Epoch 79/100 │ Loss: 0.1946\n",
      "Epoch 80/100 │ Loss: 0.0347\n",
      "Epoch 81/100 │ Loss: 0.0556\n",
      "Epoch 82/100 │ Loss: 0.2331\n",
      "Epoch 83/100 │ Loss: 0.0219\n",
      "Epoch 84/100 │ Loss: 0.1131\n",
      "Epoch 85/100 │ Loss: 0.0050\n",
      "Epoch 86/100 │ Loss: 0.0956\n",
      "Epoch 87/100 │ Loss: 0.0812\n",
      "Epoch 88/100 │ Loss: 0.2269\n",
      "Epoch 89/100 │ Loss: 0.0037\n",
      "Epoch 90/100 │ Loss: 0.1785\n",
      "Epoch 91/100 │ Loss: 0.1510\n",
      "Epoch 92/100 │ Loss: 0.1210\n",
      "Epoch 93/100 │ Loss: 0.0795\n",
      "Epoch 94/100 │ Loss: 0.2099\n",
      "Epoch 95/100 │ Loss: 0.1501\n",
      "Epoch 96/100 │ Loss: 0.2869\n",
      "Epoch 97/100 │ Loss: 0.1095\n",
      "Epoch 98/100 │ Loss: 0.2352\n",
      "Epoch 99/100 │ Loss: 0.0877\n",
      "Epoch 100/100 │ Loss: 0.0992\n",
      "CPU times: user 5.08 s, sys: 1.56 s, total: 6.64 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "44abdba35c6ed8bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
