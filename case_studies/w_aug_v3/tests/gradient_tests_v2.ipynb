{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T22:49:36.434452Z",
     "start_time": "2025-06-06T22:49:36.042846Z"
    }
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn import softmax\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import tree"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T22:49:36.942879Z",
     "start_time": "2025-06-06T22:49:36.882193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### RECURRING HELPERS FOR ROTATION MATRIX\n",
    "\n",
    "def get_computational_basis_vectors(num_qubits: int) -> jnp.ndarray:\n",
    "    indices = jnp.arange(2 ** num_qubits, dtype=jnp.uint32)  # shape (2**n,)\n",
    "    powers = 2 ** jnp.arange(num_qubits - 1, -1, -1, dtype=jnp.uint32)  # shape (n,)\n",
    "    bits = (indices[:, None] & powers) > 0  # shape (2**n, n), bool\n",
    "    return bits.astype(jnp.float32)\n",
    "\n",
    "def construct_rotation_matrix(measurement_basis: tuple[int, ...]) -> jnp.ndarray:\n",
    "    SQRT2 = jnp.sqrt(2.0)\n",
    "    single_qubit_rotation_matrices = [\n",
    "        jnp.array([[1, 0], [0, 1]], dtype=jnp.complex64),               # Z\n",
    "        jnp.array([[1, 1], [1, -1]], dtype=jnp.complex64) / SQRT2,      # X\n",
    "        jnp.array([[1, -1j], [1j, -1]], dtype=jnp.complex64) / SQRT2    # Y\n",
    "    ]\n",
    "\n",
    "    rotation_matrix = jnp.array([1.0+0j], dtype=jnp.complex64).reshape((1, 1))\n",
    "    for idx in measurement_basis:\n",
    "        rotation_matrix = jnp.kron(rotation_matrix, single_qubit_rotation_matrices[idx])\n",
    "    return rotation_matrix # (2**n, 2**n)\n",
    "\n",
    "def bitstring_to_int(bitstring: jnp.ndarray) -> jnp.ndarray:\n",
    "    powers = 2 ** jnp.arange(bitstring.shape[-1] - 1, -1, -1)\n",
    "    return jnp.sum(bitstring * powers, axis=-1).astype(jnp.int32)\n",
    "\n",
    "def multiply_with_leaf(factor, leaf):\n",
    "    if leaf.ndim == 1:\n",
    "        return factor * leaf\n",
    "    else:\n",
    "        return factor[:, None] * leaf\n",
    "\n",
    "\n",
    "#### MOCKING FREE ENERGY FUNCTIONS\n",
    "\n",
    "def dummy_free_energy(sigma: jnp.ndarray, params: dict) -> jnp.ndarray:\n",
    "    return jnp.dot(sigma, params[\"W\"]) + params[\"b\"]  # (2**n,)\n",
    "\n",
    "\n",
    "#### TEST VARIABLES\n",
    "\n",
    "measurement_basis = (1, 0, 2, 1)  # X Z Y X\n",
    "measurements = jnp.array([\n",
    "    [0,0,0,0],\n",
    "    [1,1,1,1],\n",
    "    [0,1,0,1],\n",
    "    [1,0,1,0],\n",
    "], dtype=jnp.int32)\n",
    "\n",
    "params_lambda = {\n",
    "    \"W\": jnp.linspace(0.05, 0.15, 4, dtype=jnp.float32),\n",
    "    \"b\": jnp.array(0.3, dtype=jnp.float32)\n",
    "}\n",
    "params_mu = {\n",
    "    \"W\": jnp.linspace(0.05, 0.15, 4, dtype=jnp.float32),\n",
    "    \"b\": jnp.array(0.3, dtype=jnp.float32)\n",
    "}"
   ],
   "id": "16f614cd7740125a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:03:49.393418Z",
     "start_time": "2025-06-06T11:03:49.373260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### GRADIENTS FROM AUTODIFF OF LOSS FUNCTION\n",
    "\n",
    "def rotated_log_probs(U_row: jnp.ndarray, F_lambda: jnp.ndarray, F_mu: jnp.ndarray) -> jnp.ndarray:\n",
    "    exponent = -0.5 * F_lambda - 0.5j * F_mu                     # (2**n,)\n",
    "    values = U_row * jnp.exp(exponent)                          # Complex vector\n",
    "\n",
    "    abs_vals = jnp.abs(values)\n",
    "    max_log = jnp.max(jnp.log(abs_vals + 1e-30))                # Scalar for stability\n",
    "    scaled_values = values * jnp.exp(-max_log)                  # Scale down before summing\n",
    "\n",
    "    return 2 * (max_log + jnp.log(jnp.abs(jnp.sum(scaled_values)) + 1e-30))\n",
    "\n",
    "def loss_fn(\n",
    "        measurements: jnp.ndarray,\n",
    "        measurement_basis: tuple[int, ...],\n",
    "        params_lambda: dict,\n",
    "        params_mu: dict) -> jnp.ndarray:\n",
    "\n",
    "    computational_basis_vectors = get_computational_basis_vectors(measurements.shape[1])  # (2**n, n)\n",
    "    free_energy_lambda = jax.vmap(dummy_free_energy, (0, None))(computational_basis_vectors, params_lambda) # (2**n,)\n",
    "    free_energy_mu  = jax.vmap(dummy_free_energy, (0, None))(computational_basis_vectors, params_mu) # (2**n,)\n",
    "\n",
    "    rotation_matrix = construct_rotation_matrix(measurement_basis)  # (2**n, 2**n)\n",
    "\n",
    "    get_log_prob = lambda m: rotated_log_probs(rotation_matrix[bitstring_to_int(m)], free_energy_lambda, free_energy_mu)\n",
    "\n",
    "    log_probs = jax.vmap(get_log_prob)(measurements)\n",
    "    loss = -jnp.mean(log_probs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "autodiff_grad_fn = jax.grad(loss_fn, argnums=3)\n",
    "\n",
    "autodiff_grad = autodiff_grad_fn(measurements, measurement_basis, params_lambda, params_mu)\n",
    "autodiff_grad"
   ],
   "id": "e235df13fc429dad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': Array([-9.99688721,  0.00006104,  0.01806641, -5.99365234,  0.00006104,\n",
       "        -4.72842407,  0.02926636,  0.00003052,  0.00006104,  0.00006104],      dtype=float32),\n",
       " 'b': Array( 0.00006294, dtype=float32)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T10:52:24.074799Z",
     "start_time": "2025-06-06T10:52:23.784695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### EXPLICIT GRADIENTS\n",
    "\n",
    "def grad_fn(measurements, basis, params_lambda, params_mu):\n",
    "    computational_basis_vectors = get_computational_basis_vectors(measurements.shape[1])\n",
    "\n",
    "    free_energy_lambda = jax.vmap(dummy_free_energy, (0, None))(computational_basis_vectors, params_lambda)\n",
    "    free_energy_mu = jax.vmap(dummy_free_energy, (0, None))(computational_basis_vectors, params_mu)\n",
    "    free_energy_mu_grads = jax.vmap(lambda s: jax.grad(lambda p: dummy_free_energy(s, p))(params_mu))(computational_basis_vectors)\n",
    "\n",
    "    rotation_matrix = construct_rotation_matrix(basis)\n",
    "\n",
    "    def per_sample(bits):\n",
    "        idx = bitstring_to_int(bits)\n",
    "        rotated_exponent = jnp.log(rotation_matrix[idx]) - 0.5 * free_energy_lambda - 0.5j * free_energy_mu\n",
    "        rotated_exponent = jnp.where(jnp.isfinite(rotated_exponent), rotated_exponent, -1e30 + 0j)  # safety\n",
    "        gradient_weights = jnp.imag(softmax(rotated_exponent))\n",
    "\n",
    "        def apply_to_leaf(leaf):\n",
    "            if leaf.ndim == 1:             # bias gradient\n",
    "                return -jnp.sum(gradient_weights * leaf)\n",
    "            else:                          # weight gradient\n",
    "                return -jnp.sum(gradient_weights[:, None] * leaf, axis=0)\n",
    "\n",
    "        return tree.map(apply_to_leaf, free_energy_mu_grads)\n",
    "\n",
    "    grads_batch = jax.vmap(per_sample)(measurements)         # stacked pytree\n",
    "    return tree.map(lambda x: jnp.mean(x, 0), grads_batch)\n",
    "\n",
    "\n",
    "\n",
    "explicit_grad = grad_fn(measurements, measurement_basis, params_lambda, params_mu)\n",
    "\n",
    "print(\"explicit :\", explicit_grad)\n",
    "print(\"autodiff :\", autodiff_grad)\n",
    "print(\"close    :\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=1e-4), explicit_grad, autodiff_grad)))"
   ],
   "id": "2e9da81002314183",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explicit : {'W': Array([-9.9956303e+00, -1.7881393e-06,  2.9097542e-02, -3.3207827e+00],      dtype=float32), 'b': Array(2.1606684e-07, dtype=float32)}\n",
      "autodiff : {'W': Array([-9.9958706e+00,  5.7220459e-06,  2.9106140e-02, -3.3208485e+00],      dtype=float32), 'b': Array(1.9073486e-06, dtype=float32)}\n",
      "close    : False\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T10:52:25.085900Z",
     "start_time": "2025-06-06T10:52:24.799912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def p_lambda(sigma, params_lambda):\n",
    "    free_energy_lambda = dummy_free_energy(sigma, params_lambda)\n",
    "    return jnp.exp(-free_energy_lambda)\n",
    "\n",
    "def phi_mu(sigma, params_mu):\n",
    "    free_energy_mu = dummy_free_energy(sigma, params_mu)\n",
    "    return -free_energy_mu\n",
    "\n",
    "def paper_loss_fn(\n",
    "        measurements: jnp.ndarray,\n",
    "        measurement_basis: tuple[int, ...],\n",
    "        params_lambda: dict,\n",
    "        params_mu: dict) -> jnp.ndarray:\n",
    "\n",
    "    computational_basis_vectors = get_computational_basis_vectors(measurements.shape[1])  # (2**n, n)\n",
    "\n",
    "    p_lambda_values = jax.vmap(p_lambda, (0, None))(computational_basis_vectors, params_lambda)  # (2**n,)\n",
    "    phi_mu_values = jax.vmap(phi_mu, (0, None))(computational_basis_vectors, params_mu)  # (2**n,)\n",
    "\n",
    "    rotation_matrix = construct_rotation_matrix(measurement_basis)  # (2**n, 2**n)\n",
    "\n",
    "    def get_log_probability(measurement):\n",
    "        idx = bitstring_to_int(measurement)\n",
    "        rotation_vector = rotation_matrix[idx]\n",
    "\n",
    "        sqrt_p_lambda = jnp.sqrt(p_lambda_values)\n",
    "        exp_phi_mu = jnp.exp(1j * phi_mu_values / 2)\n",
    "\n",
    "        rotated_amp = jnp.vdot(rotation_vector, sqrt_p_lambda * exp_phi_mu)\n",
    "\n",
    "        log_probability = jnp.log(jnp.abs(rotated_amp))\n",
    "        return log_probability\n",
    "\n",
    "    log_probs = []\n",
    "    for measurement in measurements:\n",
    "        contribution = get_log_probability(measurement)\n",
    "        # plus the complex conjugated part\n",
    "        log_probs.append(contribution + contribution.conj())\n",
    "\n",
    "    log_probs = jnp.array(log_probs)\n",
    "\n",
    "    loss = -jnp.mean(log_probs)\n",
    "    return loss\n",
    "\n",
    "import jax.numpy as jnp\n",
    "jnp.set_printoptions(suppress=True, formatter={'float_kind': '{: .8f}'.format})\n",
    "\n",
    "autodiff_grad_fn_paper = jax.grad(paper_loss_fn, argnums=3)\n",
    "\n",
    "autodiff_grad_paper = autodiff_grad_fn_paper(measurements, measurement_basis, params_lambda, params_mu)\n",
    "print(\"explicit (mine) :\", explicit_grad)\n",
    "print(\"autodiff (mine) :\", autodiff_grad)\n",
    "print(\"autodiff (paper):\", autodiff_grad_paper)\n",
    "\n",
    "print(\"close (paper):\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=1e-4), explicit_grad, autodiff_grad_paper)))"
   ],
   "id": "a1f46c51a765f1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explicit (mine) : {'W': Array([-9.99563026, -0.00000179,  0.02909754, -3.32078266], dtype=float32), 'b': Array( 0.00000022, dtype=float32)}\n",
      "autodiff (mine) : {'W': Array([-9.99587059,  0.00000572,  0.02910614, -3.32084846], dtype=float32), 'b': Array( 0.00000191, dtype=float32)}\n",
      "autodiff (paper): {'W': Array([-9.99580383, -0.00000572,  0.02909851, -3.32083893], dtype=float32), 'b': Array(-0.00000381, dtype=float32)}\n",
      "close (paper): True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:04:00.878110Z",
     "start_time": "2025-06-06T11:04:00.837601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# testing how big we cna go\n",
    "measurement_basis = (1, 0, 2, 1, 0, 1, 2, 0, 0, 0)  # X Z Y X Z Y X Z\n",
    "measurements = jnp.array([\n",
    "    [0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1],\n",
    "    [0,1,0,1,0,1,0,1,0,1],\n",
    "    [1,0,1,0,1,0,1,0,1,0],\n",
    "    [0,0,1,1,0,0,1,1,0,0],\n",
    "    [1,1,0,0,1,1,0,0,1,1],\n",
    "], dtype=jnp.int32)\n",
    "\n",
    "params_lambda = {\n",
    "    \"W\": jnp.linspace(0.05, 0.15, 10, dtype=jnp.float32),\n",
    "    \"b\": jnp.array(0.3, dtype=jnp.float32)\n",
    "}\n",
    "\n",
    "params_mu = {\n",
    "    \"W\": jnp.linspace(0.05, 0.15, 10, dtype=jnp.float32),\n",
    "    \"b\": jnp.array(0.3, dtype=jnp.float32)\n",
    "}\n",
    "\n",
    "autodiff_grad_paper_fn_large = jax.grad(paper_loss_fn, argnums=3)\n",
    "%time autodiff_grad_paper_large = autodiff_grad_paper_fn_large(measurements, measurement_basis, params_lambda, params_mu)"
   ],
   "id": "68b9027edaa7c39b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 1.46 ms, total: 33.5 ms\n",
      "Wall time: 31.9 ms\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:04:01.913908Z",
     "start_time": "2025-06-06T11:04:01.896222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# explicit\n",
    "%time explicit_grad_large = grad_fn(measurements, measurement_basis, params_lambda, params_mu)"
   ],
   "id": "673c88c2d1efebd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 ms, sys: 3.17 ms, total: 18.2 ms\n",
      "Wall time: 14.9 ms\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:04:03.061989Z",
     "start_time": "2025-06-06T11:04:03.038843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "autodiff_grad_fn_large = jax.grad(loss_fn, argnums=3)\n",
    "%time autodiff_grad_large = autodiff_grad_fn_large(measurements, measurement_basis, params_lambda, params_mu)"
   ],
   "id": "62d55ac88eaf19b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 ms, sys: 3.81 ms, total: 25 ms\n",
      "Wall time: 19.8 ms\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:04:03.959418Z",
     "start_time": "2025-06-06T11:04:03.953202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print out gradients of all three larger ones\n",
    "print(\"Explicit Grad (mine) :\", explicit_grad_large)\n",
    "print(\"Autodiff Loss (mine) :\", autodiff_grad_large)\n",
    "print(\"Autodiff Loss (paper):\", autodiff_grad_paper_large)\n",
    "\n",
    "# check pairwise closeness\n",
    "abs_tol = 1e-1\n",
    "print(f\"Within tol. {abs_tol} (explicit mine vs autodiff paper):\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=abs_tol), explicit_grad_large, autodiff_grad_paper_large)))\n",
    "print(f\"Within tol. {abs_tol} (explicit mine vs autodiff mine):\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=abs_tol), explicit_grad_large, autodiff_grad_large)))"
   ],
   "id": "e1b8bb8e2026a4f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicit Grad (mine) : {'W': Array([-10.01608562, -0.00000151,  0.01833252, -6.00497437, -0.00000123,\n",
      "       -4.73766327,  0.02894065,  0.00000006, -0.00000123, -0.00000151],      dtype=float32), 'b': Array(-0.00004204, dtype=float32)}\n",
      "Autodiff Loss (mine) : {'W': Array([-9.99688721,  0.00006104,  0.01806641, -5.99365234,  0.00006104,\n",
      "       -4.72842407,  0.02926636,  0.00003052,  0.00006104,  0.00006104],      dtype=float32), 'b': Array( 0.00006294, dtype=float32)}\n",
      "Autodiff Loss (paper): {'W': Array([-10.00207520, -0.00018311,  0.01821899, -5.99688721, -0.00009155,\n",
      "       -4.73104858,  0.02920532, -0.00021362, -0.00009155, -0.00018311],      dtype=float32), 'b': Array(-0.00012445, dtype=float32)}\n",
      "Within tol. 0.1 (explicit mine vs autodiff paper): True\n",
      "Within tol. 0.1 (explicit mine vs autodiff mine): True\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T22:49:41.240444Z",
     "start_time": "2025-06-06T22:49:40.558821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### GRADIENTS FROM AUTODIFF OF LOSS FUNCTION\n",
    "\n",
    "def rotated_log_probs(U_row: jnp.ndarray, F_lambda: jnp.ndarray, F_mu: jnp.ndarray) -> jnp.ndarray:\n",
    "    exponent = -0.5 * F_lambda - 0.5j * F_mu                     # (2**n,)\n",
    "    values = U_row * jnp.exp(exponent)                           # Complex vector\n",
    "\n",
    "    abs_vals = jnp.abs(values)\n",
    "    max_log = jnp.max(jnp.log(abs_vals + 1e-30))                 # Scalar for stability\n",
    "    scaled_values = values * jnp.exp(-max_log)                   # Scale down before summing\n",
    "\n",
    "    return 2 * (max_log + jnp.log(jnp.abs(jnp.sum(scaled_values)) + 1e-30))\n",
    "\n",
    "\n",
    "def single_measurement_log_prob(\n",
    "        measurement: jnp.ndarray,\n",
    "        rotation_matrix: jnp.ndarray,\n",
    "        F_lambda: jnp.ndarray,\n",
    "        F_mu: jnp.ndarray\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Compute ln|∑_σ U_b(σ^{[b]},σ) e^{-½Fλ(σ) - i/2 Fμ(σ)}|^2\n",
    "    for a single bitstring 'measurement'.\n",
    "    \"\"\"\n",
    "    idx = bitstring_to_int(measurement)                          # int index in [0,2**n)\n",
    "    U_row = rotation_matrix[idx]                                 # (2**n,)\n",
    "    return rotated_log_probs(U_row, F_lambda, F_mu)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "        measurements: jnp.ndarray,\n",
    "        measurement_basis: tuple[int, ...],\n",
    "        params_lambda: dict,\n",
    "        params_mu: dict\n",
    ") -> jnp.ndarray:\n",
    "    # build all 2^n basis vectors and their free energies\n",
    "    n = measurements.shape[1]\n",
    "    comp_basis = get_computational_basis_vectors(n)              # (2**n, n)\n",
    "    F_lambda = jax.vmap(dummy_free_energy, (0, None))(comp_basis, params_lambda)  # (2**n,)\n",
    "    F_mu     = jax.vmap(dummy_free_energy, (0, None))(comp_basis, params_mu)      # (2**n,)\n",
    "\n",
    "    # rotation for this basis\n",
    "    R = construct_rotation_matrix(measurement_basis)             # (2**n, 2**n)\n",
    "\n",
    "    # per‐measurement log‐probabilities\n",
    "    log_probs = jax.vmap(\n",
    "        lambda m: single_measurement_log_prob(m, R, F_lambda, F_mu)\n",
    "    )(measurements)                                              # (batch,)\n",
    "\n",
    "    return -jnp.mean(log_probs)\n",
    "\n",
    "\n",
    "# get gradient w.r.t. the 'mu' parameters\n",
    "autodiff_grad_fn = jax.grad(loss_fn, argnums=3)\n",
    "autodiff_grad    = autodiff_grad_fn(\n",
    "    measurements, measurement_basis, params_lambda, params_mu\n",
    ")\n",
    "\n",
    "print(\"Autodiff Grad:\", autodiff_grad)"
   ],
   "id": "9cd2f2f6641d2505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodiff Grad: {'W': Array([-9.9958706e+00,  5.7220459e-06,  2.9106140e-02, -3.3208485e+00],      dtype=float32), 'b': Array(1.9073486e-06, dtype=float32)}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f2b8541bfff26011"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
