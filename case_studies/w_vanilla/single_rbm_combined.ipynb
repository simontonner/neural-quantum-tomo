{
 "cells": [
  {
   "cell_type": "code",
   "id": "106a346b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:36:05.956185Z",
     "start_time": "2025-05-06T18:36:05.472083Z"
    }
   },
   "source": [
    "import functools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from jax.random import PRNGKey\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "df2358dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:36:09.022564Z",
     "start_time": "2025-05-06T18:36:09.010871Z"
    }
   },
   "source": [
    "class RBM(nn.Module):\n",
    "    \"\"\"Restricted Boltzmann Machine with CD‑k or PCD‑k training.\"\"\"\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "    k: int = 1  # CD-k / PCD-k\n",
    "\n",
    "    # ─────────────────────── model forward ────────────────────────\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 data_batch: jnp.ndarray,\n",
    "                 v_persistent: jnp.ndarray,\n",
    "                 rng: PRNGKey) -> tuple[jnp.ndarray, dict]:\n",
    "        W = self.param(\"W\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        b = self.param(\"b\", nn.initializers.zeros,         (self.n_visible,))\n",
    "        c = self.param(\"c\", nn.initializers.zeros,         (self.n_hidden,))\n",
    "        params = {\"W\": W, \"b\": b, \"c\": c}\n",
    "\n",
    "        # ── positive & negative phases ────────────────────────────\n",
    "        v_k, key = self._gibbs_sample(params, v_persistent, rng, k=self.k)\n",
    "        v_k = jax.lax.stop_gradient(v_k)\n",
    "\n",
    "        free_e_data  = self._free_energy(params, data_batch)\n",
    "        free_e_model = self._free_energy(params, v_k)\n",
    "\n",
    "        pcd_loss = jnp.mean(free_e_data) - jnp.mean(free_e_model)\n",
    "        aux      = {\"v_persistent\": v_k, \"key\": key}\n",
    "        return pcd_loss, aux\n",
    "\n",
    "    # ─────────────────────── statics ──────────────────────────────\n",
    "    @staticmethod\n",
    "    def _free_energy(params, v):\n",
    "        W, b, c   = params[\"W\"], params[\"b\"], params[\"c\"]\n",
    "        v_term    = jnp.dot(v, b)\n",
    "        h_term    = jnp.sum(jax.nn.softplus(v @ W + c), axis=-1)\n",
    "        return -(v_term + h_term)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(_, state, params, T=1.0):\n",
    "        v, key      = state\n",
    "        W, b, c     = params[\"W\"], params[\"b\"], params[\"c\"]\n",
    "        key, hk, vk = jax.random.split(key, 3)\n",
    "\n",
    "        h_prob = jax.nn.sigmoid((v @ W + c) / T)\n",
    "        h      = jax.random.bernoulli(hk, h_prob).astype(jnp.float32)\n",
    "\n",
    "        v_prob = jax.nn.sigmoid((h @ W.T + b) / T)\n",
    "        v      = jax.random.bernoulli(vk, v_prob).astype(jnp.float32)\n",
    "        return v, key\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_sample(params, v0, rng, k=1, T=1.0):\n",
    "        body = lambda i, st: RBM._gibbs_step(i, st, params, T)\n",
    "        v_k, key = jax.lax.fori_loop(0, k, body, (v0, rng))\n",
    "        return v_k, key\n",
    "\n",
    "    # ────────────────────── sampling util ─────────────────────────\n",
    "    @nn.nowrap\n",
    "    def generate(self,\n",
    "                 params: dict,\n",
    "                 n_samples: int,\n",
    "                 T_schedule: jnp.ndarray,\n",
    "                 rng: PRNGKey) -> jnp.ndarray:\n",
    "        rng, key = jax.random.split(rng)\n",
    "        v = jax.random.bernoulli(key, p=0.5,\n",
    "                                 shape=(n_samples, self.n_visible)).astype(jnp.float32)\n",
    "        state = (v, rng)\n",
    "\n",
    "        step = lambda i, st: RBM._gibbs_step(i, st, params, T_schedule[i])\n",
    "        v_fin, _ = jax.lax.fori_loop(0, len(T_schedule), step, state)\n",
    "        return v_fin\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "769aa6b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:36:12.256794Z",
     "start_time": "2025-05-06T18:36:12.248540Z"
    }
   },
   "source": [
    "class RBMTrainState(train_state.TrainState):\n",
    "    \"\"\"Bundles params and opt-state (Flax API).\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit)\n",
    "def train_step(state: RBMTrainState,\n",
    "               data_batch: jnp.ndarray,\n",
    "               v_persistent: jnp.ndarray,\n",
    "               key: PRNGKey):\n",
    "    pcd_loss_fn = lambda params: state.apply_fn(\n",
    "        {\"params\": params}, data_batch, v_persistent, key)\n",
    "\n",
    "    (loss, aux), grads = jax.value_and_grad(pcd_loss_fn,\n",
    "                                            has_aux=True)(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss, aux[\"v_persistent\"], aux[\"key\"]\n",
    "\n",
    "\n",
    "def train_rbm(state: RBMTrainState,\n",
    "              train_loader,\n",
    "              num_epochs: int,\n",
    "              rng: PRNGKey,\n",
    "              pcd_reset: int = 5,\n",
    "              scheduler=None):\n",
    "    \"\"\"Training loop for the RBM.\"\"\"\n",
    "    metrics = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        # initialize persistent chain once per epoch\n",
    "        rng, sk = jax.random.split(rng)\n",
    "        # shape = (batch_size, n_visible), grab from the first batch\n",
    "        first_batch = next(iter(train_loader))\n",
    "        v_persistent = jax.random.bernoulli(sk, p=0.5,\n",
    "                                            shape=first_batch.shape).astype(jnp.float32)\n",
    "\n",
    "        tot_loss, n_batches = 0.0, 0\n",
    "        for b_idx, data in enumerate(train_loader):\n",
    "            # only reset if pcd_reset is an integer\n",
    "            if (pcd_reset is not None) and (b_idx % pcd_reset == 0):\n",
    "                rng, sk = jax.random.split(rng)\n",
    "                v_persistent = jax.random.bernoulli(sk, p=0.5,\n",
    "                                                    shape=data.shape).astype(jnp.float32)\n",
    "\n",
    "            state, loss, v_persistent, rng = train_step(state,\n",
    "                                                        data,\n",
    "                                                        v_persistent,\n",
    "                                                        rng)\n",
    "            tot_loss += loss\n",
    "            n_batches += 1\n",
    "\n",
    "        avg = tot_loss / n_batches\n",
    "        metrics[epoch] = {\"free_energy_loss\": float(avg)}\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] – FE-loss: {avg:.4f}\")\n",
    "    return state, metrics, rng\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "470ab4e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:36:15.688418Z",
     "start_time": "2025-05-06T18:36:15.683930Z"
    }
   },
   "source": [
    "def get_cosine_schedule(T_high: float,\n",
    "                        T_low: float,\n",
    "                        n_steps: int) -> jnp.ndarray:\n",
    "    \"\"\"Cosine annealed temperature schedule.\"\"\"\n",
    "    steps = jnp.arange(n_steps, dtype=jnp.float32)\n",
    "    cos   = 0.5 * (1 + jnp.cos(jnp.pi * steps / (n_steps - 1)))\n",
    "    return T_low + (T_high - T_low) * cos\n"
   ],
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
