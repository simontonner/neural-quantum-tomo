{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:26:33.601876Z",
     "start_time": "2025-05-20T19:26:33.596418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json # For potentially saving/loading parameters if we had them\n",
    "\n",
    "# --- Configuration (should match the data generation script) ---\n",
    "NUM_QUBITS = 6 # Or whatever NUM_QUBITS you used for data generation\n",
    "RNG_SEED = 42   # Same seed to regenerate the original W-state\n",
    "\n",
    "# --- Define Data Directory (where measurements are stored) ---\n",
    "DATA_DIR = Path(\"./w_aug_tomography_data\") # Must match previous notebook\n",
    "if not DATA_DIR.exists():\n",
    "    print(f\"ERROR: Data directory {DATA_DIR.resolve()} not found!\")\n",
    "    print(\"Please run the data generation notebook first.\")\n",
    "    # You might want to raise an error or stop execution here in a real script\n",
    "else:\n",
    "    print(f\"Reading data from: {DATA_DIR.resolve()}\")\n",
    "\n",
    "# --- Initialize RNG for regenerating the original W-state ---\n",
    "rng_phases_original = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "print(f\"Number of qubits for reconstruction: {NUM_QUBITS}\")"
   ],
   "id": "5547452bd286f641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: /Users/Tonni/Desktop/master-code/neural-quantum-tomo/case_studies/w_phase_augmented/w_aug_tomography_data\n",
      "Number of qubits for reconstruction: 6\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:26:05.525721Z",
     "start_time": "2025-05-20T19:26:05.523206Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "def format_bytes(b):\n",
    "    \"\"\"Utility function to format bytes into KB, MB, GB, etc.\"\"\"\n",
    "    for u in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if abs(b) < 1024.0:\n",
    "            return f\"{b:.2f} {u}\"\n",
    "        b /= 1024.0\n",
    "    return f\"{b:.2f} PB\""
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Regenerating the original {NUM_QUBITS}-qubit phase-augmented W state for comparison...\")\n",
    "state_dim = 1 << NUM_QUBITS\n",
    "w_aug_original = np.zeros(state_dim, dtype=complex)\n",
    "\n",
    "# Generate random phases using the same RNG seed as in data generation\n",
    "thetas_original = rng_phases_original.uniform(0, 2 * np.pi, size=NUM_QUBITS)\n",
    "\n",
    "for k in range(NUM_QUBITS):\n",
    "    idx = 1 << (NUM_QUBITS - 1 - k)\n",
    "    w_aug_original[idx] = np.exp(1j * thetas_original[k]) / np.sqrt(NUM_QUBITS)\n",
    "\n",
    "print(f\"Size of original W state vector: {format_bytes(w_aug_original.nbytes)}\")\n",
    "print(f\"Original W state norm: {np.linalg.norm(w_aug_original):.6f}\")"
   ],
   "id": "e4cf9501f5b076fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:26:50.594309Z",
     "start_time": "2025-05-20T19:26:50.590399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_measurement_bases_strings = []\n",
    "amplitude_basis_str = 'Z' * NUM_QUBITS\n",
    "all_measurement_bases_strings.append(amplitude_basis_str)\n",
    "\n",
    "for i in range(NUM_QUBITS - 1):\n",
    "    basis_list = ['Z'] * NUM_QUBITS; basis_list[i] = 'X'; basis_list[i+1] = 'X'\n",
    "    all_measurement_bases_strings.append(\"\".join(basis_list))\n",
    "for i in range(NUM_QUBITS - 1):\n",
    "    basis_list = ['Z'] * NUM_QUBITS; basis_list[i] = 'X'; basis_list[i+1] = 'Y'\n",
    "    all_measurement_bases_strings.append(\"\".join(basis_list))\n",
    "\n",
    "print(f\"Expecting data for {len(all_measurement_bases_strings)} bases.\")"
   ],
   "id": "9e1ede6ba6c43d21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting data for 11 bases.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:27:16.779299Z",
     "start_time": "2025-05-20T19:27:16.769515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We need to know NUM_SAMPLES_PER_BASIS from the filename if not hardcoded\n",
    "# For simplicity, let's try to infer it from one of the filenames or assume it's known.\n",
    "# This is a bit fragile; ideally, metadata would be saved with the data.\n",
    "\n",
    "# Try to find a file to infer NUM_SAMPLES_PER_BASIS\n",
    "num_samples_inferred = None\n",
    "first_basis_str = all_measurement_bases_strings[0]\n",
    "expected_pattern = f\"w_aug_{first_basis_str}_*.txt\"\n",
    "try:\n",
    "    # Find one file matching the pattern for the first basis\n",
    "    matching_files = list(DATA_DIR.glob(expected_pattern))\n",
    "    if matching_files:\n",
    "        fname_parts = matching_files[0].name.split('_') # e.g., ['w', 'aug', 'ZZZZZZ', '20.txt']\n",
    "        num_samples_inferred = int(fname_parts[-1].replace('.txt', ''))\n",
    "        print(f\"Inferred NUM_SAMPLES_PER_BASIS = {num_samples_inferred} from filenames.\")\n",
    "    else:\n",
    "        print(f\"Warning: Could not find files matching {expected_pattern} to infer NUM_SAMPLES_PER_BASIS.\")\n",
    "        # Fallback or error\n",
    "        num_samples_inferred = 20 # Example: Set a default if not found\n",
    "        print(f\"Using default NUM_SAMPLES_PER_BASIS = {num_samples_inferred}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error inferring NUM_SAMPLES_PER_BASIS: {e}\")\n",
    "    num_samples_inferred = 20 # Fallback\n",
    "    print(f\"Using default NUM_SAMPLES_PER_BASIS = {num_samples_inferred}\")\n",
    "\n",
    "\n",
    "all_datasets = {} # Dictionary to store data: {basis_string: [list_of_measurements]}\n",
    "\n",
    "print(\"\\n--- Ingesting Measurement Data ---\")\n",
    "for basis_str in all_measurement_bases_strings:\n",
    "    # Filename format: w_aug_<basis_string>_<num_samples>.txt\n",
    "    filename = DATA_DIR / f\"w_aug_{basis_str}_{num_samples_inferred}.txt\"\n",
    "    dataset_for_basis = []\n",
    "    if filename.exists():\n",
    "        print(f\"Reading data for basis: {basis_str} from {filename.name}\")\n",
    "        with open(filename, 'r') as f_in:\n",
    "            for line in f_in:\n",
    "                # Measurements are stored as 'XxY...'\n",
    "                dataset_for_basis.append(line.strip())\n",
    "        all_datasets[basis_str] = dataset_for_basis\n",
    "        print(f\"  Read {len(dataset_for_basis)} samples.\")\n",
    "    else:\n",
    "        print(f\"Warning: Measurement file not found for basis {basis_str}: {filename}\")\n",
    "\n",
    "if not all_datasets:\n",
    "    print(\"ERROR: No measurement data was loaded. Cannot proceed.\")\n",
    "else:\n",
    "    print(\"\\n--- Data Ingestion Complete ---\")\n",
    "    print(f\"Loaded data for {len(all_datasets)} bases.\")\n",
    "    # Example: print data for one basis\n",
    "    # example_basis = list(all_datasets.keys())[0]\n",
    "    # print(f\"First 5 samples for basis {example_basis}: {all_datasets[example_basis][:5]}\")"
   ],
   "id": "8dc7546fd878a2d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred NUM_SAMPLES_PER_BASIS = 20 from filenames.\n",
      "\n",
      "--- Ingesting Measurement Data ---\n",
      "Reading data for basis: ZZZZZZ from w_aug_ZZZZZZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: XXZZZZ from w_aug_XXZZZZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZXXZZZ from w_aug_ZXXZZZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZZXXZZ from w_aug_ZZXXZZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZZZXXZ from w_aug_ZZZXXZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZZZZXX from w_aug_ZZZZXX_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: XYZZZZ from w_aug_XYZZZZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZXYZZZ from w_aug_ZXYZZZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZZXYZZ from w_aug_ZZXYZZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZZZXYZ from w_aug_ZZZXYZ_20.txt\n",
      "  Read 20 samples.\n",
      "Reading data for basis: ZZZZXY from w_aug_ZZZZXY_20.txt\n",
      "  Read 20 samples.\n",
      "\n",
      "--- Data Ingestion Complete ---\n",
      "Loaded data for 11 bases.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:31:50.071210Z",
     "start_time": "2025-05-20T19:31:50.022434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Estimating Properties from Measurement Statistics ---\")\n",
    "\n",
    "# 1. Amplitude Distribution from 'ZZ...Z' basis\n",
    "z_basis_str = 'Z' * NUM_QUBITS\n",
    "if z_basis_str in all_datasets:\n",
    "    z_measurements = all_datasets[z_basis_str]\n",
    "\n",
    "    # Convert 'Z'/'z' to binary strings '1'/'0' (or any consistent numeric representation)\n",
    "    # 'Z' (+1 eigenvalue) -> maps to computational |0> if spin-up is |0>\n",
    "    # 'z' (-1 eigenvalue) -> maps to computational |1> if spin-down is |1>\n",
    "    # Let's assume 'Z' means outcome '0' (for +1 of sigma_z) and 'z' means '1' (for -1 of sigma_z)\n",
    "    # This matches the output_bits of sample_state_fast\n",
    "\n",
    "    binary_outcomes_z_basis = []\n",
    "    for m_str in z_measurements:\n",
    "        bits = []\n",
    "        for char_idx, char_val in enumerate(m_str):\n",
    "            if char_val == 'Z': bits.append('0')\n",
    "            elif char_val == 'z': bits.append('1')\n",
    "            else: bits.append('?') # Should not happen for ZZZ...Z basis\n",
    "        binary_outcomes_z_basis.append(\"\".join(bits))\n",
    "\n",
    "    # Count frequencies\n",
    "    from collections import Counter\n",
    "    z_outcome_counts = Counter(binary_outcomes_z_basis)\n",
    "\n",
    "    print(f\"\\n--- Statistics for ZZZ...Z basis ({len(z_measurements)} samples) ---\")\n",
    "    print(f\"Number of unique outcomes observed: {len(z_outcome_counts)}\")\n",
    "\n",
    "    # Expected W-state outcomes (binary representations)\n",
    "    expected_w_binary_outcomes = []\n",
    "    for k in range(NUM_QUBITS):\n",
    "        bits = ['0'] * NUM_QUBITS\n",
    "        bits[k] = '1' # If W-state is sum of |..1..> where 1 is at k-th position (MSB=0)\n",
    "        # And if '1' in binary_outcomes maps to where W-state amp is.\n",
    "        # Original W-state: idx = 1 << (NUM_QUBITS - 1 - k) means 1 at (NUM_QUBITS-1-k) from LSB.\n",
    "        # If binary_outcomes_z_basis[k] is k-th char, and k=0 is MSB.\n",
    "        # A '1' at position `k` (MSB) means index 2^(N-1-k).\n",
    "        expected_w_binary_outcomes.append(\"\".join(bits))\n",
    "\n",
    "    # This mapping needs to be careful:\n",
    "    # Our w_aug_initial[idx] = value, where idx = 1 << (NUM_QUBITS - 1 - k_msb_pos_of_1)\n",
    "    # A binary string \"b0b1b2...\" (b0 is MSB) corresponds to int(val, 2)\n",
    "    # If W-state is |100> + |010> + |001>\n",
    "    # k_msb_pos_of_1 = 0 -> \"100...\" -> index 2^(N-1)\n",
    "    # k_msb_pos_of_1 = 1 -> \"010...\" -> index 2^(N-2)\n",
    "    # So, the expected binary strings are those with a single '1'.\n",
    "\n",
    "    print(\"Top 10 most frequent outcomes in Z-basis:\")\n",
    "    total_z_samples = len(z_measurements)\n",
    "    for outcome, count in z_outcome_counts.most_common(10):\n",
    "        is_w_component = (outcome.count('1') == 1) # Simple check for W-state like components\n",
    "        prob = count / total_z_samples\n",
    "        print(f\"  Outcome: {outcome} (W-like: {is_w_component}) - Count: {count} (Prob: {prob:.4f})\")\n",
    "\n",
    "    # Check sum of probabilities for expected W-components\n",
    "    prob_sum_w_components = 0\n",
    "    for k in range(NUM_QUBITS):\n",
    "        # Construct the binary string for |...1 (at k_th MSB)...>\n",
    "        temp_bits = ['0'] * NUM_QUBITS\n",
    "        temp_bits[k] = '1'\n",
    "        w_outcome_k_str = \"\".join(temp_bits)\n",
    "        prob_sum_w_components += z_outcome_counts.get(w_outcome_k_str, 0) / total_z_samples\n",
    "    print(f\"Sum of probabilities for single '1' (W-like) components in Z-basis: {prob_sum_w_components:.4f} (expected close to 1.0)\")\n",
    "\n",
    "else:\n",
    "    print(\"Warning: Data for ZZZ...Z basis not found in datasets.\")\n",
    "\n",
    "\n",
    "# 2. Two-qubit correlators\n",
    "# Example: <sigma_0^X sigma_1^X> from basis \"XXZ...Z\"\n",
    "xx_basis_str_list = ['Z'] * NUM_QUBITS\n",
    "if NUM_QUBITS >= 2:\n",
    "    xx_basis_str_list[0] = 'X'\n",
    "    xx_basis_str_list[1] = 'X'\n",
    "    xx_basis_str = \"\".join(xx_basis_str_list)\n",
    "\n",
    "    if xx_basis_str in all_datasets:\n",
    "        xx_measurements = all_datasets[xx_basis_str]\n",
    "        count_XX = 0; count_Xx = 0; count_xX = 0; count_xx = 0\n",
    "        for m_str in xx_measurements:\n",
    "            q0_outcome = m_str[0] # Assuming first char is for qubit 0\n",
    "            q1_outcome = m_str[1] # Assuming second char is for qubit 1\n",
    "\n",
    "            if q0_outcome == 'X' and q1_outcome == 'X': count_XX += 1\n",
    "            elif q0_outcome == 'X' and q1_outcome == 'x': count_Xx += 1\n",
    "            elif q0_outcome == 'x' and q1_outcome == 'X': count_xX += 1\n",
    "            elif q0_outcome == 'x' and q1_outcome == 'x': count_xx += 1\n",
    "\n",
    "        if len(xx_measurements) > 0:\n",
    "            corr_X0X1 = (count_XX - count_Xx - count_xX + count_xx) / len(xx_measurements)\n",
    "            print(f\"\\n--- Statistics for {xx_basis_str} basis ({len(xx_measurements)} samples) ---\")\n",
    "            print(f\"  Counts for (q0, q1): XX={count_XX}, Xx={count_Xx}, xX={count_xX}, xx={count_xx}\")\n",
    "            print(f\"  Estimated <sigma_0^X sigma_1^X>: {corr_X0X1:.4f}\")\n",
    "        else:\n",
    "            print(f\"Warning: No samples found for {xx_basis_str} basis, though key exists.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Warning: Data for {xx_basis_str} basis not found.\")\n",
    "else:\n",
    "    print(\"Skipping two-qubit correlator example (NUM_QUBITS < 2).\")\n",
    "\n",
    "# Example: <sigma_0^X sigma_1^Y> from basis \"XYZ...Z\"\n",
    "xy_basis_str_list = ['Z'] * NUM_QUBITS\n",
    "if NUM_QUBITS >= 2:\n",
    "    xy_basis_str_list[0] = 'X'\n",
    "    xy_basis_str_list[1] = 'Y'\n",
    "    xy_basis_str = \"\".join(xy_basis_str_list)\n",
    "\n",
    "    if xy_basis_str in all_datasets:\n",
    "        xy_measurements = all_datasets[xy_basis_str]\n",
    "        # For <X0 Y1>:\n",
    "        # XY (+1,+1) -> +1 contribution to correlator\n",
    "        # Xy (+1,-1) -> -1\n",
    "        # xY (-1,+1) -> -1\n",
    "        # xy (-1,-1) -> +1\n",
    "        count_XY = 0; count_Xy = 0; count_xY = 0; count_xy = 0\n",
    "        for m_str in xy_measurements:\n",
    "            q0_outcome = m_str[0]\n",
    "            q1_outcome = m_str[1]\n",
    "\n",
    "            if q0_outcome == 'X' and q1_outcome == 'Y': count_XY += 1\n",
    "            elif q0_outcome == 'X' and q1_outcome == 'y': count_Xy += 1\n",
    "            elif q0_outcome == 'x' and q1_outcome == 'Y': count_xY += 1\n",
    "            elif q0_outcome == 'x' and q1_outcome == 'y': count_xy += 1\n",
    "\n",
    "        if len(xy_measurements) > 0:\n",
    "            corr_X0Y1 = (count_XY - count_Xy - count_xY + count_xy) / len(xy_measurements)\n",
    "            print(f\"\\n--- Statistics for {xy_basis_str} basis ({len(xy_measurements)} samples) ---\")\n",
    "            print(f\"  Counts for (q0, q1): XY={count_XY}, Xy={count_Xy}, xY={count_xY}, xy={count_xy}\")\n",
    "            print(f\"  Estimated <sigma_0^X sigma_1^Y>: {corr_X0Y1:.4f}\")\n",
    "        else:\n",
    "            print(f\"Warning: No samples found for {xy_basis_str} basis, though key exists.\")\n",
    "    else:\n",
    "        print(f\"Warning: Data for {xy_basis_str} basis not found.\")"
   ],
   "id": "bab9eb18ba87b1e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Estimating Properties from Measurement Statistics ---\n",
      "\n",
      "--- Statistics for ZZZ...Z basis (20 samples) ---\n",
      "Number of unique outcomes observed: 1\n",
      "Top 10 most frequent outcomes in Z-basis:\n",
      "  Outcome: ?????? (W-like: False) - Count: 20 (Prob: 1.0000)\n",
      "Sum of probabilities for single '1' (W-like) components in Z-basis: 0.0000 (expected close to 1.0)\n",
      "\n",
      "--- Statistics for XXZZZZ basis (20 samples) ---\n",
      "  Counts for (q0, q1): XX=7, Xx=2, xX=6, xx=5\n",
      "  Estimated <sigma_0^X sigma_1^X>: 0.2000\n",
      "\n",
      "--- Statistics for XYZZZZ basis (20 samples) ---\n",
      "  Counts for (q0, q1): XY=0, Xy=0, xY=0, xy=0\n",
      "  Estimated <sigma_0^X sigma_1^Y>: 0.0000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "23e249550971d159"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
