{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T16:31:51.148642Z",
     "start_time": "2025-06-09T16:31:51.117363Z"
    }
   },
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib.basis import get_computational_basis_vectors, construct_rotation_matrix\n",
    "from lib.formatting import bitstring_to_int\n",
    "\n",
    "####\n",
    "\n",
    "import jax\n",
    "from jax.nn import softmax\n",
    "from jax import tree\n",
    "\n",
    "import jax.numpy as jnp\n",
    "jnp.set_printoptions(suppress=True, formatter={'float_kind': '{: .8f}'.format})"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:32:31.852532Z",
     "start_time": "2025-06-09T16:32:31.824407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### DUMMY FREE ENERGY AND PYTREE HELPERS\n",
    "\n",
    "def dummy_free_energy(sigma: jnp.ndarray, params: dict) -> jnp.ndarray:\n",
    "    return jnp.dot(sigma, params[\"W\"]) + params[\"b\"]  # (2**n,)\n",
    "\n",
    "def multiply_with_leaf(factor, leaf):\n",
    "    if leaf.ndim == 1:\n",
    "        return factor * leaf\n",
    "    else:\n",
    "        return factor[:, None] * leaf\n",
    "\n",
    "\n",
    "#### TEST VARIABLES\n",
    "\n",
    "basis = jnp.array([1, 0, 2, 1, 0, 1, 2, 0, 0, 0])  # X Z Y X Z Y X Z Z Z\n",
    "measurements = jnp.array([\n",
    "    [0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1],\n",
    "    [0,1,0,1,0,1,0,1,0,1],\n",
    "    [1,0,1,0,1,0,1,0,1,0],\n",
    "    [0,0,1,1,0,0,1,1,0,0],\n",
    "    [1,1,0,0,1,1,0,0,1,1],\n",
    "], dtype=jnp.int32)\n",
    "\n",
    "basis_local = jnp.array([0, 0, 0, 0, 0, 1, 2, 0, 0, 0])  # Z Z Z Z Z X Y Z Z Z Z\n",
    "measurements_local = jnp.array([\n",
    "    [0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1],\n",
    "    [0,1,0,1,0,1,0,1,0,1],\n",
    "    [1,0,1,0,1,0,1,0,1,0],\n",
    "    [0,0,1,1,0,0,1,1,0,0],\n",
    "    [1,1,0,0,1,1,0,0,1,1],\n",
    "], dtype=jnp.int32)\n",
    "\n",
    "params_lambda = {\n",
    "    \"W\": jnp.linspace(0.05, 0.15, 10, dtype=jnp.float32),\n",
    "    \"b\": jnp.array(0.3, dtype=jnp.float32)\n",
    "}\n",
    "\n",
    "params_mu = {\n",
    "    \"W\": jnp.linspace(0.05, 0.15, 10, dtype=jnp.float32),\n",
    "    \"b\": jnp.array(0.3, dtype=jnp.float32)\n",
    "}"
   ],
   "id": "16f614cd7740125a",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:31:53.600918Z",
     "start_time": "2025-06-09T16:31:53.542425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### GRADIENTS USING AUTODIFF OF MY DERIVATION (STUPID VERSION)\n",
    "\n",
    "\n",
    "def rotated_log_prob_vanilla(rotation_weights, free_energy_lambda, free_energy_mu):\n",
    "    computational_amplitudes = jnp.exp(-0.5 * free_energy_lambda) * jnp.exp(-0.5j * free_energy_mu) # unnormalized\n",
    "    rotated_amplitude = jnp.vdot(rotation_weights, computational_amplitudes)\n",
    "    rotated_log_prob = jnp.log(jnp.abs(rotated_amplitude) ** 2 + 1e-30)\n",
    "    return rotated_log_prob\n",
    "\n",
    "\n",
    "def loss_fn_mine_vanilla(measurements, basis, params_lambda, params_mu):\n",
    "\n",
    "    # get the free energies for all computational basis vectors to construct the full state vector\n",
    "    computational_basis_vectors = get_computational_basis_vectors(measurements.shape[1])\n",
    "    free_energy_lambda = dummy_free_energy(computational_basis_vectors, params_lambda)\n",
    "    free_energy_mu = dummy_free_energy(computational_basis_vectors, params_mu)\n",
    "\n",
    "    rotation_matrix = construct_rotation_matrix(basis)          # (2**n, 2**n)\n",
    "\n",
    "    get_log_prob = lambda m: rotated_log_prob_vanilla(rotation_matrix[bitstring_to_int(m)], free_energy_lambda, free_energy_mu)\n",
    "    log_probs = jax.vmap(get_log_prob)(measurements)\n",
    "\n",
    "    loss = -jnp.mean(log_probs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "autodiff_grad_fn_mine = jax.grad(loss_fn_mine_vanilla, argnums=3)\n",
    "\n",
    "autodiff_grad_mine = autodiff_grad_fn_mine(measurements, basis, params_lambda, params_mu)\n",
    "autodiff_grad_mine"
   ],
   "id": "6be1c19313f3ba3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': Array([-9.99774170, -0.00006104,  0.01800537, -5.99432373,  0.00003052,\n",
       "        -4.72903442,  0.02932739,  0.00006104,  0.00003052, -0.00006104],      dtype=float32),\n",
       " 'b': Array( 0.00002766, dtype=float32)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:31:57.021955Z",
     "start_time": "2025-06-09T16:31:56.970289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### GRADIENTS USING AUTODIFF OF MY DERIVATION (STABLE VERSION FOR FISHER)\n",
    "\n",
    "\n",
    "def rotated_log_prob_stable(rotation_weights, free_energy_lambda, free_energy_mu):\n",
    "    computational_amplitudes = jnp.exp(-0.5 * free_energy_lambda) * jnp.exp(-0.5j * free_energy_mu)\n",
    "    rotated_amplitude_contributions = rotation_weights * computational_amplitudes\n",
    "\n",
    "    # instead of subtracting the max_log in the exponent we divide by exp(max_log)\n",
    "    max_log = jnp.max(jnp.log(jnp.abs(rotated_amplitude_contributions) + 1e-30))\n",
    "    scaled_exp = rotated_amplitude_contributions * jnp.exp(-max_log)\n",
    "\n",
    "    log_sum_exp = max_log + jnp.log(jnp.abs(jnp.sum(scaled_exp)) + 1e-30)\n",
    "    return 2 * log_sum_exp\n",
    "\n",
    "\n",
    "def loss_fn_mine(measurement, basis, params_lambda, params_mu):\n",
    "\n",
    "    # get the free energies for all computational basis vectors to construct the full state vector\n",
    "    computational_basis_vectors = get_computational_basis_vectors(measurement.shape[0])\n",
    "    free_energy_lambda = dummy_free_energy(computational_basis_vectors, params_lambda)\n",
    "    free_energy_mu = dummy_free_energy(computational_basis_vectors, params_mu)\n",
    "\n",
    "    rotation_matrix = construct_rotation_matrix(basis)          # (2**n, 2**n)\n",
    "\n",
    "    # we pick the row corresponding to the rotated amplitude. it contains all the weights for the computational basis amplitudes\n",
    "    rotation_weights = rotation_matrix[bitstring_to_int(measurement)]       # (2**n,)\n",
    "\n",
    "    log_prob = rotated_log_prob_stable(rotation_weights, free_energy_lambda, free_energy_mu)\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "# per sample gradient function (sadly we cannot take batch gradients later for the natural gradient)\n",
    "autodiff_grad_fn_mine = jax.grad(loss_fn_mine, argnums=3)\n",
    "\n",
    "# use vmap to compute the gradient for all measurements\n",
    "autodiff_grads_mine = jax.vmap(autodiff_grad_fn_mine, in_axes=(0, None, None, None))(measurements, basis, params_lambda, params_mu)\n",
    "\n",
    "# since we get a bunch of pytrees, we need to use the tree map to calculate the mean over the batch\n",
    "autodiff_grad_mine = tree.map(lambda x: -jnp.mean(x, axis=0), autodiff_grads_mine)\n",
    "autodiff_grad_mine"
   ],
   "id": "3daec8f041b7be6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': Array([-9.99703217, -0.00019717,  0.01772640, -5.99372959, -0.00019697,\n",
       "        -4.72853422,  0.02901845, -0.00020343, -0.00019697, -0.00019717],      dtype=float32),\n",
       " 'b': Array(-0.00019916, dtype=float32)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:32:00.331646Z",
     "start_time": "2025-06-09T16:32:00.288934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### EXPLICIT GRADIENTS FROM MY DERIVATION\n",
    "\n",
    "\n",
    "def grad_fn_mine(measurements, basis, params_lambda, params_mu):\n",
    "    computational_basis_vectors = get_computational_basis_vectors(measurements.shape[1])\n",
    "\n",
    "    free_energy_lambda = jax.vmap(dummy_free_energy, (0, None))(computational_basis_vectors, params_lambda)\n",
    "    free_energy_mu = jax.vmap(dummy_free_energy, (0, None))(computational_basis_vectors, params_mu)\n",
    "    free_energy_mu_grads = jax.vmap(lambda s: jax.grad(lambda p: dummy_free_energy(s, p))(params_mu))(computational_basis_vectors)\n",
    "\n",
    "    rotation_matrix = construct_rotation_matrix(basis)\n",
    "\n",
    "    def per_sample(measurement):\n",
    "        idx = bitstring_to_int(measurement)\n",
    "        rotated_exponent = jnp.log(rotation_matrix[idx]) - 0.5 * free_energy_lambda - 0.5j * free_energy_mu\n",
    "        rotated_exponent = jnp.where(jnp.isfinite(rotated_exponent), rotated_exponent, -1e30 + 0j)\n",
    "        gradient_weights = jnp.imag(softmax(rotated_exponent))\n",
    "\n",
    "        def apply_to_leaf(leaf):\n",
    "            if leaf.ndim == 1:             # bias gradient\n",
    "                return -jnp.sum(gradient_weights * leaf)\n",
    "            else:                          # weight gradient\n",
    "                return -jnp.sum(gradient_weights[:, None] * leaf, axis=0)\n",
    "\n",
    "        return tree.map(apply_to_leaf, free_energy_mu_grads)\n",
    "\n",
    "    grads_batch = jax.vmap(per_sample)(measurements)         # stacked pytree\n",
    "    return tree.map(lambda x: jnp.mean(x, 0), grads_batch)\n",
    "\n",
    "\n",
    "explicit_grad_mine = grad_fn_mine(measurements, basis, params_lambda, params_mu)\n",
    "\n",
    "print(\"explicit mine:\", explicit_grad_mine)\n",
    "print(\"autodiff mine:\", autodiff_grad_mine)\n",
    "\n",
    "abs_tol = 1e-1\n",
    "print(f\"Within tol. {abs_tol} (explicit mine vs autodiff mine):\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=abs_tol), explicit_grad_mine, autodiff_grad_mine)))"
   ],
   "id": "2e9da81002314183",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explicit mine: {'W': Array([-9.98472214,  0.00001884,  0.01825257, -5.98650360,  0.00001876,\n",
      "       -4.72285175,  0.02916548,  0.00002033,  0.00001876,  0.00001884],      dtype=float32), 'b': Array(-0.00004276, dtype=float32)}\n",
      "autodiff mine: {'W': Array([-9.99703217, -0.00019717,  0.01772640, -5.99372959, -0.00019697,\n",
      "       -4.72853422,  0.02901845, -0.00020343, -0.00019697, -0.00019717],      dtype=float32), 'b': Array(-0.00019916, dtype=float32)}\n",
      "Within tol. 0.1 (explicit mine vs autodiff mine): True\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:32:04.679526Z",
     "start_time": "2025-06-09T16:32:04.609891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### GRADIENTS USING AUTODIFF OF PAPER DERIVATION (VERY STUPID 1 TO 1)\n",
    "\n",
    "\n",
    "def p_lambda(sigma, params_lambda):\n",
    "    free_energy_lambda = dummy_free_energy(sigma, params_lambda)\n",
    "    return jnp.exp(-free_energy_lambda)\n",
    "\n",
    "def phi_mu(sigma, params_mu):\n",
    "    free_energy_mu = dummy_free_energy(sigma, params_mu)\n",
    "    return -free_energy_mu\n",
    "\n",
    "def loss_fn_paper(\n",
    "        measurements: jnp.ndarray,\n",
    "        basis: tuple[int, ...],\n",
    "        params_lambda: dict,\n",
    "        params_mu: dict) -> jnp.ndarray:\n",
    "\n",
    "    computational_basis_vectors = get_computational_basis_vectors(measurements.shape[1])  # (2**n, n)\n",
    "\n",
    "    p_lambda_values = jax.vmap(p_lambda, (0, None))(computational_basis_vectors, params_lambda)  # (2**n,)\n",
    "    phi_mu_values = jax.vmap(phi_mu, (0, None))(computational_basis_vectors, params_mu)  # (2**n,)\n",
    "\n",
    "    rotation_matrix = construct_rotation_matrix(basis)  # (2**n, 2**n)\n",
    "\n",
    "    def get_log_probability(measurement):\n",
    "        idx = bitstring_to_int(measurement)\n",
    "        rotation_vector = rotation_matrix[idx]\n",
    "\n",
    "        sqrt_p_lambda = jnp.sqrt(p_lambda_values)\n",
    "        exp_phi_mu = jnp.exp(1j * phi_mu_values / 2)\n",
    "\n",
    "        rotated_amp = jnp.vdot(rotation_vector, sqrt_p_lambda * exp_phi_mu)\n",
    "\n",
    "        log_probability = jnp.log(jnp.abs(rotated_amp))\n",
    "        return log_probability\n",
    "\n",
    "    log_probs = []\n",
    "    for measurement in measurements:\n",
    "        contribution = get_log_probability(measurement)\n",
    "        log_probs.append(contribution + contribution.conj())\n",
    "\n",
    "    log_probs = jnp.array(log_probs)\n",
    "\n",
    "    loss = -jnp.mean(log_probs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "autodiff_grad_fn_paper = jax.grad(loss_fn_paper, argnums=3)\n",
    "autodiff_grad_paper = autodiff_grad_fn_paper(measurements, basis, params_lambda, params_mu)\n",
    "\n",
    "print(\"autodiff paper:\", autodiff_grad_paper)\n",
    "print(\"autodiff mine :\", autodiff_grad_mine)\n",
    "print(\"explicit mine :\", explicit_grad_mine)\n",
    "\n",
    "abs_tol = 1e-2\n",
    "print(f\"Within tol. {abs_tol} (explicit mine vs autodiff paper):\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=abs_tol), explicit_grad_mine, autodiff_grad_paper)))\n",
    "print(f\"Within tol. {abs_tol} (autodiff mine vs autodiff paper):\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=abs_tol), autodiff_grad_mine, autodiff_grad_paper)))"
   ],
   "id": "a1f46c51a765f1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autodiff paper: {'W': Array([-10.00207520, -0.00018311,  0.01821899, -5.99688721, -0.00009155,\n",
      "       -4.73104858,  0.02920532, -0.00021362, -0.00009155, -0.00018311],      dtype=float32), 'b': Array(-0.00012445, dtype=float32)}\n",
      "autodiff mine : {'W': Array([-9.99703217, -0.00019717,  0.01772640, -5.99372959, -0.00019697,\n",
      "       -4.72853422,  0.02901845, -0.00020343, -0.00019697, -0.00019717],      dtype=float32), 'b': Array(-0.00019916, dtype=float32)}\n",
      "explicit mine : {'W': Array([-9.98472214,  0.00001884,  0.01825257, -5.98650360,  0.00001876,\n",
      "       -4.72285175,  0.02916548,  0.00002033,  0.00001876,  0.00001884],      dtype=float32), 'b': Array(-0.00004276, dtype=float32)}\n",
      "Within tol. 0.01 (explicit mine vs autodiff paper): False\n",
      "Within tol. 0.01 (autodiff mine vs autodiff paper): True\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T17:22:45.126438Z",
     "start_time": "2025-06-09T17:22:45.066535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### GRADIENTS USING THE REDUCED ROTATOR\n",
    "\n",
    "def rotated_log_prob_stable_real(rotation_weights, free_energy_lambda, free_energy_mu):\n",
    "    # weights are stable known values and the phase part is bounded by +-1. We do log_sum_exp only the real part\n",
    "\n",
    "    computational_log_magnitudes = -0.5 * free_energy_lambda\n",
    "    computational_phases = -0.5j * free_energy_mu\n",
    "\n",
    "    max_computational_log_magnitude = jnp.max(computational_log_magnitudes)\n",
    "\n",
    "    scaled_computational_amplitudes = jnp.exp(computational_log_magnitudes - max_computational_log_magnitude + computational_phases)\n",
    "\n",
    "    scaled_measurement_amplitude = jnp.vdot(rotation_weights, scaled_computational_amplitudes)\n",
    "\n",
    "    log_measurement_amplitude = max_computational_log_magnitude + jnp.log(jnp.abs(scaled_measurement_amplitude) + 1e-30)\n",
    "    return 2 * log_measurement_amplitude\n",
    "\n",
    "\n",
    "def loss_fn_local(measurement, basis, params_lambda, params_mu):\n",
    "\n",
    "    local_indices = jnp.array(jnp.nonzero(basis != 0, size=2, fill_value=-1)[0])\n",
    "\n",
    "    # get the free energies for all computational basis vectors to construct the full state vector\n",
    "    local_rotation_matrix = construct_rotation_matrix(basis[local_indices])\n",
    "\n",
    "    # for outcome 00 we pick first row, for 01 second row, etc.\n",
    "    local_rotation_weights = local_rotation_matrix[bitstring_to_int(measurement[local_indices])]\n",
    "\n",
    "    # amplitudes mismatching with our Z measurements are 0. There are only 4 remaining amplitudes with the local variations\n",
    "    local_measurement_combos = jnp.array([[0,0], [0,1], [1,0], [1,1]], dtype=measurement.dtype)\n",
    "    local_computational_basis_vectors = jnp.tile(measurement, (4, 1)).at[:, local_indices].set(local_measurement_combos)  # (4, n)\n",
    "\n",
    "    local_free_energy_lambda = dummy_free_energy(local_computational_basis_vectors, params_lambda)\n",
    "    local_free_energy_mu = dummy_free_energy(local_computational_basis_vectors, params_mu)\n",
    "\n",
    "    rotated_log_prob = rotated_log_prob_stable_real(local_rotation_weights, local_free_energy_lambda, local_free_energy_mu)\n",
    "    return rotated_log_prob\n",
    "\n",
    "\n",
    "autodiff_grad_fn_local = jax.grad(loss_fn_local, argnums=3)"
   ],
   "id": "86f5f2a1fef40774",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T17:22:45.901421Z",
     "start_time": "2025-06-09T17:22:45.820877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### COMPARE WITH SOME LOCAL MEASUREMENTS\n",
    "\n",
    "autodiff_grads_mine = jax.vmap(autodiff_grad_fn_mine, in_axes=(0, None, None, None))(measurements_local, basis_local, params_lambda, params_mu)\n",
    "autodiff_grads_local = jax.vmap(autodiff_grad_fn_local, in_axes=(0, None, None, None))(measurements_local, basis_local, params_lambda, params_mu)\n",
    "\n",
    "autodiff_grad_mine = tree.map(lambda x: -jnp.mean(x, axis=0), autodiff_grads_mine)\n",
    "autodiff_grad_local = tree.map(lambda x: -jnp.mean(x, axis=0), autodiff_grads_local)\n",
    "\n",
    "print(f\"autodiff mine (local): {autodiff_grad_mine}\")\n",
    "print(f\"autodiff local (local): {autodiff_grad_local}\")\n",
    "\n",
    "abs_tol = 1e-6\n",
    "print(f\"Within tol. {abs_tol} (autodiff mine vs autodiff local):\", tree.all(tree.map(lambda a, b: jnp.allclose(a, b, atol=abs_tol), autodiff_grad_mine, autodiff_grad_local)))"
   ],
   "id": "982551fcdd3d1096",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autodiff mine (local): {'W': Array([-0.00000017, -0.00000017, -0.00000017, -0.00000017, -0.00000017,\n",
      "       -4.72804928,  0.02910010, -0.00000017, -0.00000017, -0.00000017],      dtype=float32), 'b': Array(-0.00000019, dtype=float32)}\n",
      "autodiff local (local): {'W': Array([-0.00000031, -0.00000018, -0.00000017, -0.00000004, -0.00000031,\n",
      "       -4.72804594,  0.02910029, -0.00000004, -0.00000031, -0.00000018],      dtype=float32), 'b': Array(-0.00000017, dtype=float32)}\n",
      "Within tol. 1e-06 (autodiff mine vs autodiff local): True\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f00e74fb16470d14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
