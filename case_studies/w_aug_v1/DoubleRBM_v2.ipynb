{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.lax\n",
    "from jax.random import PRNGKey\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "data_dir = \"./data\"\n",
    "device = jax.devices('cpu')[0]\n",
    "\n",
    "print(f\"Data resides in        : {data_dir}\")\n",
    "print(f\"Training model on      : {str(device)}\")"
   ],
   "id": "36ccfe17e6361a11"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Tuple, Dict, Any\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class DoubleRBM(nn.Module):\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "    k: int = 1\n",
    "    n_chains: int = 100\n",
    "\n",
    "    def setup(self):\n",
    "        self.param(\"W_amp\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        self.param(\"b_amp\", nn.initializers.zeros, (self.n_visible,))\n",
    "        self.param(\"c_amp\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "        self.param(\"W_pha\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        self.param(\"b_pha\", nn.initializers.zeros, (self.n_visible,))\n",
    "        self.param(\"c_pha\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "\n",
    "    def __call__(self, data_dict: Dict[str, jnp.ndarray], aux_vars: Dict[str, Any]) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n",
    "        # access aux vars\n",
    "        random_key = aux_vars.get(\"random_key\") # we require some random key to be there\n",
    "        persistent_chains = aux_vars.get(\"v_persistent\", None)\n",
    "\n",
    "        aux_vars = dict()\n",
    "\n",
    "        comp_basis = 'Z' * self.n_visible\n",
    "        if len(data_dict) == 1 and comp_basis in data_dict:\n",
    "            data_batch = data_dict[comp_basis]\n",
    "            pcd_loss, persistent_chains, random_key = self._loss_amp(data_batch, persistent_chains, random_key)\n",
    "\n",
    "            if persistent_chains is not None:\n",
    "                aux_vars[\"v_persistent\"] = persistent_chains\n",
    "\n",
    "            aux_vars[\"random_key\"] = random_key\n",
    "\n",
    "            return pcd_loss, aux_vars\n",
    "\n",
    "        # else throw\n",
    "        raise ValueError(\"Encountered batch of unsupported basis.\")\n",
    "\n",
    "\n",
    "    def _loss_amp(self, data_batch, persistent_chains, random_key):\n",
    "        W = self.params[\"W_amp\"]\n",
    "        b = self.params[\"b_amp\"]\n",
    "        c = self.params[\"c_amp\"]\n",
    "\n",
    "        if persistent_chains is None:\n",
    "            random_key, random_key_bern = jax.random.split(random_key)\n",
    "            chains = jax.random.bernoulli(random_key_bern, p=0.5, shape=(self.n_chains, self.n_visible))\n",
    "            model_batch, random_key = self._gibbs_sample(W, b, c, chains, random_key, k=self.k)\n",
    "        else:\n",
    "            model_batch, random_key = self._gibbs_sample(W, b, c, persistent_chains, random_key, k=self.k)\n",
    "            persistent_chains = model_batch\n",
    "\n",
    "        model_batch = jax.lax.stop_gradient(model_batch)  # stopping gradient tracking before computing the loss\n",
    "\n",
    "        free_energy_data = self._free_energy(W, b, c, data_batch)\n",
    "        free_energy_model = self._free_energy(W, b, c, model_batch)\n",
    "        pcd_loss = jnp.mean(free_energy_data) - jnp.mean(free_energy_model)\n",
    "\n",
    "        return pcd_loss, persistent_chains, random_key\n",
    "\n",
    "    @staticmethod\n",
    "    def _free_energy(W, b, c, v):\n",
    "        visible_term = jnp.dot(v, b)\n",
    "        hidden_term  = jnp.sum(jax.nn.softplus(v @ W + c), axis=-1)\n",
    "        free_energy = -visible_term - hidden_term\n",
    "        return free_energy\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(i, state, W, b, c, T=1.0):\n",
    "        v, key = state\n",
    "\n",
    "        # splitting generates different random numbers for each step, one of them is passed on\n",
    "        key, h_key, v_key = jax.random.split(key, 3)\n",
    "\n",
    "        h_logits = (v @ W + c) / T\n",
    "        h_probs = jax.nn.sigmoid(h_logits)\n",
    "        h = jax.random.bernoulli(h_key, h_probs).astype(jnp.float32)\n",
    "\n",
    "        v_logits = (h @ W.T + b) / T\n",
    "        v_probs = jax.nn.sigmoid(v_logits)\n",
    "        v = jax.random.bernoulli(v_key, v_probs).astype(jnp.float32)\n",
    "        return v, key\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_sample(W, b, c, v_init, rng, k=1, T=1.0):\n",
    "        # the fori_loop enables JIT compilation of loops. It basically unrolls the loop over the fixed length k.\n",
    "\n",
    "        body_fun = lambda i, state: DoubleRBM._gibbs_step(i, state, W, b, c, T)\n",
    "        v_final, key = jax.lax.fori_loop(0, k, body_fun, (v_init, rng))\n",
    "        return v_final, key\n",
    "\n",
    "    @staticmethod\n",
    "    def _annealing_step(i, state, params, T_schedule):\n",
    "        v, rng = state\n",
    "        T = T_schedule[i]\n",
    "        # Perform one Gibbs step using the current temperature T.\n",
    "        # Note: _gibbs_sample already handles k=1 and T\n",
    "        v_next, rng_next = DoubleRBM._gibbs_sample(params, v, rng, k=1, T=T)\n",
    "        return (v_next, rng_next)\n",
    "\n",
    "    # the nowrap attribute basically tells JAX to not do the magic wrapping, which injects the params argument\n",
    "    @nn.nowrap\n",
    "    def generate(self, params: dict, n_samples: int, T_schedule: jnp.ndarray, rng: PRNGKey) -> jnp.ndarray:\n",
    "\n",
    "        # get the initial state and perform initial key splitting\n",
    "        rng, init_key = jax.random.split(rng)\n",
    "        v = jax.random.bernoulli(init_key, p=0.5, shape=(n_samples, self.n_visible)).astype(jnp.float32)\n",
    "        init_state = (v, rng)\n",
    "\n",
    "        body_fun = lambda i, state: DoubleRBM._gibbs_step(i, state, params, T_schedule[i])\n",
    "\n",
    "        # the fori_loop is still required, since this function will also be JIT-compiled\n",
    "        v_final, _ = jax.lax.fori_loop(0, len(T_schedule), body_fun, init_state)\n",
    "        return v_final"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b902bc51b77ed2c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
