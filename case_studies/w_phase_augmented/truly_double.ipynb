{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Any\n",
    "from typing import Optional\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "from jax.nn.initializers import normal, zeros\n",
    "from collections.abc import Callable, Sequence\n",
    "\n",
    "\n",
    "class DoubleRBM(nn.Module):\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "    k: int = 1\n",
    "    n_chains: int = 100\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_amp = self.param(\"W_amp\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        self.b_amp = self.param(\"b_amp\", nn.initializers.zeros, (self.n_visible,))\n",
    "        self.c_amp = self.param(\"c_amp\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "        self.W_pha = self.param(\"W_pha\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        self.b_pha = self.param(\"b_pha\", nn.initializers.zeros, (self.n_visible,))\n",
    "        self.c_pha = self.param(\"c_pha\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "        self.rotators = {\n",
    "            'X': jnp.array([[1, 1], [1, -1]], dtype=jnp.complex64) / jnp.sqrt(2),\n",
    "            'Y': jnp.array([[1, -1j], [1, 1j]], dtype=jnp.complex64) / jnp.sqrt(2),\n",
    "        }\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            data_dict: Dict[str, jnp.ndarray],\n",
    "            aux_vars: Dict[str, Any]) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n",
    "\n",
    "        random_key = aux_vars.get(\"random_key\")\n",
    "        persistent_chains = aux_vars.get(\"v_persistent\", None)\n",
    "\n",
    "        aux_vars = dict()\n",
    "\n",
    "        # amplitude training\n",
    "        comp_basis = 'Z' * self.n_visible\n",
    "        if len(data_dict) == 1 and comp_basis in data_dict:\n",
    "            data_batch = data_dict[comp_basis]\n",
    "            pcd_loss, persistent_chains, random_key = self._loss_amp(data_batch, persistent_chains, random_key)\n",
    "\n",
    "            if persistent_chains is not None:\n",
    "                aux_vars[\"v_persistent\"] = persistent_chains\n",
    "\n",
    "            aux_vars[\"random_key\"] = random_key\n",
    "\n",
    "            return pcd_loss, aux_vars\n",
    "\n",
    "\n",
    "        # phase training\n",
    "        all_two_different = all(sum(b != 'Z' for b in basis) == 2 for basis in data_dict)\n",
    "        if all_two_different:\n",
    "            phase_loss = self._loss_phase(data_dict)\n",
    "            return phase_loss, aux_vars\n",
    "\n",
    "        raise ValueError(\"Encountered batch of unsupported basis.\")\n",
    "\n",
    "\n",
    "    def _loss_amp(self, data_batch, persistent_chains, random_key):\n",
    "        W = self.variables[\"params\"][\"W_amp\"]\n",
    "        b = self.variables[\"params\"][\"b_amp\"]\n",
    "        c = self.variables[\"params\"][\"c_amp\"]\n",
    "\n",
    "        if persistent_chains is None:\n",
    "            random_key, random_key_bern = jax.random.split(random_key)\n",
    "            chains = jax.random.bernoulli(random_key_bern, p=0.5, shape=(self.n_chains, self.n_visible))\n",
    "            model_batch, random_key = self._gibbs_sample(W, b, c, chains, random_key, k=self.k)\n",
    "        else:\n",
    "            model_batch, random_key = self._gibbs_sample(W, b, c, persistent_chains, random_key, k=self.k)\n",
    "            persistent_chains = model_batch\n",
    "\n",
    "        model_batch = jax.lax.stop_gradient(model_batch)  # stopping gradient tracking before computing the loss\n",
    "\n",
    "        # stacking the batches here could maybe yield some performance, not sure though since half the data is gradient detached\n",
    "        free_energy_data = self._free_energy(W, b, c, data_batch)\n",
    "        free_energy_model = self._free_energy(W, b, c, model_batch)\n",
    "        pcd_loss = jnp.mean(free_energy_data) - jnp.mean(free_energy_model)\n",
    "\n",
    "        return pcd_loss, persistent_chains, random_key\n",
    "\n",
    "\n",
    "    def _loss_phase(self, data_dict: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for basis, batch in data_dict.items():  # batch: shape (B, n)\n",
    "            amps = self.get_rotated_amplitude(batch, basis)  # shape (B,)\n",
    "            log_probs = jnp.log(jnp.abs(amps) ** 2 + 1e-10)   # shape (B,)\n",
    "            total_loss -= jnp.mean(log_probs)  # NLL\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _free_energy(W, b, c, v):\n",
    "        visible_term = jnp.dot(v, b)\n",
    "        hidden_term  = jnp.sum(jax.nn.softplus(v @ W + c), axis=-1)\n",
    "        free_energy = -visible_term - hidden_term\n",
    "        return free_energy\n",
    "\n",
    "    # amplitude RBM specific\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(i, state, W, b, c, T=1.0):\n",
    "        v, key = state\n",
    "\n",
    "        key, h_key, v_key = jax.random.split(key, 3)\n",
    "\n",
    "        h_logits = (v @ W + c) / T\n",
    "        h_probs = jax.nn.sigmoid(h_logits)\n",
    "        h = jax.random.bernoulli(h_key, h_probs).astype(jnp.float32)\n",
    "\n",
    "        v_logits = (h @ W.T + b) / T\n",
    "        v_probs = jax.nn.sigmoid(v_logits)\n",
    "        v = jax.random.bernoulli(v_key, v_probs).astype(jnp.float32)\n",
    "        return v, key\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_sample(W, b, c, v_init, rng, k=1, T=1.0):\n",
    "        body_fun = lambda i, state: DoubleRBM._gibbs_step(i, state, W, b, c, T)\n",
    "        v_final, key = jax.lax.fori_loop(0, k, body_fun, (v_init, rng))\n",
    "        return v_final, key\n",
    "\n",
    "    # phase RBM specific\n",
    "\n",
    "    def get_amplitude(self, sigma_batch: jnp.ndarray) -> jnp.complex64:\n",
    "        W_amp = self.variables[\"params\"][\"W_amp\"]\n",
    "        b_amp = self.variables[\"params\"][\"b_amp\"]\n",
    "        c_amp = self.variables[\"params\"][\"c_amp\"]\n",
    "\n",
    "        W_pha = self.variables[\"params\"][\"W_pha\"]\n",
    "        b_pha = self.variables[\"params\"][\"b_pha\"]\n",
    "        c_pha = self.variables[\"params\"][\"c_pha\"]\n",
    "\n",
    "        F_amp = self._free_energy(W_amp, b_amp, c_amp, sigma_batch)\n",
    "        F_pha = self._free_energy(W_pha, b_pha, c_pha, sigma_batch)\n",
    "\n",
    "        return jnp.exp(-0.5 * F_amp) * jnp.exp(-0.5j * F_pha)\n",
    "\n",
    "\n",
    "    def get_rotated_amplitude(self, sigma_b: jnp.ndarray, basis: Sequence[str]) -> jnp.ndarray:\n",
    "        B, n = sigma_b.shape\n",
    "\n",
    "        non_z = [i for i, b in enumerate(basis) if b != 'Z']\n",
    "        if len(non_z) != 2:\n",
    "            raise ValueError(\"Only bases with exactly two non-Z entries are supported.\")\n",
    "\n",
    "        j, k = non_z\n",
    "        Rj = self.rotators[basis[j]]\n",
    "        Rk = self.rotators[basis[k]]\n",
    "        U = jnp.kron(Rj, Rk)  # shape (4, 4)\n",
    "\n",
    "        # 4 local bit combinations to insert at positions j and k\n",
    "        local_bit_combos = jnp.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=sigma_b.dtype)\n",
    "\n",
    "        # Expand: (B, n) → (4, B, n) with 4 modified versions per sample\n",
    "        sigma_b_tiled = jnp.repeat(sigma_b[None, :, :], 4, axis=0)  # (4, B, n)\n",
    "        sigma_b_modified = sigma_b_tiled.at[:, :, [j, k]].set(local_bit_combos[:, None, :])  # (4, B, n)\n",
    "\n",
    "        # Reshape to (4B, n)\n",
    "        sigma_b_flat = sigma_b_modified.transpose(1, 0, 2).reshape(4 * B, n)\n",
    "\n",
    "        # Single call to get_amplitudes\n",
    "        psis_flat = self.get_amplitudes(sigma_b_flat)  # shape (4B,)\n",
    "        psis = psis_flat.reshape(B, 4)  # shape (B, 4)\n",
    "\n",
    "        # Compute index into rotated basis\n",
    "        idx_in = (sigma_b[:, j].astype(int) << 1) | sigma_b[:, k].astype(int)  # shape (B,)\n",
    "\n",
    "        # Apply U[:, idx_in] ⋅ psis per batch entry\n",
    "        U_selected = U[:, idx_in]  # shape (4, B)\n",
    "        amp = jnp.einsum(\"bi,ib->b\", psis, U_selected)  # shape (B,)\n",
    "\n",
    "        return amp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
