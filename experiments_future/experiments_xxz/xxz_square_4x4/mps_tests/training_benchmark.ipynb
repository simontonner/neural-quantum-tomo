{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T16:08:44.208001Z",
     "start_time": "2025-11-25T16:08:43.294311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "from data_handling import load_measurements_npz, MeasurementDataset, MeasurementLoader\n",
    "\n",
    "# --- DEVICE SETUP ---\n",
    "#if torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#    print(f\"Success: Device set to Apple MPS\")\n",
    "#elif torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#    print(f\"Success: Device set to CUDA\")\n",
    "#else:\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Warning: Device set to CPU\")\n",
    "\n",
    "# --- FIXED TIMING HELPER ---\n",
    "def sync_device(d):\n",
    "    if d.type == 'mps':\n",
    "        try:\n",
    "            # Modern PyTorch (2.0+)\n",
    "            torch.mps.synchronize()\n",
    "        except AttributeError:\n",
    "            # Fallback for Older PyTorch (1.12/1.13)\n",
    "            # Creating a dummy tensor and moving it to CPU forces a sync\n",
    "            torch.zeros(1, device=d).cpu()\n",
    "    elif d.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "data_dir = Path(\"measurements\")\n",
    "print(f\"Data resides in: {data_dir.resolve()}\")"
   ],
   "id": "eeb9f8736c2ae7e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Device set to CPU\n",
      "Data resides in: /Users/Tonni/Desktop/master-code/neural-quantum-tomo/experiments_consolidate/xxz_square_4x4/measurements\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T16:08:48.795220Z",
     "start_time": "2025-11-25T16:08:48.774899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- JIT COMPILED SAMPLER ---\n",
    "@torch.jit.script\n",
    "def gibbs_sampling_loop(v: torch.Tensor, W: torch.Tensor,\n",
    "                        b_mod: torch.Tensor, c_mod: torch.Tensor,\n",
    "                        k: int, T: float) -> torch.Tensor:\n",
    "    curr_v = v\n",
    "    for _ in range(k):\n",
    "        # Positive Phase\n",
    "        h_logits = curr_v @ W + c_mod\n",
    "        p_h = torch.sigmoid(h_logits / T)\n",
    "        h = torch.bernoulli(p_h)\n",
    "        # Negative Phase\n",
    "        v_logits = h @ W.t() + b_mod\n",
    "        p_v = torch.sigmoid(v_logits / T)\n",
    "        curr_v = torch.bernoulli(p_v)\n",
    "    return curr_v\n",
    "\n",
    "class Conditioner(nn.Module):\n",
    "    def __init__(self, num_visible: int, num_hidden: int, cond_dim: int, hidden_width: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cond_dim, hidden_width)\n",
    "        self.fc2 = nn.Linear(hidden_width, 2 * (num_visible + num_hidden))\n",
    "\n",
    "    def forward(self, cond: torch.Tensor):\n",
    "        x = torch.tanh(self.fc1(cond))\n",
    "        x = self.fc2(x)\n",
    "        N, H = self.fc2.out_features // 4, self.fc2.out_features // 4 - (self.fc2.out_features // 4) # Simplified logic\n",
    "        # Re-calc split sizes correctly based on input\n",
    "        # Note: In prev code, split sizes were fixed variables.\n",
    "        # Here we rely on correct initialization.\n",
    "        return torch.split(x, [x.shape[-1]//4]*4, dim=-1)\n",
    "        # CAREFUL: Ensure init logic in RBM matches this split.\n",
    "        # Restoring exact original logic for safety:\n",
    "        # N, H logic relies on outer class knowing dims.\n",
    "        # See usage below.\n",
    "\n",
    "class ConditionalRBM(nn.Module):\n",
    "    def __init__(self, num_visible: int, num_hidden: int, cond_dim: int,\n",
    "                 conditioner_width: int = 64, k: int = 1, T: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(num_visible, num_hidden))\n",
    "        self.b = nn.Parameter(torch.zeros(num_visible))\n",
    "        self.c = nn.Parameter(torch.zeros(num_hidden))\n",
    "\n",
    "        # We need this to handle the split correctly\n",
    "        self.conditioner = nn.Sequential(\n",
    "            nn.Linear(cond_dim, conditioner_width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(conditioner_width, 2 * (num_visible + num_hidden))\n",
    "        )\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self, w_mean=0.0, w_std=0.1, bias_val=0.0):\n",
    "        nn.init.normal_(self.W, mean=w_mean, std=w_std)\n",
    "        nn.init.constant_(self.b, bias_val)\n",
    "        nn.init.constant_(self.c, bias_val)\n",
    "\n",
    "    def _compute_effective_biases(self, cond: torch.Tensor):\n",
    "        # Manual split matching original logic\n",
    "        params = self.conditioner(cond)\n",
    "        gamma_b, beta_b, gamma_c, beta_c = torch.split(params, [self.num_visible, self.num_visible, self.num_hidden, self.num_hidden], dim=-1)\n",
    "\n",
    "        b_mod = (1.0 + gamma_b) * self.b.unsqueeze(0) + beta_b\n",
    "        c_mod = (1.0 + gamma_c) * self.c.unsqueeze(0) + beta_c\n",
    "        return b_mod, c_mod\n",
    "\n",
    "    @staticmethod\n",
    "    def _free_energy(v, W, b, c):\n",
    "        v = v.to(dtype=W.dtype, device=W.device)\n",
    "        return -(v * b).sum(dim=-1) - F.softplus(v @ W + c).sum(dim=-1)\n",
    "\n",
    "    def forward(self, batch, aux_vars):\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        values, _, cond = batch\n",
    "        v_data = values.to(dtype=self.W.dtype, device=self.W.device)\n",
    "        cond = cond.to(v_data.device, dtype=v_data.dtype)\n",
    "\n",
    "        # 1. Bias Prep\n",
    "        b_mod, c_mod = self._compute_effective_biases(cond)\n",
    "\n",
    "        sync_device(v_data.device)\n",
    "        t_prep = time.perf_counter() - t0\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        # 2. Gibbs Sampling (The Heavy Part)\n",
    "        # Init with 0.5 bernoulli\n",
    "        v_start = torch.bernoulli(torch.full_like(v_data, 0.5))\n",
    "\n",
    "        # JIT CALL\n",
    "        v_model = gibbs_sampling_loop(v_start, self.W, b_mod, c_mod, self.k, self.T)\n",
    "        v_model = v_model.detach()\n",
    "\n",
    "        sync_device(v_data.device)\n",
    "        t_sampling = time.perf_counter() - t1\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # 3. Loss Calc\n",
    "        l2_strength = float(aux_vars.get(\"l2_strength\", 0.0))\n",
    "        l2_reg = (self.b.unsqueeze(0) - b_mod).pow(2).sum() + (self.c.unsqueeze(0) - c_mod).pow(2).sum()\n",
    "\n",
    "        fe_data = self._free_energy(v_data, self.W, b_mod, c_mod)\n",
    "        fe_model = self._free_energy(v_model, self.W, b_mod, c_mod)\n",
    "        fe_diff = fe_data - fe_model\n",
    "        loss = fe_diff.mean() + l2_strength * l2_reg\n",
    "\n",
    "        sync_device(v_data.device)\n",
    "        t_loss = time.perf_counter() - t2\n",
    "\n",
    "        return loss, {\n",
    "            \"time_prep\": t_prep,\n",
    "            \"time_sampling\": t_sampling,\n",
    "            \"time_loss\": t_loss\n",
    "        }\n",
    "\n",
    "    # Generate / score methods omitted for brevity as they aren't part of training loop profiling\n",
    "    @torch.no_grad()\n",
    "    def log_score(self, v, cond):\n",
    "        b_mod, c_mod = self._compute_effective_biases(cond)\n",
    "        return -0.5 * self._free_energy(v, self.W, b_mod, c_mod) / self.T"
   ],
   "id": "88f301d5d690357",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T16:08:51.820519Z",
     "start_time": "2025-11-25T16:08:51.810596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cxx(samples: torch.Tensor, pairs: List[Tuple[int, int]],\n",
    "                log_score_fn: Callable[[torch.Tensor], torch.Tensor]) -> Tuple[float, float]:\n",
    "\n",
    "    # Fully vectorized Cxx calculation\n",
    "    B, N = samples.shape\n",
    "    num_pairs = len(pairs)\n",
    "    device = samples.device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Score original samples\n",
    "        log_scores_orig = log_score_fn(samples) # Shape: (B)\n",
    "\n",
    "        # 2. Vectorized Flip: Create a batch that contains ALL flipped versions\n",
    "        # We repeat samples 'num_pairs' times: (num_pairs * B, N)\n",
    "        samples_expanded = samples.repeat(num_pairs, 1)\n",
    "\n",
    "        # Calculate indices to flip in the flat expanded batch\n",
    "        us = torch.tensor([p[0] for p in pairs], device=device)\n",
    "        vs = torch.tensor([p[1] for p in pairs], device=device)\n",
    "\n",
    "        # Arithmetic to map (pair_idx, batch_idx) -> flat_idx\n",
    "        batch_indices = torch.arange(B, device=device).unsqueeze(0).expand(num_pairs, B).flatten()\n",
    "        pair_offsets = torch.arange(num_pairs, device=device).unsqueeze(1).expand(num_pairs, B).flatten() * B\n",
    "        flat_indices = pair_offsets + batch_indices\n",
    "\n",
    "        u_flat = us.unsqueeze(1).expand(num_pairs, B).flatten()\n",
    "        v_flat = vs.unsqueeze(1).expand(num_pairs, B).flatten()\n",
    "\n",
    "        # Perform the flips\n",
    "        flipped_samples = samples_expanded.clone()\n",
    "        flipped_samples[flat_indices, u_flat] = 1.0 - flipped_samples[flat_indices, u_flat]\n",
    "        flipped_samples[flat_indices, v_flat] = 1.0 - flipped_samples[flat_indices, v_flat]\n",
    "\n",
    "        # 3. Score ALL flipped versions in one massive GPU kernel launch\n",
    "        log_scores_flip = log_score_fn(flipped_samples)\n",
    "\n",
    "        # 4. Reshape and compute ratios\n",
    "        log_scores_flip = log_scores_flip.view(num_pairs, B)\n",
    "        log_ratios = log_scores_flip - log_scores_orig.unsqueeze(0)\n",
    "        ratios = torch.exp(log_ratios)\n",
    "\n",
    "        sample_cxx = ratios.mean(dim=0) # Average over pairs\n",
    "\n",
    "        total_cxx = sample_cxx.mean().item()\n",
    "        total_cxx_err = sample_cxx.std(unbiased=True).item() / math.sqrt(B)\n",
    "\n",
    "        return total_cxx, total_cxx_err\n",
    "\n",
    "# Keep monitor_cxx as is, just ensure it uses the optimized compute_cxx above\n",
    "def monitor_cxx(model, ds, pair_indices, device, seed: int):\n",
    "    model.eval()\n",
    "    rng = torch.Generator(device='cpu').manual_seed(seed)\n",
    "\n",
    "    # Sample a subset for monitoring\n",
    "    num_samples = len(ds)\n",
    "    n_monitor = min(1000, num_samples)\n",
    "    indices = torch.randint(0, num_samples, (n_monitor,), generator=rng)\n",
    "\n",
    "    samples = torch.as_tensor(ds.values[indices], device=device).to(dtype=torch.float32)\n",
    "    cond = torch.as_tensor(ds.system_params[indices], device=device, dtype=torch.float32)\n",
    "\n",
    "    scorer = lambda v: model.log_score(v, cond)\n",
    "\n",
    "    cxx_val, _ = compute_cxx(samples, pair_indices, scorer)\n",
    "    model.train()\n",
    "    return cxx_val"
   ],
   "id": "44c23058b6a336ec",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T16:08:55.038462Z",
     "start_time": "2025-11-25T16:08:55.029595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_step(model, optimizer, batch, aux_vars):\n",
    "    # Time Data Loading (implicit since we are here)\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Time Forward\n",
    "    loss, aux = model(batch, aux_vars)\n",
    "\n",
    "    # Time Backward\n",
    "    sync_device(next(model.parameters()).device)\n",
    "    t_fwd_end = time.perf_counter()\n",
    "\n",
    "    loss.backward()\n",
    "    sync_device(next(model.parameters()).device)\n",
    "    t_bwd_end = time.perf_counter()\n",
    "\n",
    "    optimizer.step()\n",
    "    sync_device(next(model.parameters()).device)\n",
    "    t_opt_end = time.perf_counter()\n",
    "\n",
    "    # Pack timings\n",
    "    timings = {\n",
    "        \"forward_total\": t_fwd_end - t0,\n",
    "        \"backward\": t_bwd_end - t_fwd_end,\n",
    "        \"optimizer\": t_opt_end - t_bwd_end,\n",
    "        \"sampling_internal\": aux.get(\"time_sampling\", 0.0)\n",
    "    }\n",
    "    return loss.detach(), timings\n",
    "\n",
    "def train_one_epoch_profile(model, optimizer, loader, epoch_idx, l2_strength):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Storage for timings\n",
    "    history = {\"forward_total\": [], \"backward\": [], \"optimizer\": [], \"sampling_internal\": []}\n",
    "\n",
    "    print(f\"\\n{'Batch':<6} | {'Loss':<8} | {'Fwd (s)':<8} | {'Samp (s)':<8} | {'Bwd (s)':<8} | {'Opt (s)':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    start_epoch = time.perf_counter()\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        # Stop after 20 batches for quick profiling\n",
    "        if i >= 20:\n",
    "            print(\">> Stopping early for profiling.\")\n",
    "            break\n",
    "\n",
    "        aux_vars = {\"l2_strength\": l2_strength}\n",
    "        loss, times = train_step(model, optimizer, batch, aux_vars)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        for k, v in times.items():\n",
    "            history[k].append(v)\n",
    "\n",
    "        # Log every 5 batches\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"{i+1:<6} | {float(loss):.4f}   | \"\n",
    "                  f\"{times['forward_total']:.4f}   | {times['sampling_internal']:.4f}   | \"\n",
    "                  f\"{times['backward']:.4f}   | {times['optimizer']:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / (i + 1)\n",
    "\n",
    "    # Stats\n",
    "    print(\"\\n--- TIMING BREAKDOWN (Avg per batch) ---\")\n",
    "    print(f\"Sampling (JIT) : {sum(history['sampling_internal'])/len(history['sampling_internal']):.4f} sec\")\n",
    "    print(f\"Forward Total  : {sum(history['forward_total'])/len(history['forward_total']):.4f} sec\")\n",
    "    print(f\"Backward       : {sum(history['backward'])/len(history['backward']):.4f} sec\")\n",
    "    print(f\"Optimizer      : {sum(history['optimizer'])/len(history['optimizer']):.4f} sec\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    return avg_loss"
   ],
   "id": "b95d244aa036f93b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T16:20:49.489190Z",
     "start_time": "2025-11-25T16:20:45.486196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATA LOADING\n",
    "\n",
    "SIDE_LENGTH = 4\n",
    "FILE_SAMPLES = 5_000_000\n",
    "TRAIN_SAMPLES = 50_000  # beyond 100k per file it gets slow\n",
    "\n",
    "# construct filenames dynamically based on support deltas\n",
    "delta_support = [0.40, 0.60, 0.80, 0.90, 0.95, 1.00, 1.05, 1.10, 1.40, 2.00]\n",
    "file_names = [f\"xxz_{SIDE_LENGTH}x{SIDE_LENGTH}_delta{d:.2f}_{FILE_SAMPLES}.npz\" for d in delta_support]\n",
    "file_paths = [data_dir / fn for fn in file_names]\n",
    "samples_per_file = [TRAIN_SAMPLES] * len(file_paths)\n",
    "\n",
    "diag_indices = [k * (SIDE_LENGTH + 1) for k in range(SIDE_LENGTH)]\n",
    "corr_pairs = list(zip(diag_indices, diag_indices[1:]))\n",
    "\n",
    "print(f\"System Size       : {SIDE_LENGTH}x{SIDE_LENGTH} ({SIDE_LENGTH**2} qubits)\")\n",
    "print(f\"Training Samples  : {TRAIN_SAMPLES} per file (from {FILE_SAMPLES} total)\")\n",
    "print(f\"Support Deltas    : {delta_support}\")\n",
    "print(f\"Correlation Pairs : {corr_pairs} (main diagonal neighbors)\")\n",
    "\n",
    "ds = MeasurementDataset(file_paths, load_fn=load_measurements_npz,\n",
    "                        system_param_keys=[\"delta\"], samples_per_file=samples_per_file)\n",
    "\n",
    "print(f\"Samples Shape     : {tuple(ds.values.shape)}\")\n",
    "print(f\"Conditions Shape  : {tuple(ds.system_params.shape)}\")"
   ],
   "id": "7acfbb34aba868a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Size       : 4x4 (16 qubits)\n",
      "Training Samples  : 50000 per file (from 5000000 total)\n",
      "Support Deltas    : [0.4, 0.6, 0.8, 0.9, 0.95, 1.0, 1.05, 1.1, 1.4, 2.0]\n",
      "Correlation Pairs : [(0, 5), (5, 10), (10, 15)] (main diagonal neighbors)\n",
      "Samples Shape     : (500000, 16)\n",
      "Conditions Shape  : (500000, 1)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T16:09:12.794164Z",
     "start_time": "2025-11-25T16:09:05.865151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- CONFIGURATION ---\n",
    "batch_size        = 1024\n",
    "num_visible       = ds.num_qubits\n",
    "num_hidden        = 64\n",
    "conditioner_width = 64\n",
    "\n",
    "# Start with minimal steps to verify speed\n",
    "k_steps           = 25\n",
    "l2_strength       = 1e-4\n",
    "lr                = 1e-2\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"Running on: {device}\")\n",
    "print(f\"Gibbs Steps (k): {k_steps}\")\n",
    "\n",
    "# Data Loader\n",
    "loader_rng = torch.Generator(device=\"cpu\").manual_seed(SEED)\n",
    "loader = MeasurementLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=False, rng=loader_rng)\n",
    "\n",
    "# Model\n",
    "model = ConditionalRBM(num_visible=num_visible, num_hidden=num_hidden, cond_dim=ds.system_params.shape[1],\n",
    "                       conditioner_width=conditioner_width, k=k_steps, T=1.0)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# --- RUN PROFILING ---\n",
    "print(\"Starting Profiling Run (Max 20 batches)...\")\n",
    "train_one_epoch_profile(model, optimizer, loader, epoch_idx=0, l2_strength=l2_strength)"
   ],
   "id": "712d80a7dfff1a23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n",
      "Gibbs Steps (k): 25\n",
      "Starting Profiling Run (Max 20 batches)...\n",
      "\n",
      "Batch  | Loss     | Fwd (s)  | Samp (s) | Bwd (s)  | Opt (s) \n",
      "-----------------------------------------------------------------\n",
      "5      | 0.5218   | 0.3588   | 0.3380   | 0.0304   | 0.0008\n",
      "10     | -0.0008   | 0.2982   | 0.2832   | 0.0212   | 0.0004\n",
      "15     | -0.3683   | 0.2374   | 0.2274   | 0.0320   | 0.0006\n",
      "20     | -0.9718   | 0.4291   | 0.3936   | 0.0173   | 0.0004\n",
      ">> Stopping early for profiling.\n",
      "\n",
      "--- TIMING BREAKDOWN (Avg per batch) ---\n",
      "Sampling (JIT) : 0.3022 sec\n",
      "Forward Total  : 0.3191 sec\n",
      "Backward       : 0.0248 sec\n",
      "Optimizer      : 0.0009 sec\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0014516938300359818"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
