{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-12T13:03:27.318970Z",
     "start_time": "2025-12-12T13:03:25.529428Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "\n",
    "# Add parent directory to path to find data_handling.py\n",
    "# ASSUMPTION: data_handling.py is in the parent folder or current folder\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "from data_handling import (\n",
    "    load_measurements_npz,\n",
    "    load_state_npz,\n",
    "    save_state_npz,\n",
    "    save_measurements_npz,\n",
    "    MeasurementDataset,\n",
    "    MeasurementLoader,\n",
    "    MultiQubitMeasurement\n",
    ")\n",
    "\n",
    "# Define and create paths\n",
    "data_dir = Path(\"measurements\")\n",
    "state_dir = Path(\"state_vectors\")\n",
    "exp_dir = Path(\"experiments\")\n",
    "\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "state_dir.mkdir(parents=True, exist_ok=True)\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory:       {data_dir.resolve()}\")\n",
    "print(f\"State directory:      {state_dir.resolve()}\")\n",
    "print(f\"Experiment artifacts: {exp_dir.resolve()}\")\n",
    "\n",
    "# Set global device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:       /Users/Tonni/Desktop/master-code/neural-quantum-tomo/experiments_polish/tfim_4x4/measurements\n",
      "State directory:      /Users/Tonni/Desktop/master-code/neural-quantum-tomo/experiments_polish/tfim_4x4/state_vectors\n",
      "Experiment artifacts: /Users/Tonni/Desktop/master-code/neural-quantum-tomo/experiments_polish/tfim_4x4/experiments\n",
      "Running on: cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:04:12.852207Z",
     "start_time": "2025-12-12T13:03:51.832972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- GENERATION CONFIG ---\n",
    "GEN_SIDE_LENGTH = 4\n",
    "GEN_SAMPLES     = 50_000 # Max pool to draw from\n",
    "J_VAL           = -1.00\n",
    "\n",
    "# 1. The \"Backbone\" (Training Data)\n",
    "h_support = [1.00, 2.00, 2.50, 3.00, 4.00, 5.00, 6.00, 7.00]\n",
    "\n",
    "# 2. The \"Interpolation\" (Evaluation Data)\n",
    "h_novel   = [1.50, 2.80, 3.20, 3.50, 4.50, 5.50]\n",
    "\n",
    "# Combine for loop\n",
    "all_h_values = sorted(list(set(h_support + h_novel)))\n",
    "\n",
    "def build_tfim_hamiltonian(L, h, J):\n",
    "    from netket.graph import Hypercube\n",
    "    from netket.hilbert import Spin\n",
    "    from netket.operator import Ising\n",
    "    graph = Hypercube(length=L, n_dim=2, pbc=True)\n",
    "    hilbert = Spin(s=0.5, N=graph.n_nodes)\n",
    "    return Ising(hilbert, graph, h=h, J=J)\n",
    "\n",
    "def get_groundstate(H):\n",
    "    from scipy.sparse.linalg import eigsh\n",
    "    sp_mat = H.to_sparse()\n",
    "    evals, evecs = eigsh(sp_mat, k=1, which=\"SA\")\n",
    "    psi = evecs[:, 0]\n",
    "    # Fix phase\n",
    "    first_idx = np.argmax(np.abs(psi))\n",
    "    phase = np.angle(psi[first_idx])\n",
    "    return psi * np.exp(-1j * phase)\n",
    "\n",
    "# --- EXECUTE GENERATION ---\n",
    "# We force regeneration to fix the header structure\n",
    "print(\"Generating Dataset with correct 'state' headers...\")\n",
    "\n",
    "rng_gen = np.random.default_rng(42)\n",
    "bases = ['Z'] * (GEN_SIDE_LENGTH**2)\n",
    "meas_op = MultiQubitMeasurement(bases, verbose=False)\n",
    "\n",
    "for h in all_h_values:\n",
    "    is_support = h in h_support\n",
    "    print(f\"Processing h={h:.2f} [{'SUPPORT' if is_support else 'NOVEL'}]\")\n",
    "\n",
    "    # 1. Solve Exact State\n",
    "    H = build_tfim_hamiltonian(GEN_SIDE_LENGTH, h, J_VAL)\n",
    "    psi = get_groundstate(H)\n",
    "\n",
    "    # Save State\n",
    "    sys_str = f\"tfim_{GEN_SIDE_LENGTH}x{GEN_SIDE_LENGTH}\"\n",
    "    state_path = state_dir / f\"{sys_str}_h{h:.2f}.npz\"\n",
    "\n",
    "    # --- FIX IS HERE: Header key must be 'state' ---\n",
    "    header_data = {\"h\": h, \"J\": J_VAL, \"system\": sys_str}\n",
    "\n",
    "    save_state_npz(state_path, psi, {\"state\": header_data})\n",
    "\n",
    "    # 2. Sample (Only for Support)\n",
    "    if is_support:\n",
    "        samples = meas_op.sample_state(psi, num_samples=GEN_SAMPLES, rng=rng_gen)\n",
    "        meas_path = data_dir / f\"{sys_str}_h{h:.2f}_{GEN_SAMPLES}.npz\"\n",
    "\n",
    "        # --- FIX IS HERE: Header key must be 'state' ---\n",
    "        save_measurements_npz(meas_path, samples, bases, {\"state\": header_data})\n",
    "\n",
    "print(\"Generation Complete.\")"
   ],
   "id": "457b0e5f88547958",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dataset with correct 'state' headers...\n",
      "Processing h=1.00 [SUPPORT]\n",
      "Processing h=1.50 [NOVEL]\n",
      "Processing h=2.00 [SUPPORT]\n",
      "Processing h=2.50 [SUPPORT]\n",
      "Processing h=2.80 [NOVEL]\n",
      "Processing h=3.00 [SUPPORT]\n",
      "Processing h=3.20 [NOVEL]\n",
      "Processing h=3.50 [NOVEL]\n",
      "Processing h=4.00 [SUPPORT]\n",
      "Processing h=4.50 [NOVEL]\n",
      "Processing h=5.00 [SUPPORT]\n",
      "Processing h=5.50 [NOVEL]\n",
      "Processing h=6.00 [SUPPORT]\n",
      "Processing h=7.00 [SUPPORT]\n",
      "Generation Complete.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:04:17.077298Z",
     "start_time": "2025-12-12T13:04:17.060224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Conditioner(nn.Module):\n",
    "    def __init__(self, num_visible: int, num_hidden: int, cond_dim: int, hidden_width: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cond_dim, hidden_width)\n",
    "        self.fc2 = nn.Linear(hidden_width, 2 * (num_visible + num_hidden))\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, cond: torch.Tensor):\n",
    "        x = torch.tanh(self.fc1(cond))\n",
    "        x = self.fc2(x)\n",
    "        return torch.split(x, [self.num_visible, self.num_visible, self.num_hidden, self.num_hidden], dim=-1)\n",
    "\n",
    "\n",
    "class ConditionalRBM(nn.Module):\n",
    "    def __init__(self, num_visible: int, num_hidden: int, cond_dim: int,\n",
    "                 conditioner_width: int = 64, k: int = 1, T: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(num_visible, num_hidden))\n",
    "        self.b = nn.Parameter(torch.zeros(num_visible))\n",
    "        self.c = nn.Parameter(torch.zeros(num_hidden))\n",
    "        self.conditioner = Conditioner(num_visible, num_hidden, cond_dim, conditioner_width)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # FIX: Ensure std=0.1 to avoid weak gradients at start\n",
    "        nn.init.normal_(self.W, std=0.1)\n",
    "        nn.init.constant_(self.b, 0.0)\n",
    "        nn.init.constant_(self.c, 0.0)\n",
    "\n",
    "    def _free_energy(self, v: torch.Tensor, b_mod: torch.Tensor, c_mod: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the Z2-symmetrized Free Energy efficiently.\n",
    "        F_sym(v) = -T * log( exp(-F(v)/T) + exp(-F(-v)/T) )\n",
    "\n",
    "        OPTIMIZATION:\n",
    "        Instead of performing two matrix multiplications (v@W and (1-v)@W),\n",
    "        we use the identity: (1-v)@W = sum(W) - v@W.\n",
    "        This provides a ~30% speedup during training and inference.\n",
    "        \"\"\"\n",
    "        v = v.to(dtype=self.W.dtype, device=self.W.device)\n",
    "\n",
    "        # 1. Compute the raw matrix product ONCE (The expensive part)\n",
    "        v_W = v @ self.W  # Shape: (Batch, Hidden)\n",
    "\n",
    "        # 2. Compute the row sums of W ONCE (cheap vector op)\n",
    "        # Represents (1 vector) @ W\n",
    "        W_sum = self.W.sum(dim=0)  # Shape: (Hidden,)\n",
    "\n",
    "        # 3. Construct the linear terms for both branches\n",
    "        # Normal:   H = v@W + c_mod\n",
    "        linear_v = v_W + c_mod\n",
    "\n",
    "        # Flipped:  H = (1-v)@W + c_mod  =>  sum(W) - v@W + c_mod\n",
    "        # Broadcasting W_sum against v_W works automatically\n",
    "        linear_flip = W_sum.unsqueeze(0) - v_W + c_mod\n",
    "\n",
    "        # 4. Non-linearity (Softplus)\n",
    "        term2_v = F.softplus(linear_v).sum(dim=-1)\n",
    "        term2_f = F.softplus(linear_flip).sum(dim=-1)\n",
    "\n",
    "        # 5. Bias Terms (Element-wise)\n",
    "        # Normal:   -v * b_mod\n",
    "        term1_v = -(v * b_mod).sum(dim=-1)\n",
    "        # Flipped:  -(1-v) * b_mod\n",
    "        term1_f = -((1.0 - v) * b_mod).sum(dim=-1)\n",
    "\n",
    "        # 6. Calculate individual Free Energies\n",
    "        fe_v = term1_v - term2_v\n",
    "        fe_flipped = term1_f - term2_f\n",
    "\n",
    "        # 7. Symmetrize via LogSumExp\n",
    "        # Returns -T * log( P(v) + P(not v) )\n",
    "        stacked = torch.stack([-fe_v, -fe_flipped], dim=-1)\n",
    "        return -self.T * torch.logsumexp(stacked / self.T, dim=-1)\n",
    "\n",
    "    def _compute_effective_biases(self, cond: torch.Tensor):\n",
    "        gamma_b, beta_b, gamma_c, beta_c = self.conditioner(cond)\n",
    "        b_mod = (1.0 + gamma_b) * self.b.unsqueeze(0) + beta_b\n",
    "        c_mod = (1.0 + gamma_c) * self.c.unsqueeze(0) + beta_c\n",
    "        return b_mod, c_mod\n",
    "\n",
    "    def _gibbs_step(self, v: torch.Tensor, b_mod: torch.Tensor, c_mod: torch.Tensor, rng: torch.Generator):\n",
    "        p_h = torch.sigmoid((v @ self.W + c_mod) / self.T)\n",
    "        h = torch.bernoulli(p_h, generator=rng)\n",
    "        p_v = torch.sigmoid((h @ self.W.t() + b_mod) / self.T)\n",
    "        return torch.bernoulli(p_v, generator=rng)\n",
    "\n",
    "    def log_score(self, v: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        # returns log(psi) = -0.5 * FE / T\n",
    "        b_mod, c_mod = self._compute_effective_biases(cond)\n",
    "        return -0.5 * self._free_energy(v, b_mod, c_mod) / self.T\n",
    "\n",
    "    def forward(self, batch: Tuple[torch.Tensor, ...], aux_vars: Dict[str, Any]):\n",
    "        v_data, _, cond = batch\n",
    "        v_data = v_data.to(device=self.W.device, dtype=self.W.dtype)\n",
    "        cond = cond.to(device=self.W.device, dtype=self.W.dtype)\n",
    "\n",
    "        rng = aux_vars.get(\"rng\")\n",
    "        b_mod, c_mod = self._compute_effective_biases(cond)\n",
    "\n",
    "        # CD-k\n",
    "        v_model = v_data.clone()\n",
    "\n",
    "        # Mix in noise: CRITICAL FIX -> Use small noise_frac (e.g. 0.1) in training loop, not 1.0!\n",
    "        n_noise = int(v_data.shape[0] * aux_vars.get(\"noise_frac\", 0.1))\n",
    "        if n_noise > 0:\n",
    "            v_model[:n_noise] = torch.bernoulli(torch.full_like(v_model[:n_noise], 0.5), generator=rng)\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            v_model = self._gibbs_step(v_model, b_mod, c_mod, rng)\n",
    "        v_model = v_model.detach()\n",
    "\n",
    "        # Compute Loss using Symmetrized Free Energy (Gradients flow to both basins)\n",
    "        fe_data = self._free_energy(v_data, b_mod, c_mod)\n",
    "        fe_model = self._free_energy(v_model, b_mod, c_mod)\n",
    "        loss = (fe_data - fe_model).mean()\n",
    "\n",
    "        return loss, {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_normalized_wavefunction(self, cond: torch.Tensor, all_states: torch.Tensor) -> torch.Tensor:\n",
    "        # cond: (1, D) or (B, D)\n",
    "        # all_states: (2^N, N)\n",
    "        if cond.dim() == 1: cond = cond.unsqueeze(0)\n",
    "\n",
    "        # Expand condition to match all states\n",
    "        cond_exp = cond.expand(all_states.shape[0], -1)\n",
    "\n",
    "        # Enforce T=1 for wavefunction extraction\n",
    "        old_T = self.T\n",
    "        self.T = 1.0\n",
    "        log_psi = self.log_score(all_states, cond_exp)\n",
    "        self.T = old_T\n",
    "\n",
    "        # Normalize: sum |psi|^2 = sum exp(2*log_psi)\n",
    "        log_norm_sq = torch.logsumexp(2.0 * log_psi, dim=0)\n",
    "        return torch.exp(log_psi - 0.5 * log_norm_sq)"
   ],
   "id": "3438949c660f96c0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:04:19.139303Z",
     "start_time": "2025-12-12T13:04:19.133983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_sigmoid_curve(high, low, steps, falloff):\n",
    "    center = steps / 2.0\n",
    "    def fn(step):\n",
    "        s = min(step, steps)\n",
    "        return float(low + (high - low) / (1.0 + math.exp(falloff * (s - center))))\n",
    "    return fn\n",
    "\n",
    "def train(model, optimizer, loader, num_epochs, rng, lr_schedule_fn):\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    # FIX: Define noise_frac here (Baseline used 0.1 or 0.05)\n",
    "    training_noise_frac = 0.1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tot_loss = 0.0\n",
    "        for batch in loader:\n",
    "            lr = lr_schedule_fn(global_step)\n",
    "            for g in optimizer.param_groups: g[\"lr\"] = lr\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # FIX: Pass the correct noise fraction\n",
    "            loss, _ = model(batch, {\"rng\": rng, \"noise_frac\": training_noise_frac})\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "    return model"
   ],
   "id": "739829fbbe3a4fff",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:04:21.173193Z",
     "start_time": "2025-12-12T13:04:21.168521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_all_states(num_qubits: int, device: torch.device):\n",
    "    lst = list(itertools.product([0, 1], repeat=num_qubits))\n",
    "    return torch.tensor(lst, dtype=torch.float32, device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_overlap(model, h_val, gt_path, all_states):\n",
    "    # 1. Load GT\n",
    "    psi_np, _ = load_state_npz(gt_path)\n",
    "    psi_true = torch.from_numpy(psi_np).real.float().to(device)\n",
    "    psi_true = psi_true / torch.norm(psi_true)\n",
    "\n",
    "    # 2. Compute Model\n",
    "    cond = torch.tensor([h_val], device=device, dtype=torch.float32)\n",
    "    psi_model = model.get_normalized_wavefunction(cond, all_states)\n",
    "\n",
    "    # 3. Overlap (Linear)\n",
    "    return torch.abs(torch.dot(psi_true, psi_model)).item()"
   ],
   "id": "de714f7918eda353",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:05:51.440869Z",
     "start_time": "2025-12-12T13:04:22.648042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EXPERIMENT CONFIG ---\n",
    "SAMPLE_SIZES = [2_000, 10_000] # The sweep\n",
    "N_EPOCHS     = 50\n",
    "\n",
    "# Setup Evaluation Points\n",
    "h_eval_support = [1.00, 2.00, 2.50, 3.00, 4.00, 5.00, 6.00, 7.00]\n",
    "h_eval_novel   = [1.50, 2.80, 3.20, 3.50, 4.50, 5.50]\n",
    "all_h_eval     = sorted(list(set(h_eval_support + h_eval_novel)))\n",
    "\n",
    "# Pre-compute basis states\n",
    "all_states = generate_all_states(GEN_SIDE_LENGTH**2, device)\n",
    "\n",
    "# Setup RNG\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "rng = torch.Generator().manual_seed(seed)\n",
    "\n",
    "# Define file paths for loading (from Cell 2 config)\n",
    "file_names = [f\"tfim_{GEN_SIDE_LENGTH}x{GEN_SIDE_LENGTH}_h{h:.2f}_{GEN_SAMPLES}.npz\" for h in h_support]\n",
    "file_paths = [data_dir / fn for fn in file_names]\n",
    "\n",
    "print(f\"Starting experiment sweep: {SAMPLE_SIZES}\")\n",
    "\n",
    "for n_samples in SAMPLE_SIZES:\n",
    "    print(f\"\\n>>> RUNNING: {n_samples} samples per support point <<<\")\n",
    "\n",
    "    # 1. Reload Dataset with specific limit\n",
    "    ds = MeasurementDataset(file_paths, load_measurements_npz, [\"h\"], [n_samples]*len(file_paths))\n",
    "    loader = MeasurementLoader(ds, batch_size=1024, shuffle=True, drop_last=False, rng=rng)\n",
    "\n",
    "    # 2. Init Model\n",
    "    model = ConditionalRBM(ds.num_qubits, num_hidden=16, cond_dim=1, conditioner_width=64, k=20).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = get_sigmoid_curve(1e-2, 1e-4, N_EPOCHS*len(loader), 0.005)\n",
    "\n",
    "    # 3. Train\n",
    "    model = train(model, optimizer, loader, N_EPOCHS, rng, scheduler)\n",
    "\n",
    "    # 4. Evaluate Overlap\n",
    "    results = []\n",
    "    for h_val in all_h_eval:\n",
    "        gt_path = state_dir / f\"tfim_{GEN_SIDE_LENGTH}x{GEN_SIDE_LENGTH}_h{h_val:.2f}.npz\"\n",
    "        if not gt_path.exists(): continue\n",
    "\n",
    "        ov = compute_overlap(model, h_val, gt_path, all_states)\n",
    "        rtype = \"support\" if h_val in h_eval_support else \"novel\"\n",
    "        results.append({\"h\": h_val, \"overlap\": ov, \"type\": rtype})\n",
    "\n",
    "    # 5. Save Artifacts\n",
    "    run_id = f\"overlap_{n_samples}\"\n",
    "    report = {\n",
    "        \"config\": {\"n_samples\": n_samples, \"epochs\": N_EPOCHS},\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "    with open(exp_dir / f\"{run_id}_report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    print(f\"Run {run_id} complete.\")\n",
    "\n",
    "print(\"\\nAll experiments finished.\")"
   ],
   "id": "583cf8fe9aa1a0a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment sweep: [2000, 10000]\n",
      "\n",
      ">>> RUNNING: 2000 samples per support point <<<\n",
      "Run overlap_2000 complete.\n",
      "\n",
      ">>> RUNNING: 10000 samples per support point <<<\n",
      "Run overlap_10000 complete.\n",
      "\n",
      "All experiments finished.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T19:22:40.020239Z",
     "start_time": "2025-12-12T19:22:39.888374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load all reports\n",
    "report_files = sorted(glob.glob(str(exp_dir / \"*_report.json\")))\n",
    "experiments = []\n",
    "for fpath in report_files:\n",
    "    with open(fpath, \"r\") as f: experiments.append(json.load(f))\n",
    "\n",
    "# Sort by sample size\n",
    "experiments.sort(key=lambda x: x[\"config\"][\"n_samples\"])\n",
    "\n",
    "plt.figure(figsize=(8, 5), dpi=200)\n",
    "colors = cm.viridis(np.linspace(0.3, 0.9, len(experiments)))\n",
    "\n",
    "for i, exp in enumerate(experiments):\n",
    "    n = exp[\"config\"][\"n_samples\"]\n",
    "    df = pd.DataFrame(exp[\"results\"])\n",
    "    supp = df[df[\"type\"] == \"support\"]\n",
    "    nov  = df[df[\"type\"] == \"novel\"]\n",
    "\n",
    "    c = colors[i]\n",
    "    # Line\n",
    "    plt.plot(df[\"h\"], df[\"overlap\"], '-', color=c, alpha=0.5, label=f\"N={n}\")\n",
    "    # Support\n",
    "    plt.plot(supp[\"h\"], supp[\"overlap\"], 'o', color=c, markersize=7)\n",
    "    # Novel\n",
    "    plt.plot(nov[\"h\"], nov[\"overlap\"], 'o', color=c, markerfacecolor='white', markeredgewidth=2, markersize=7)\n",
    "\n",
    "plt.xlabel(r\"Transverse Field $h$\", fontsize=12)\n",
    "plt.ylabel(r\"Overlap $|\\langle \\psi_\\mathrm{ED} | \\psi_\\mathrm{RBM} \\rangle|$\", fontsize=12)\n",
    "plt.title(f\"TFIM {GEN_SIDE_LENGTH}x{GEN_SIDE_LENGTH} - Overlap\", fontsize=12)\n",
    "\n",
    "# Legends\n",
    "color_leg = plt.legend(loc='lower right', title=\"Training Samples\")\n",
    "plt.gca().add_artist(color_leg)\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color='gray', marker='o', linestyle='None', label='Support'),\n",
    "    Line2D([0], [0], color='gray', marker='o', markerfacecolor='white', linestyle='None', label='Novel')\n",
    "]\n",
    "plt.legend(handles=custom_lines, loc='lower left')\n",
    "\n",
    "plt.ylim(0.95, 1.005)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "312f4311fd8b32ed",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load all reports\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m report_files \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[43mglob\u001B[49m\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;28mstr\u001B[39m(exp_dir \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*_report.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[1;32m      3\u001B[0m experiments \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fpath \u001B[38;5;129;01min\u001B[39;00m report_files:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'glob' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
