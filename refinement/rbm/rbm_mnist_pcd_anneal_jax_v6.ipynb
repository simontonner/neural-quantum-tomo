{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:20:55.245149Z",
     "start_time": "2025-03-28T17:20:55.239176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.lax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "import functools\n",
    "\n",
    "from load_mnist import download_mnist_if_needed, load_images, load_labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_dir = \"./data\"\n",
    "device = jax.devices('cpu')[0]\n",
    "\n",
    "print(f\"Data resides in        : {data_dir}\")\n",
    "print(f\"Training model on      : {str(device)}\")"
   ],
   "id": "27c3d547f23b4972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data resides in        : ./data\n",
      "Training model on      : TFRT_CPU_0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:20:56.468624Z",
     "start_time": "2025-03-28T17:20:55.978826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(x):\n",
    "    x = x.astype(np.float32) / 255.0    # normalize to [0, 1]\n",
    "    x = x > 0.5                         # binarize\n",
    "    x = x.reshape(x.shape[0], -1)       # flatten for RBM\n",
    "    x = jnp.array(x, dtype=jnp.float32) # use jax numpy array of dtype float32, because RBM has float32 params\n",
    "    return x\n",
    "\n",
    "\n",
    "data_paths = download_mnist_if_needed(root=data_dir, train_only=True)\n",
    "x_train_raw = load_images(data_paths['train_images'])\n",
    "y_train = load_labels(data_paths['train_labels'])\n",
    "\n",
    "x_train = preprocess(x_train_raw)\n",
    "print(f\"x_train dtype: {x_train.dtype}, shape: {x_train.shape}\")\n",
    "print(f\"x_train min: {x_train.min()}, max: {x_train.max()}\")"
   ],
   "id": "dc789a9d880e97c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train dtype: float32, shape: (60000, 784)\n",
      "x_train min: 0.0, max: 1.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:20:57.458012Z",
     "start_time": "2025-03-28T17:20:57.449485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, v):\n",
    "        # In Flax, __call__ is the main computation, analogous to PyTorchâ€™s forward().\n",
    "        # We have moved the implementation of the free energy here.\n",
    "        #\n",
    "        # Even though __call__ is invoked twice during contrastive divergence, the\n",
    "        # parameters are only initialized once on the first call via @nn.compact.\n",
    "        W = self.param(\"W\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        b = self.param(\"b\", nn.initializers.zeros, (self.n_visible,))\n",
    "        c = self.param(\"c\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "        # Assumes v has shape (batch_size, n_visible); ensure it before calling\n",
    "        visible_term = jnp.dot(v, b)\n",
    "        hidden_term = jnp.sum(jax.nn.softplus(v @ W + c), axis=-1)\n",
    "        return -visible_term - hidden_term\n",
    "\n",
    "    @staticmethod\n",
    "    def _gibbs_step(i, state, params, T=1.0):\n",
    "        v, key = state\n",
    "        W, b, c = params[\"W\"], params[\"b\"], params[\"c\"]\n",
    "\n",
    "        key, h_key, v_key = jax.random.split(key, 3)\n",
    "\n",
    "        h_probs = jax.nn.sigmoid((v @ W + c) / T)\n",
    "        h = jax.random.bernoulli(h_key, h_probs).astype(jnp.float32)\n",
    "\n",
    "        v_probs = jax.nn.sigmoid((h @ W.T + b) / T)\n",
    "        v = jax.random.bernoulli(v_key, v_probs).astype(jnp.float32)\n",
    "\n",
    "        return v, key\n",
    "\n",
    "    @staticmethod\n",
    "    def gibbs_sample(params, v0, rng, k=1, T=1.0):\n",
    "        # The fori_loop enables JIT compilation of loops. It basically unrolls the loop over the fixed length k.\n",
    "        return jax.lax.fori_loop(0, k, lambda i, state: RBM._gibbs_step(i, state, params, T), (v0, rng))[0]"
   ],
   "id": "3d8fb14ccceb3134",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:18:37.498983Z",
     "start_time": "2025-03-28T17:18:37.492543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "\n",
    "class RBMTrainState(train_state.TrainState):\n",
    "    \"\"\"A value object bundling parameters to be passed between training steps.\n",
    "    It holds all the necessary state information (model parameters, optimizer state, step count, etc.).\n",
    "    Since it is immutable, the training function needs to return a new instance after each update step.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# this function will be compiled by the function below\n",
    "def pcd_loss(\n",
    "        params: FrozenDict, # parameters to calculate loss for\n",
    "        apply_fn: callable, # The model's apply function stored in the TrainState\n",
    "        data_batch: jnp.ndarray,\n",
    "        current_fantasy_particles: jnp.ndarray,\n",
    "        key_loss: jax.random.PRNGKey,\n",
    "        k: int              # Number of Gibbs sampling steps\n",
    ") -> jnp.ndarray:\n",
    "\n",
    "    v_k = RBM.gibbs_sample(params, current_fantasy_particles, key_loss, k)\n",
    "    # even though the gibbs_sample function is perfectly differentiable, we are only interested in v_k as a sample\n",
    "    v_k = jax.lax.stop_gradient(v_k)\n",
    "\n",
    "    # retrieves the models parameters and makes sure they are available inside the specific function\n",
    "    fe_data = apply_fn({'params': params}, data_batch)\n",
    "    fe_model = apply_fn({'params': params}, v_k)\n",
    "\n",
    "    loss = jnp.mean(fe_data) - jnp.mean(fe_model)\n",
    "    return loss"
   ],
   "id": "649fa004ecfe846",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['k'])\n",
    "def train_step(\n",
    "        state: RBMTrainState,\n",
    "        data_batch: jnp.ndarray,\n",
    "        current_fantasy_particles: jnp.ndarray,\n",
    "        key: jax.random.PRNGKey,\n",
    "        k: int):\n",
    "    \"\"\"\n",
    "    Performs a single, JIT-compilable training step using PCD.\n",
    "\n",
    "    Args:\n",
    "        state: The current RBMTrainState value object containing parameters,\n",
    "               optimizer state, apply_fn, etc.\n",
    "        data_batch: A batch of input data (visible units).\n",
    "        current_fantasy_particles: The persistent chain samples from the previous step or a new one after reset.\n",
    "        key: A JAX PRNG key for this step's random operations.\n",
    "        k: The number of Gibbs sampling steps (static for JIT compilation).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "          - new_state: The updated RBMTrainState value object.\n",
    "          - loss_value: The scalar loss calculated for this step (for metrics).\n",
    "          - new_fantasy_particles: The updated persistent chain samples for the next step.\n",
    "    \"\"\"\n",
    "\n",
    "    key_loss, key_update = jax.random.split(key)\n",
    "\n",
    "    # Define the loss function. It captures 'state', 'data_batch',\n",
    "    # 'current_fantasy_particles', 'key_loss', and 'k' from the outer scope.\n",
    "    # It takes only 'params' as input, which is what jax.grad needs.\n",
    "    def pcd_loss_fn(params):\n",
    "        # --- Negative Phase Calculation ---\n",
    "        # Generate k samples starting from the persistent chain (fantasy particles)\n",
    "        # using the *current* parameters 'params'.\n",
    "        v_k = RBM.gibbs_sample(params, current_fantasy_particles, key_loss, k)\n",
    "        # Stop gradients from flowing back through the Gibbs sampling process itself.\n",
    "        # The gradient should only depend on how params affect the free energy.\n",
    "        v_k = jax.lax.stop_gradient(v_k)\n",
    "\n",
    "        # --- Free Energy Calculation ---\n",
    "        # Calculate free energy for the data (positive phase)\n",
    "        fe_data = state.apply_fn({'params': params}, data_batch, method='free_energy')\n",
    "        # Calculate free energy for the model samples (negative phase)\n",
    "        fe_model = state.apply_fn({'params': params}, v_k, method='free_energy')\n",
    "\n",
    "        # --- Loss Calculation ---\n",
    "        # The PCD loss is the difference in mean free energies.\n",
    "        loss = jnp.mean(fe_data) - jnp.mean(fe_model)\n",
    "        return loss\n",
    "\n",
    "    # Calculate the loss value and the gradients w.r.t. 'state.params'\n",
    "    loss_value, grads = jax.value_and_grad(pcd_loss_fn)(state.params)\n",
    "\n",
    "    # Apply the gradients to the state.\n",
    "    # This uses the optimizer defined in state.tx and updates\n",
    "    # state.params, state.opt_state, and state.step.\n",
    "    # It returns a NEW state object.\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Update the fantasy particles for the *next* training step.\n",
    "    # Important: Sample using the parameters *before* the gradient update (`state.params`),\n",
    "    # consistent with how v_k was generated for the loss calculation and mirroring\n",
    "    # the detach() -> step() order in PyTorch.\n",
    "    new_fantasy_particles = RBM.gibbs_sample(state.params, current_fantasy_particles, key_update, k)\n",
    "    # Ensure no gradients accidentally flow through this state update path either.\n",
    "    new_fantasy_particles = jax.lax.stop_gradient(new_fantasy_particles)\n",
    "\n",
    "    # Return the new state, the loss, and the updated particles\n",
    "    return new_state, loss_value, new_fantasy_particles\n",
    "\n"
   ],
   "id": "c166a9918132efa8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T01:02:53.741223Z",
     "start_time": "2025-03-28T01:02:53.731265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@functools.partial(jax.jit, static_argnames=(\"rbm\", \"k\", \"T\"))\n",
    "def cd_loss(rbm, params, v0, rng, k=1, T=1.0):\n",
    "    v_k = rbm.gibbs_sample(params, v0, rng, k, T)\n",
    "    fe_data = rbm.apply({'params': params}, v0)\n",
    "    fe_model = rbm.apply({'params': params}, v_k)\n",
    "    return jnp.mean(fe_data) - jnp.mean(fe_model)"
   ],
   "id": "511f15b680068371",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T01:02:54.998108Z",
     "start_time": "2025-03-28T01:02:54.718216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "rbm = RBM(n_visible=784, n_hidden=128)\n",
    "v0 = jax.random.bernoulli(key, shape=(32, 784)).astype(jnp.float32)\n",
    "params = rbm.init(key, v0)['params']\n",
    "\n",
    "loss = cd_loss(rbm, params, v0, key)\n",
    "print(\"CD-1 Loss:\", loss)"
   ],
   "id": "9f01a57cba5259e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD-1 Loss: 0.7732544\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ededcb27224b2925"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
