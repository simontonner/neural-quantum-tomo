{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:31:22.065715Z",
     "start_time": "2025-03-28T00:31:21.319812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.lax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "from load_mnist import download_mnist_if_needed, load_images, load_labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_dir = \"./data\"\n",
    "device = jax.devices('cpu')[0]\n",
    "\n",
    "print(f\"Data resides in        : {data_dir}\")\n",
    "print(f\"Training model on      : {str(device)}\")"
   ],
   "id": "27c3d547f23b4972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data resides in        : ./data\n",
      "Training model on      : TFRT_CPU_0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:31:23.449195Z",
     "start_time": "2025-03-28T00:31:22.913694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(x):\n",
    "    x = x.astype(np.float32) / 255.0    # normalize to [0, 1]\n",
    "    x = x > 0.5                         # binarize\n",
    "    x = x.reshape(x.shape[0], -1)       # flatten for RBM\n",
    "    x = jnp.array(x, dtype=jnp.float32) # use jax numpy array of dtype float32, because RBM has float32 params\n",
    "    return x\n",
    "\n",
    "\n",
    "data_paths = download_mnist_if_needed(root=data_dir, train_only=True)\n",
    "x_train_raw = load_images(data_paths['train_images'])\n",
    "y_train = load_labels(data_paths['train_labels'])\n",
    "\n",
    "x_train = preprocess(x_train_raw)\n",
    "print(f\"x_train dtype: {x_train.dtype}, shape: {x_train.shape}\")\n",
    "print(f\"x_train min: {x_train.min()}, max: {x_train.max()}\")"
   ],
   "id": "dc789a9d880e97c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train dtype: float32, shape: (60000, 784)\n",
      "x_train min: 0.0, max: 1.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:31:24.583131Z",
     "start_time": "2025-03-28T00:31:24.572240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RBM(nn.Module):\n",
    "    # The following variables form part of the constructor since this is a dataclass.\n",
    "    n_visible: int\n",
    "    n_hidden: int\n",
    "\n",
    "    def _sample_hidden(self, key: jax.random.PRNGKey, v, W, c, T=1.0):\n",
    "        # Random key is passed explicitly to ensure purity and JIT-compatibility\n",
    "        logits = (v @ W + c) / T\n",
    "        h_probs = jax.nn.sigmoid(logits)\n",
    "        h_sample = jax.random.bernoulli(key, h_probs)\n",
    "        return h_sample.astype(jnp.float32), h_probs\n",
    "\n",
    "    def _sample_visible(self, key: jax.random.PRNGKey, h, W, b, T=1.0):\n",
    "        # Random key is passed explicitly to ensure purity and JIT-compatibility\n",
    "        logits = (h @ W.T + b) / T\n",
    "        v_probs = jax.nn.sigmoid(logits)\n",
    "        v_sample = jax.random.bernoulli(key, v_probs)\n",
    "        return v_sample.astype(jnp.float32), v_probs\n",
    "\n",
    "    def sample_gibbs(self, v0_sample, k=1, T=1.0):\n",
    "        # JAX follows a functional programming paradigm with pure functions. However,\n",
    "        # methods like this one use self.param() and self.make_rng(), which require access\n",
    "        # to Flax's internal scope. These scopes are only available during .apply() or .init(),\n",
    "        # which safely inject parameters and RNGs while preserving functional purity externally.\n",
    "        loop_key = self.make_rng(\"sample\")\n",
    "        W, b, c = self.param(\"W\"), self.param(\"b\"), self.param(\"c\")\n",
    "\n",
    "        def gibbs_step_body(i, carry):\n",
    "            v_carry, key_carry = carry\n",
    "            key_carry, hidden_key, visible_key = jax.random.split(key_carry, 3)\n",
    "            h, _ = self._sample_hidden(hidden_key, v_carry, W, c, T)\n",
    "            v_next, _ = self._sample_visible(visible_key, h, W, b, T)\n",
    "            return (v_next, key_carry)\n",
    "\n",
    "        initial_carry = (v0_sample, loop_key)\n",
    "        final_carry = jax.lax.fori_loop(0, k, gibbs_step_body, initial_carry)\n",
    "        final_v = final_carry[0]\n",
    "        return final_v\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, v):\n",
    "        # In Flax, __call__ is the main computation, analogous to PyTorch’s forward().\n",
    "        # We have moved the implementation of the free energy here.\n",
    "        #\n",
    "        # Even though __call__ is invoked twice during contrastive divergence, the\n",
    "        # parameters are only initialized once on the first call via @nn.compact.\n",
    "        W = self.param(\"W\", nn.initializers.normal(0.01), (self.n_visible, self.n_hidden))\n",
    "        b = self.param(\"b\", nn.initializers.zeros, (self.n_visible,))\n",
    "        c = self.param(\"c\", nn.initializers.zeros, (self.n_hidden,))\n",
    "\n",
    "        # Assumes v has shape (batch_size, n_visible); ensure it before calling\n",
    "        visible_term = jnp.dot(v, b)\n",
    "        hidden_logits = v @ W + c\n",
    "        hidden_term = jnp.sum(jax.nn.softplus(hidden_logits), axis=-1)\n",
    "        return -visible_term - hidden_term"
   ],
   "id": "3d8fb14ccceb3134",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:31:26.126957Z",
     "start_time": "2025-03-28T00:31:25.858700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test RBM model, make sure to call apply\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rbm = RBM(n_visible=28*28, n_hidden=256)\n",
    "params = rbm.init(key, jnp.ones((1, 28*28)))['params']\n",
    "v = jnp.ones((1, 28*28))\n",
    "fe = rbm.apply({'params': params}, v)\n",
    "print(fe)"
   ],
   "id": "14041c116d50ed1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-179.0123]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:39:23.713834Z",
     "start_time": "2025-03-28T00:39:23.707645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=(\"rbm\", \"k\", \"T\"))\n",
    "def cd_loss(rbm, params, v0_sample, rng, k=1, T=1.0):\n",
    "    # v_k ← Gibbs sample starting from data\n",
    "    v_k = rbm.apply({'params': params}, v0_sample, method=rbm.sample_gibbs, rngs={'sample': rng}, k=k, T=T)\n",
    "\n",
    "    # __call__ is already the free energy (negative log unnormalized prob)\n",
    "    fe_data = rbm.apply({'params': params}, v0_sample)\n",
    "    fe_model = rbm.apply({'params': params}, v_k)\n",
    "\n",
    "    loss = jnp.mean(fe_data) - jnp.mean(fe_model)\n",
    "    return loss"
   ],
   "id": "2f0938b68e86184d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:39:25.324825Z",
     "start_time": "2025-03-28T00:39:24.556242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "batch_size = 32\n",
    "n_visible = 784\n",
    "n_hidden = 128\n",
    "\n",
    "# Dummy binarized input\n",
    "v0_sample = jax.random.bernoulli(key, p=0.5, shape=(batch_size, n_visible)).astype(jnp.float32)\n",
    "\n",
    "# Init RBM\n",
    "rbm = RBM(n_visible=n_visible, n_hidden=n_hidden)\n",
    "params = rbm.init(key, v0_sample)['params']\n",
    "\n",
    "# Run test\n",
    "loss = cd_loss(rbm, params, v0_sample, key, k=1)\n",
    "print(\"CD-1 Loss:\", loss)\n"
   ],
   "id": "d80aefab6564cf99",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "param() missing 1 required positional argument: 'init_fn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m params \u001B[38;5;241m=\u001B[39m rbm\u001B[38;5;241m.\u001B[39minit(key, v0_sample)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Run test\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcd_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrbm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv0_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCD-1 Loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss)\n",
      "    \u001B[0;31m[... skipping hidden 11 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[10], line 7\u001B[0m, in \u001B[0;36mcd_loss\u001B[0;34m(rbm, params, v0_sample, rng, k, T)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mpartial(jax\u001B[38;5;241m.\u001B[39mjit, static_argnames\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrbm\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcd_loss\u001B[39m(rbm, params, v0_sample, rng, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, T\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# v_k ← Gibbs sample starting from data\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m     v_k \u001B[38;5;241m=\u001B[39m \u001B[43mrbm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mparams\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv0_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrbm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_gibbs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrngs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msample\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrng\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# __call__ is already the free energy (negative log unnormalized prob)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     fe_data \u001B[38;5;241m=\u001B[39m rbm\u001B[38;5;241m.\u001B[39mapply({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m: params}, v0_sample)\n",
      "    \u001B[0;31m[... skipping hidden 6 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[3], line 26\u001B[0m, in \u001B[0;36mRBM.sample_gibbs\u001B[0;34m(self, v0_sample, k, T)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample_gibbs\u001B[39m(\u001B[38;5;28mself\u001B[39m, v0_sample, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, T\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m# JAX follows a functional programming paradigm with pure functions. However,\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# methods like this one use self.param() and self.make_rng(), which require access\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# to Flax's internal scope. These scopes are only available during .apply() or .init(),\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# which safely inject parameters and RNGs while preserving functional purity externally.\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     loop_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_rng(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m     W, b, c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mW\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgibbs_step_body\u001B[39m(i, carry):\n\u001B[1;32m     29\u001B[0m         v_carry, key_carry \u001B[38;5;241m=\u001B[39m carry\n",
      "\u001B[0;31mTypeError\u001B[0m: param() missing 1 required positional argument: 'init_fn'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:35:54.587877Z",
     "start_time": "2025-03-28T00:35:54.574899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "import optax\n",
    "from tqdm import trange\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    params: FrozenDict\n",
    "    rng: jax.random.PRNGKey\n",
    "    particles: jnp.ndarray  # persistent fantasy particles\n",
    "\n",
    "def free_energy(rbm, params, v):\n",
    "    return rbm.apply({'params': params}, v)\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(0, 5))\n",
    "def train_step(rbm, state, batch, k, weight_decay, pcd_reset):\n",
    "    def loss_fn(params, rng, particles):\n",
    "        v_data = batch\n",
    "        v_model = jax.lax.cond(\n",
    "            state.step % pcd_reset == 0,\n",
    "            lambda _: jax.random.bernoulli(rng, p=0.5, shape=v_data.shape),\n",
    "            lambda _: particles,\n",
    "            operand=None\n",
    "        )\n",
    "        rng, subkey = jax.random.split(rng)\n",
    "        v_model = rbm.apply({'params': params}, v_model, method=rbm.sample_gibbs, rngs={'sample': subkey}, k=k)\n",
    "        fe_data = free_energy(rbm, params, v_data).mean()\n",
    "        fe_model = free_energy(rbm, params, v_model).mean()\n",
    "        loss = fe_data - fe_model\n",
    "        l2_penalty = sum(jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "        return loss + weight_decay * l2_penalty, (loss, rng, v_model)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (fe_loss, rng, new_particles)), grads = grad_fn(state.params, state.rng, state.particles)\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng, particles=new_particles)\n",
    "    return new_state, fe_loss\n",
    "\n",
    "def train_rbm_jax(rbm, x_train, batch_size, num_epochs, k, lr, weight_decay, lr_decay, pcd_reset):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    init_rng, sample_rng = jax.random.split(rng)\n",
    "    init_data = jnp.ones((batch_size, rbm.n_visible), dtype=jnp.float32)\n",
    "    variables = rbm.init(init_rng, init_data)\n",
    "    params = variables['params']\n",
    "\n",
    "    tx = optax.chain(\n",
    "        optax.add_decayed_weights(weight_decay),\n",
    "        optax.exponential_decay(lr, transition_steps=1, decay_rate=lr_decay, staircase=True),\n",
    "        optax.adam(learning_rate=lr)\n",
    "    )\n",
    "\n",
    "    init_particles = jax.random.bernoulli(sample_rng, p=0.5, shape=(batch_size, rbm.n_visible)).astype(jnp.float32)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=rbm.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        rng=sample_rng,\n",
    "        particles=init_particles\n",
    "    )\n",
    "\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    metrics = {}\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Training\"):\n",
    "        perm = jax.random.permutation(state.rng, x_train.shape[0])\n",
    "        x_shuffled = x_train[perm]\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch = x_shuffled[i * batch_size:(i + 1) * batch_size]\n",
    "            state, loss = train_step(rbm, state, batch, k, weight_decay, pcd_reset)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        metrics[epoch] = {\"free_energy_loss\": float(avg_loss)}\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Free Energy Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return metrics, state\n"
   ],
   "id": "71ce7698fa2b8b3b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:36:09.315632Z",
     "start_time": "2025-03-28T00:36:09.130349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size      = 128\n",
    "visible_units   = 28*28 # 784\n",
    "hidden_units    = 256\n",
    "k               = 1      # Gibbs steps for PCD\n",
    "lr              = 1e-3\n",
    "num_epochs      = 40\n",
    "pcd_reset       = 75     # Reset persistent chain every N batches\n",
    "weight_decay    = 1e-5   # L2 regularization\n",
    "lr_decay_rate   = 0.95   # Learning rate decay factor PER EPOCH\n",
    "temperature     = 1.0    # Sampling temperature (fixed)\n",
    "\n",
    "\n",
    "\n",
    "rbm = RBM(n_visible=visible_units, n_hidden=hidden_units)\n",
    "metrics, state = train_rbm_jax(\n",
    "    rbm,\n",
    "    x_train,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    k=k,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_decay=lr_decay_rate,\n",
    "    pcd_reset=pcd_reset\n",
    ")"
   ],
   "id": "83d0ed0b268d3bdc",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 15\u001B[0m\n\u001B[1;32m     10\u001B[0m temperature     \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m    \u001B[38;5;66;03m# Sampling temperature (fixed)\u001B[39;00m\n\u001B[1;32m     14\u001B[0m rbm \u001B[38;5;241m=\u001B[39m RBM(n_visible\u001B[38;5;241m=\u001B[39mvisible_units, n_hidden\u001B[38;5;241m=\u001B[39mhidden_units)\n\u001B[0;32m---> 15\u001B[0m metrics, state \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_rbm_jax\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrbm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr_decay_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpcd_reset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpcd_reset\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 47\u001B[0m, in \u001B[0;36mtrain_rbm_jax\u001B[0;34m(rbm, x_train, batch_size, num_epochs, k, lr, weight_decay, lr_decay, pcd_reset)\u001B[0m\n\u001B[1;32m     44\u001B[0m variables \u001B[38;5;241m=\u001B[39m rbm\u001B[38;5;241m.\u001B[39minit(init_rng, init_data)\n\u001B[1;32m     45\u001B[0m params \u001B[38;5;241m=\u001B[39m variables[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 47\u001B[0m tx \u001B[38;5;241m=\u001B[39m \u001B[43moptax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_decayed_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexponential_decay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransition_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecay_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr_decay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstaircase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m init_particles \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mbernoulli(sample_rng, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, shape\u001B[38;5;241m=\u001B[39m(batch_size, rbm\u001B[38;5;241m.\u001B[39mn_visible))\u001B[38;5;241m.\u001B[39mastype(jnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     55\u001B[0m state \u001B[38;5;241m=\u001B[39m TrainState\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m     56\u001B[0m     apply_fn\u001B[38;5;241m=\u001B[39mrbm\u001B[38;5;241m.\u001B[39mapply,\n\u001B[1;32m     57\u001B[0m     params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     60\u001B[0m     particles\u001B[38;5;241m=\u001B[39minit_particles\n\u001B[1;32m     61\u001B[0m )\n",
      "File \u001B[0;32m~/miniforge3/envs/dlnn2/lib/python3.9/site-packages/optax/transforms/_combining.py:60\u001B[0m, in \u001B[0;36mchain\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchain\u001B[39m(\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;241m*\u001B[39margs: base\u001B[38;5;241m.\u001B[39mGradientTransformation,\n\u001B[1;32m     27\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m base\u001B[38;5;241m.\u001B[39mGradientTransformationExtraArgs:\n\u001B[1;32m     28\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Applies a list of chainable update transformations.\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m  This function creates a new :func:`optax.GradientTransformation` that applies\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m      >>> updates, new_state = chained_transform.update(updates, state, params)\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m   transforms \u001B[38;5;241m=\u001B[39m [base\u001B[38;5;241m.\u001B[39mwith_extra_args_support(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m     61\u001B[0m   init_fns, update_fns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mtransforms)\n\u001B[1;32m     63\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_fn\u001B[39m(params):\n",
      "File \u001B[0;32m~/miniforge3/envs/dlnn2/lib/python3.9/site-packages/optax/transforms/_combining.py:60\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchain\u001B[39m(\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;241m*\u001B[39margs: base\u001B[38;5;241m.\u001B[39mGradientTransformation,\n\u001B[1;32m     27\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m base\u001B[38;5;241m.\u001B[39mGradientTransformationExtraArgs:\n\u001B[1;32m     28\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Applies a list of chainable update transformations.\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m  This function creates a new :func:`optax.GradientTransformation` that applies\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m      >>> updates, new_state = chained_transform.update(updates, state, params)\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m   transforms \u001B[38;5;241m=\u001B[39m [\u001B[43mbase\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwith_extra_args_support\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m     61\u001B[0m   init_fns, update_fns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mtransforms)\n\u001B[1;32m     63\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_fn\u001B[39m(params):\n",
      "File \u001B[0;32m~/miniforge3/envs/dlnn2/lib/python3.9/site-packages/optax/_src/base.py:335\u001B[0m, in \u001B[0;36mwith_extra_args_support\u001B[0;34m(tx)\u001B[0m\n\u001B[1;32m    332\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m extra_args\n\u001B[1;32m    333\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m tx\u001B[38;5;241m.\u001B[39mupdate(updates, state, params)\n\u001B[0;32m--> 335\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m GradientTransformationExtraArgs(\u001B[43mtx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m, update)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'init'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "511f15b680068371"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
